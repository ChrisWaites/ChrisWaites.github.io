{"version":3,"sources":["serviceWorker.js","components/page.js","pages/research.js","pages/about.js","pages/writing.js","pages/reading.js","pages/design.js","posts/machine-unlearning.js","posts/differentially-private-deep-learning.js","posts/how-differential-privacy-fits-into-industry.js","posts/risk-aware-reinforcement-learning.js","posts/deaths-of-despair.js","posts/where-we-are-now.js","App.js","index.js"],"names":["Boolean","window","location","hostname","match","Page","props","style","display","alignItems","justifyContent","padding","width","textAlign","children","paddingTop","className","to","href","Entry","Button","variant","src","borderRadius","Post","math","height","frameborder","allow","allowfullscreen","url","maxWidth","hideCaption","App","exact","path","component","About","Research","Writing","Reading","Design","DeathsOfDespair","WhereWeAreNow","HowDifferentialPrivacyFitsIntoIndustry","DifferentiallyPrivateDeepLearning","RiskAwareReinforcementLearning","MachineUnlearning","ReactDOM","render","document","getElementById","navigator","serviceWorker","ready","then","registration","unregister","catch","error","console","message"],"mappings":"gNAYoBA,QACW,cAA7BC,OAAOC,SAASC,UAEe,UAA7BF,OAAOC,SAASC,UAEhBF,OAAOC,SAASC,SAASC,MACvB,2D,0CCkBSC,MAhCf,SAAcC,GACZ,OACE,yBAAKC,MAAO,CAAEC,QAAS,OAAQC,WAAY,aAAcC,eAAgB,WACvE,yBAAKH,MAAO,CAAEI,QAAS,OAAQC,MAAO,MAAOC,UAAW,SACrDP,EAAMQ,UAET,kBAAC,IAAD,CAAWP,MAAO,CAAEQ,WAAY,OAAQF,UAAW,UACjD,+BACE,4BAAI,4BACF,gCAAQ,kBAAC,IAAD,CAAMG,UAAU,OAAOC,GAAG,KAA1B,mBAEV,4BAAI,4BACF,kBAAC,IAAD,CAAMD,UAAU,OAAOC,GAAG,aAA1B,cAEF,4BAAI,4BACF,kBAAC,IAAD,CAAMD,UAAU,OAAOC,GAAG,YAA1B,aAEF,4BAAI,4BACF,kBAAC,IAAD,CAAMD,UAAU,OAAOC,GAAG,YAA1B,aAEF,4BAAI,4BACF,kBAAC,IAAD,CAAMD,UAAU,OAAOC,GAAG,WAA1B,YAEF,4BAAI,4BACF,uBAAGC,KAAK,kCAAR,gB,eCvBZ,SAASC,EAAMb,GACb,OACE,2BACE,kBAACc,EAAA,EAAD,CAAQC,QAAQ,WAAWH,KAAMZ,EAAMY,MACpCZ,EAAMQ,WA4BAT,MApBf,WACI,OACE,oCACE,kBAACc,EAAD,CAAOD,KAAK,oEACV,kHAA2F,6BAC3F,6BAFF,oCAGmC,6BACjC,gIAGF,kBAACC,EAAD,CAAOD,KAAK,oCACV,2GAAoF,6BACpF,6BAFF,mGAGkG,6BAChG,oDCRKb,MApBf,WACI,OACE,oCACE,2BACE,yBAAKiB,IAAI,yBAAyBf,MAAO,CAAEgB,aAAc,OAAQX,MAAO,WAE1E,mCACQ,IADR,YAGA,0CACe,IADf,0CAC2D,uBAAGM,KAAK,6BAAR,uBAD3D,0DAC6K,uBAAGA,KAAK,2BAAR,mCAD7K,KAGA,wCAGA,kBAAC,EAAD,QCbR,SAASC,EAAMb,GACb,OACE,2BACE,kBAAC,IAAD,CAAMU,UAAU,OAAOC,GAAIX,EAAMY,MAC/B,kBAACE,EAAA,EAAD,CAAQC,QAAQ,WAAWH,KAAMZ,EAAMY,MACrC,4BAAKZ,EAAMQ,aA0BNT,MAnBf,WACI,OACE,oCACE,kBAAC,EAAD,CAAOa,KAAK,uBAAZ,sBAGA,kBAAC,EAAD,CAAOA,KAAK,sCAAZ,qCAGA,kBAAC,EAAD,CAAOA,KAAK,gDAAZ,sDAGA,kBAAC,EAAD,CAAOA,KAAK,yCAAZ,qDCxBR,SAASC,EAAMb,GACb,OACE,2BACE,kBAACc,EAAA,EAAD,CAAQC,QAAQ,WAAWH,KAAMZ,EAAMY,MACrC,4BAAKZ,EAAMQ,YA+BJT,MAzBf,WACI,OACE,oCACE,kBAAC,EAAD,CAAOa,KAAK,6EAAZ,kCAGA,kBAAC,EAAD,CAAOA,KAAK,iGAAZ,8CAGA,kBAAC,EAAD,CAAOA,KAAK,mEAAZ,sDAGA,kBAAC,EAAD,CAAOA,KAAK,oCAAZ,sCAGA,kBAAC,EAAD,CAAOA,KAAK,oCAAZ,wDAGA,kBAAC,EAAD,CAAOA,KAAK,oCAAZ,oECfOb,MAbf,WACI,OACE,oCACE,2BACE,yBAAKiB,IAAI,0BAA0Bf,MAAO,CAAEgB,aAAc,OAAQX,MAAO,WAE3E,2BACE,yBAAKU,IAAI,eAAef,MAAO,CAAEgB,aAAc,OAAQX,MAAO,a,eCsGzDY,MA7Gf,WACI,OACE,oCACE,kDAGA,kCACO,IADP,gCAC0C,IAD1C,iHAC6J,yCAD7J,uHAGA,kCACM,kBAAC,aAAD,CAAYC,KAAK,MADvB,2IACqK,IADrK,qFAGA,wCACa,IADb,+DAGA,4BACE,4BACA,gGACmE,kBAAC,aAAD,CAAYA,KAAK,MADpF,eACqG,kBAAC,aAAD,CAAYA,KAAK,MADtH,6EAIA,4BACA,6DACgC,kBAAC,aAAD,CAAYA,KAAK,MADjD,4EACgI,IADhI,aAC+I,kBAAC,aAAD,CAAYA,KAAK,MADhK,kDAKF,iMACqK,uBAAGP,KAAK,oCAAR,qBADrK,4BAGA,kNACsL,kBAAC,aAAD,CAAYO,KAAK,YADvM,oBAGA,iDACsB,IADtB,+BAGA,kBAAC,YAAD,CAAWA,KAAK,yJAChB,2EAC+C,kBAAC,aAAD,CAAYA,KAAK,MADhE,wBAC0F,kBAAC,aAAD,CAAYA,KAAK,MAD3G,iBAC8H,kBAAC,aAAD,CAAYA,KAAK,iBAD/I,QACmK,kBAAC,aAAD,CAAYA,KAAK,qBADpL,KAGA,6SAGA,mCACM,kBAAC,aAAD,CAAYA,KAAK,mBADvB,OAC4C,kBAAC,aAAD,CAAYA,KAAK,MAD7D,0CAC0G,kBAAC,aAAD,CAAYA,KAAK,6DAD3H,wBACuM,kBAAC,aAAD,CAAYA,KAAK,MADxN,6CACwQ,kBAAC,aAAD,CAAYA,KAAK,6BADzR,KAGA,kBAAC,YAAD,CAAWA,KAAK,2GAChB,iKAIA,4BAAQlB,MAAO,CAAE,mBAAoB,QAAS,QAAW,OAAQ,gBAAiB,SAChF,yBAAKe,IAAI,mCAAmCV,MAAM,SAGpD,6BAAK,8GAC6E,kBAAC,aAAD,CAAYa,KAAK,MAD9F,gBACgH,kBAAC,aAAD,CAAYA,KAAK,cADjI,YACsJ,kBAAC,aAAD,CAAYA,KAAK,cADvK,6CAC6N,kBAAC,aAAD,CAAYA,KAAK,cAD9O,yBACgR,kBAAC,aAAD,CAAYA,KAAK,cADjS,sCAIL,4BAAQlB,MAAO,CAAE,mBAAoB,QAAS,QAAW,OAAQ,gBAAiB,SAChF,yBAAKe,IAAI,mCAAmCV,MAAM,SAGpD,6BAAK,sCACM,IADN,6EACqF,kBAAC,aAAD,CAAYa,KAAK,MADtG,uBACgI,IADhI,sCACwK,2CADxK,uEACgQ,kBAAC,aAAD,CAAYA,KAAK,cADjR,cACwS,2CADxS,0GACma,kBAAC,aAAD,CAAYA,KAAK,MADpb,+CACqe,kBAAC,aAAD,CAAYA,KAAK,6BADtf,OACqhB,IADrhB,QAC+hB,mCAD/hB,cAIL,4BAAQlB,MAAO,CAAE,mBAAoB,QAAS,QAAW,OAAQ,gBAAiB,SAChF,yBAAKe,IAAI,mCAAmCV,MAAM,SAGpD,6BAAK,+DAC8B,kBAAC,aAAD,CAAYa,KAAK,cAD/C,gBACwE,kBAAC,aAAD,CAAYA,KAAK,cADzF,gBACkH,kBAAC,aAAD,CAAYA,KAAK,cADnI,gBAC4J,kBAAC,aAAD,CAAYA,KAAK,6BAD7K,gCACoO,kBAAC,aAAD,CAAYA,KAAK,cADrP,2BACyR,kBAAC,aAAD,CAAYA,KAAK,6BAD1S,6JAC8d,kBAAC,aAAD,CAAYA,KAAK,6BAD/e,oBAC2hB,IAD3hB,uCACokB,kBAAC,aAAD,CAAYA,KAAK,cADrlB,mCACkoB,IADloB,iCACqqB,kBAAC,aAAD,CAAYA,KAAK,cADtrB,wBACutB,kBAAC,aAAD,CAAYA,KAAK,6BADxuB,oBACmxB,kBAAC,aAAD,CAAYA,KAAK,MADpyB,gBAIL,4BAAQlB,MAAO,CAAE,mBAAoB,QAAS,QAAW,OAAQ,gBAAiB,SAChF,yBAAKe,IAAI,mCAAmCV,MAAM,SAGpD,6BAAK,4IAC2G,kBAAC,aAAD,CAAYa,KAAK,MAD5H,sKAIL,4BAAQlB,MAAO,CAAE,mBAAoB,QAAS,QAAW,OAAQ,gBAAiB,SAChF,yBAAKe,IAAI,mCAAmCV,MAAM,SAGpD,6BAAK,sFACqD,kBAAC,aAAD,CAAYa,KAAK,6BADtE,0CACuI,kBAAC,aAAD,CAAYA,KAAK,cADxJ,QACyK,kBAAC,aAAD,CAAYA,KAAK,YAD1L,6CAIL,4BAAQlB,MAAO,CAAE,mBAAoB,QAAS,QAAW,OAAQ,gBAAiB,SAChF,yBAAKe,IAAI,mCAAmCV,MAAM,SAGpD,6BAAK,sMACqK,kBAAC,aAAD,CAAYa,KAAK,cADtL,QACuM,kBAAC,aAAD,CAAYA,KAAK,YADxN,qIAIL,yCAGA,4OACgN,uBAAGP,KAAK,uDAAR,QADhN,mCCOOM,MAnFf,WACI,OACE,oCACE,+EAGA,mTAIA,oDAGA,0GAC+E,IAD/E,+CACgI,qCADhI,uEAGA,qDACyB,oDADzB,4CAC+F,0CAD/F,kJACiQ,kBAAC,aAAD,CAAYC,KAAK,MADlR,QAC6R,kBAAC,aAAD,CAAYA,KAAK,MAD9S,KAGA,uSAC2Q,sCAD3Q,4DAGA,4CACiB,IADjB,6HACiJ,IADjJ,iIACoR,oKADpR,4BAGA,oFACwD,6BAAK,uBAAGP,KAAK,yDAAR,MAD7D,KAGA,2BAAG,mCACG,kBAAC,aAAD,CAAYO,KAAK,+CADpB,uCACoG,kBAAC,aAAD,CAAYA,KAAK,MADrH,KAC6H,kBAAC,aAAD,CAAYA,KAAK,iBAD9I,uCACkM,kBAAC,aAAD,CAAYA,KAAK,+BADnN,wDACqS,kBAAC,aAAD,CAAYA,KAAK,wBADtT,kBAEH,kBAAC,YAAD,CAAWA,KAAK,oHAEhB,kDACuB,IADvB,sNAIA,6CAGA,uXAGA,iaAGA,kBAAC,YAAD,CAAWA,KAAK,iEAChB,kBAAC,YAAD,CAAWA,KAAK,mEAChB,kBAAC,YAAD,CAAWA,KAAK,uEAChB,gKAIA,kFAGA,2BACE,uBAAGP,KAAK,oCAAR,gBADF,qNAGA,iOACqM,kBAAC,aAAD,CAAYO,KAAK,MADtN,2IACoW,kCADpW,cAC2X,kCAD3X,+EAGA,4IACgH,kBAAC,aAAD,CAAYA,KAAK,MADjI,2BAC+J,kBAAC,aAAD,CAAYA,KAAK,YADhL,uEAC+P,kBAAC,aAAD,CAAYA,KAAK,YADhR,6KAGA,kBAAC,YAAD,CAAWA,KAAK,iEAChB,kBAAC,YAAD,CAAWA,KAAK,mEAChB,kBAAC,YAAD,CAAWA,KAAK,0EAChB,kBAAC,YAAD,CAAWA,KAAK,8GAChB,kBAAC,YAAD,CAAWA,KAAK,mDAChB,sFAC0D,kBAAC,aAAD,CAAYA,KAAK,MAD3E,uEACqJ,kDADrJ,uQACob,kBAAC,aAAD,CAAYA,KAAK,MADrc,KAC6c,kBAAC,aAAD,CAAYA,KAAK,YAD9d,KAC2e,kBAAC,aAAD,CAAYA,KAAK,MAD5f,yBACwhB,kBAAC,aAAD,CAAYA,KAAK,iBADziB,4GAIA,yCAGA,yOC3DOD,MA9Cf,WACE,OACE,6BACE,kFAGA,2EACkD,IADlD,oQAGA,8HACoG,gDADpG,+XAGA,+gBAGA,kVAGA,0FAC2D,4LAD3D,KAGA,uiBAGA,0TAGA,0jBAGA,gRACsP,sCADtP,QAC0Q,0CAD1Q,KAGA,yVAGA,0bAGA,wRCyDSA,MA9Ff,WACE,OACE,oCACE,iEAIA,6HAIA,4CAGA,0KAC+I,IAD/I,kCAGA,iEACqC,kBAAC,aAAD,CAAYC,KAAK,yEADtD,2BACiJ,kBAAC,aAAD,CAAYA,KAAK,MADlK,kBACuL,kBAAC,aAAD,CAAYA,KAAK,MADxM,6BACwO,kBAAC,aAAD,CAAYA,KAAK,mBADzP,sDAC8T,kBAAC,aAAD,CAAYA,KAAK,MAD/U,aAC+V,kBAAC,aAAD,CAAYA,KAAK,MADhX,+HACkf,kBAAC,aAAD,CAAYA,KAAK,mEADngB,iRAGA,kBAAC,YAAD,CAAWA,KAAK,6FAChB,uEAC2C,kBAAC,aAAD,CAAYA,KAAK,8CAD5D,kCACsI,kBAAC,aAAD,CAAYA,KAAK,MADvJ,0BACoL,kBAAC,aAAD,CAAYA,KAAK,MADrM,+DACuQ,kBAAC,aAAD,CAAYA,KAAK,MADxR,QACmS,kBAAC,aAAD,CAAYA,KAAK,MADpT,gFACuY,kBAAC,aAAD,CAAYA,KAAK,MADxZ,OACka,kBAAC,aAAD,CAAYA,KAAK,MADnb,yLAGA,kBAAC,YAAD,CAAWA,KAAK,kFAChB,oPAIA,6CAGA,+DACoC,IADpC,8EACoH,4CADpH,mLAGA,iFACsD,IADtD,KAC6D,gGAD7D,4CAC+K,oCAD/K,mHAGA,wEAC4C,gEAD5C,0FAC4K,kBAAC,aAAD,CAAYA,KAAK,YAD7L,sBAC2N,kBAAC,aAAD,CAAYA,KAAK,uBAD5O,6DAC2T,kBAAC,aAAD,CAAYA,KAAK,gBAD5U,gJAGA,qEAC2C,4BAAI,uBAAGP,KAAK,oCAAR,+BAD/C,iDAIA,mDAGA,4OAGA,kBAAC,YAAD,CAAWO,KAAK,kEAChB,wEAC8C,kBAAC,aAAD,CAAYA,KAAK,mBAD/D,QACsF,kBAAC,aAAD,CAAYA,KAAK,2BADvG,6NAGA,kBAAC,YAAD,CAAWA,KAAK,mEAChB,4NAGA,kBAAC,YAAD,CAAWA,KAAK,0FAChB,kBAAC,YAAD,CAAWA,KAAK,2IAChB,8CACoB,yCADpB,YAC+C,kBAAC,aAAD,CAAYA,KAAK,mBADhE,QACsF,kBAAC,aAAD,CAAYA,KAAK,2BADvG,oBACgJ,uCADhJ,YACyK,kBAAC,aAAD,CAAYA,KAAK,YAD1L,QAC0M,kBAAC,aAAD,CAAYA,KAAK,oBAD3N,KAGA,4XAGA,kBAAC,YAAD,CAAWA,KAAK,kDAChB,kBAAC,YAAD,CAAWA,KAAK,iGAChB,mGACuE,kBAAC,aAAD,CAAYA,KAAK,6BADxF,QACyH,kBAAC,aAAD,CAAYA,KAAK,6CAD1I,6BAIA,kDAGA,yJAC6H,kBAAC,aAAD,CAAYA,KAAK,MAD9I,8CAC+L,kBAAC,aAAD,CAAYA,KAAK,MADhN,qEACwR,kBAAC,aAAD,CAAYA,KAAK,mBADzS,mBAC2U,kBAAC,aAAD,CAAYA,KAAK,2BAD5V,iEACmb,sCADnb,kCACme,kBAAC,aAAD,CAAYA,KAAK,mBADpf,yDAC2jB,kBAAC,aAAD,CAAYA,KAAK,YAD5kB,QAC4lB,kBAAC,aAAD,CAAYA,KAAK,mBAD7mB,gBAGA,kBAAC,YAAD,CAAWA,KAAK,mHAChB,qHACyF,0CADzF,8BAIA,0CAGA,2L,QCQSD,MA9Ff,WACE,OACE,oCACE,iDAGA,2BACA,0JAIA,iPAGA,+VAGA,uSAGA,sRAIA,iDAGA,4FACmE,IADnE,yDAC+H,IAD/H,wUAGA,+eAGA,kEACyC,IADzC,+vBAC2yB,IAD3yB,2KAGA,8UACqT,IADrT,6YAGA,2yBAGA,kOACyM,IADzM,mEAGA,4BAAQZ,MAAM,MAAMc,OAAO,MAAMJ,IAAI,uDAAuDK,YAAY,IAAIC,MAAM,0EAA0EC,iBAAe,IAE3M,4CAIA,sCAGA,mCACQ,IADR,uFACiG,uBAAGX,KAAK,iJAAR,QADjG,aAIA,2BACE,iCACA,iCAAS,6FACT,2JAC8H,IAD9H,wBACyJ,IADzJ,oDACgN,IADhN,yCAC2P,6BAC3P,6BACE,sCAAY,IAAZ,sDAAqE,IAArE,0GAHF,oGAGwR,IAHxR,8GAGyY,IAHzY,4SAGwrB,IAHxrB,yPAGo7B,IAHp7B,mDAG0+B,IAH1+B,wDAGqiC,IAHriC,eAGsjC,6BACtjC,6BACE,sKALF,+jBAK6sB,IAL7sB,oFAKoyB,IALpyB,8DAKq2B,IALr2B,oDAK45B,IAL55B,4RAK2rC,IAL3rC,qKAKm2C,IALn2C,yJAK+/C,IAL//C,qLAKurD,IALvrD,yIAKm0D,IALn0D,qCAK02D,6BAC12D,6BACE,sCAAY,IAAZ,mBAAkC,IAAlC,2KAPF,uCAOyP,IAPzP,gYAO2nB,6BAC3nB,6BACE,kDAAwB,IAAxB,yKATF,gLASsX,IATtX,kSAS2pB,IAT3pB,qFASmvB,IATnvB,kCASwxB,IATxxB,mJAS66B,6BAC76B,6BACE,qIAA2G,IAA3G,iDAXF,SAW0K,IAX1K,2VAWugB,6BACvgB,6BAZA,+CAa+C,IAb/C,sFAgBA,8BAGF,sCAIA,4BAAQN,MAAM,MAAMc,OAAO,MAAMJ,IAAI,uDAAuDK,YAAY,IAAIC,MAAM,0EAA0EC,iBAAe,IAE3M,kBAAC,IAAD,CAAgBC,IAAI,oCAAoCC,SAAU,IAAKC,aAAa,IACpF,kBAAC,IAAD,CAAgBF,IAAI,oCAAoCC,SAAU,IAAKC,aAAa,IAEpF,4BAAQpB,MAAM,MAAMc,OAAO,MAAMJ,IAAI,uDAAuDK,YAAY,IAAIC,MAAM,0EAA0EC,iBAAe,MCvDlML,MAnCf,WACE,OACE,oCACE,gDAGA,wCAGA,4BAAQZ,MAAM,MAAMc,OAAO,MAAMJ,IAAI,qDAAqDK,YAAY,IAAIC,MAAM,0EAA0EC,iBAAe,IACzM,4BAAQjB,MAAM,MAAMc,OAAO,MAAMJ,IAAI,4CAA4CK,YAAY,IAAIC,MAAM,0EAA0EC,iBAAe,IAEhM,4BAAQjB,MAAM,MAAMc,OAAO,MAAMJ,IAAI,4CAA4CK,YAAY,IAAIC,MAAM,0EAA0EC,iBAAe,IAEhM,4BAAQjB,MAAM,MAAMc,OAAO,MAAMJ,IAAI,4CAA4CK,YAAY,IAAIC,MAAM,0EAA0EC,iBAAe,IAEhM,4BAAQjB,MAAM,MAAMc,OAAO,MAAMJ,IAAI,4CAA4CK,YAAY,IAAIC,MAAM,0EAA0EC,iBAAe,IAEhM,4BAAQjB,MAAM,MAAMc,OAAO,MAAMJ,IAAI,4CAA4CK,YAAY,IAAIC,MAAM,0EAA0EC,iBAAe,IAEhM,2BACE,qPAKF,2BACE,wOCSOI,MArBf,WACE,OACE,kBAAC,EAAD,KACE,kBAAC,IAAD,CAAOC,OAAK,EAACC,KAAK,IAAIC,UAAWC,IAEjC,kBAAC,IAAD,CAAOF,KAAK,SAASC,UAAWC,IAChC,kBAAC,IAAD,CAAOF,KAAK,YAAYC,UAAWE,IACnC,kBAAC,IAAD,CAAOH,KAAK,WAAWC,UAAWG,IAClC,kBAAC,IAAD,CAAOJ,KAAK,WAAWC,UAAWI,IAClC,kBAAC,IAAD,CAAOL,KAAK,UAAUC,UAAWK,IAEjC,kBAAC,IAAD,CAAON,KAAK,qBAAqBC,UAAWM,IAC5C,kBAAC,IAAD,CAAOP,KAAK,oBAAoBC,UAAWO,IAC3C,kBAAC,IAAD,CAAOR,KAAK,+CAA+CC,UAAWQ,IACtE,kBAAC,IAAD,CAAOT,KAAK,wCAAwCC,UAAWS,IAC/D,kBAAC,IAAD,CAAOV,KAAK,qCAAqCC,UAAWU,IAC5D,kBAAC,IAAD,CAAOX,KAAK,sBAAsBC,UAAWW,M,eC5BnDC,EAASC,OACP,kBAAC,IAAD,KACE,kBAAC,EAAD,OAECC,SAASC,eAAe,SdwHvB,kBAAmBC,WACrBA,UAAUC,cAAcC,MACrBC,MAAK,SAAAC,GACJA,EAAaC,gBAEdC,OAAM,SAAAC,GACLC,QAAQD,MAAMA,EAAME,c","file":"static/js/main.4dbbb0b7.chunk.js","sourcesContent":["// This optional code is used to register a service worker.\n// register() is not called by default.\n\n// This lets the app load faster on subsequent visits in production, and gives\n// it offline capabilities. However, it also means that developers (and users)\n// will only see deployed updates on subsequent visits to a page, after all the\n// existing tabs open on the page have been closed, since previously cached\n// resources are updated in the background.\n\n// To learn more about the benefits of this model and instructions on how to\n// opt-in, read https://bit.ly/CRA-PWA\n\nconst isLocalhost = Boolean(\n  window.location.hostname === 'localhost' ||\n    // [::1] is the IPv6 localhost address.\n    window.location.hostname === '[::1]' ||\n    // 127.0.0.0/8 are considered localhost for IPv4.\n    window.location.hostname.match(\n      /^127(?:\\.(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)){3}$/\n    )\n);\n\nexport function register(config) {\n  if (process.env.NODE_ENV === 'production' && 'serviceWorker' in navigator) {\n    // The URL constructor is available in all browsers that support SW.\n    const publicUrl = new URL(process.env.PUBLIC_URL, window.location.href);\n    if (publicUrl.origin !== window.location.origin) {\n      // Our service worker won't work if PUBLIC_URL is on a different origin\n      // from what our page is served on. This might happen if a CDN is used to\n      // serve assets; see https://github.com/facebook/create-react-app/issues/2374\n      return;\n    }\n\n    window.addEventListener('load', () => {\n      const swUrl = `${process.env.PUBLIC_URL}/service-worker.js`;\n\n      if (isLocalhost) {\n        // This is running on localhost. Let's check if a service worker still exists or not.\n        checkValidServiceWorker(swUrl, config);\n\n        // Add some additional logging to localhost, pointing developers to the\n        // service worker/PWA documentation.\n        navigator.serviceWorker.ready.then(() => {\n          console.log(\n            'This web app is being served cache-first by a service ' +\n              'worker. To learn more, visit https://bit.ly/CRA-PWA'\n          );\n        });\n      } else {\n        // Is not localhost. Just register service worker\n        registerValidSW(swUrl, config);\n      }\n    });\n  }\n}\n\nfunction registerValidSW(swUrl, config) {\n  navigator.serviceWorker\n    .register(swUrl)\n    .then(registration => {\n      registration.onupdatefound = () => {\n        const installingWorker = registration.installing;\n        if (installingWorker == null) {\n          return;\n        }\n        installingWorker.onstatechange = () => {\n          if (installingWorker.state === 'installed') {\n            if (navigator.serviceWorker.controller) {\n              // At this point, the updated precached content has been fetched,\n              // but the previous service worker will still serve the older\n              // content until all client tabs are closed.\n              console.log(\n                'New content is available and will be used when all ' +\n                  'tabs for this page are closed. See https://bit.ly/CRA-PWA.'\n              );\n\n              // Execute callback\n              if (config && config.onUpdate) {\n                config.onUpdate(registration);\n              }\n            } else {\n              // At this point, everything has been precached.\n              // It's the perfect time to display a\n              // \"Content is cached for offline use.\" message.\n              console.log('Content is cached for offline use.');\n\n              // Execute callback\n              if (config && config.onSuccess) {\n                config.onSuccess(registration);\n              }\n            }\n          }\n        };\n      };\n    })\n    .catch(error => {\n      console.error('Error during service worker registration:', error);\n    });\n}\n\nfunction checkValidServiceWorker(swUrl, config) {\n  // Check if the service worker can be found. If it can't reload the page.\n  fetch(swUrl, {\n    headers: { 'Service-Worker': 'script' },\n  })\n    .then(response => {\n      // Ensure service worker exists, and that we really are getting a JS file.\n      const contentType = response.headers.get('content-type');\n      if (\n        response.status === 404 ||\n        (contentType != null && contentType.indexOf('javascript') === -1)\n      ) {\n        // No service worker found. Probably a different app. Reload the page.\n        navigator.serviceWorker.ready.then(registration => {\n          registration.unregister().then(() => {\n            window.location.reload();\n          });\n        });\n      } else {\n        // Service worker found. Proceed as normal.\n        registerValidSW(swUrl, config);\n      }\n    })\n    .catch(() => {\n      console.log(\n        'No internet connection found. App is running in offline mode.'\n      );\n    });\n}\n\nexport function unregister() {\n  if ('serviceWorker' in navigator) {\n    navigator.serviceWorker.ready\n      .then(registration => {\n        registration.unregister();\n      })\n      .catch(error => {\n        console.error(error.message);\n      });\n  }\n}\n","import React from 'react';\nimport StickyBox from 'react-sticky-box';\nimport { Link } from 'react-router-dom';\n\nfunction Page(props) {\n  return (\n    <div style={{ display: 'flex', alignItems: 'flex-start', justifyContent: 'center' }}>\n      <div style={{ padding: '60px', width: '40%', textAlign: 'left' }}>\n        {props.children}\n      </div>\n      <StickyBox style={{ paddingTop: '60px', textAlign: 'right' }}>\n        <table>\n          <tr><td>\n            <strong><Link className='link' to='/'>Chris Waites</Link></strong>\n          </td></tr>\n          <tr><td>\n            <Link className='link' to='/research'>Research</Link>\n          </td></tr>\n          <tr><td>\n            <Link className='link' to='/writing'>Writing</Link>\n          </td></tr>\n          <tr><td>\n            <Link className='link' to='/reading'>Reading</Link>\n          </td></tr>\n          <tr><td>\n            <Link className='link' to='/design'>Design</Link>\n          </td></tr>\n          <tr><td>\n            <a href='https://github.com/ChrisWaites'>Github</a>\n          </td></tr>\n        </table>\n      </StickyBox>\n    </div>\n  );\n}\n\nexport default Page;\n","import React from 'react';\nimport Button from '@material-ui/core/Button';\nimport '../App.css';\n\n\nfunction Entry(props) {\n  return (\n    <p>\n      <Button variant='outlined' href={props.href}>\n        {props.children}\n      </Button>\n    </p>\n\n  );\n}\n\n\nfunction Page() {\n    return (\n      <>\n        <Entry href='https://invertibleworkshop.github.io/accepted_papers/pdfs/41.pdf'>\n          <em>Differentially Private Normalizing Flows for Privacy-Preserving Density Estimation</em><br/>\n          <hr/>\n          Chris Waites and Rachel Cummings.<br/>\n          <em>ICML Workshop on Invertible Neural Networks, Normalizing Flows, and Explicit Likelihood Models.</em>\n        </Entry>\n\n        <Entry href='https://arxiv.org/abs/1912.03250'>\n          <em>Differentially Private Mixed-Type Data Generation For Unsupervised Learning</em><br/>\n          <hr/>\n          Uthaipon Tantipongpipat*, Chris Waites*, Digvijay Boob, Amaresh Ankit Siva, and Rachel Cummings.<br/>\n          <em>arXiv:1912.03250.</em>\n        </Entry>\n      </>\n    );\n}\n\nexport default Page;\n","import React from 'react';\nimport Research from './research';\n\nfunction Page() {\n    return (\n      <>\n        <p>\n          <img src='./academic-banner.jpeg' style={{ borderRadius: '10px', width: '100%' }} />\n        </p>\n        <p>\n          Hi! I{\"\\'\"}m Chris.\n        </p>\n        <p>\n          Currently, I{\"\\'\"}m doing my M.S. in Computer Science at <a href='https://www.stanford.edu/'>Stanford University</a>. Previously, I did my B.S. in Computer Science at the <a href='https://www.gatech.edu/'>Georgia Institute of Technology</a>.\n        </p>\n        <h3>\n          Research\n        </h3>\n        <Research/>\n      </>\n    );\n}\n\nexport default Page;\n","import React from 'react';\nimport { Link } from 'react-router-dom';\nimport Button from '@material-ui/core/Button';\nimport '../App.css';\n\nfunction Entry(props) {\n  return (\n    <p>\n      <Link className='link' to={props.href}>\n        <Button variant='outlined' href={props.href}>\n          <em>{props.children}</em>\n        </Button>\n      </Link>\n    </p>\n  );\n}\n\nfunction Page() {\n    return (\n      <>\n        <Entry href='/machine-unlearning'>\n          Machine Unlearning\n        </Entry>\n        <Entry href='/risk-aware-reinforcement-learning'>\n          Risk-Aware Reinforcement Learning\n        </Entry>\n        <Entry href='/how-differential-privacy-fits-into-industry'>\n          How Differential Privacy (Could) Fit Into Industry\n        </Entry>\n        <Entry href='/differentially-private-deep-learning'>\n          A Guide to Differentially Private Deep Learning\n        </Entry>\n      </>\n    );\n}\n\nexport default Page;\n","import React from 'react';\nimport Button from '@material-ui/core/Button';\nimport '../App.css';\n\n\nfunction Entry(props) {\n  return (\n    <p>\n      <Button variant='outlined' href={props.href}>\n        <em>{props.children}</em>\n      </Button>\n    </p>\n  );\n}\n\nfunction Page() {\n    return (\n      <>\n        <Entry href='https://law.stanford.edu/wp-content/uploads/2019/01/Bellovin_20190129.pdf'>\n            Privacy and Synthetic Datasets\n        </Entry>\n        <Entry href='https://cset.georgetown.edu/wp-content/uploads/Keeping-Top-AI-Talent-in-the-United-States.pdf'>\n            Keeping Top AI Talent in the United States\n        </Entry>\n        <Entry href='http://web.stanford.edu/class/psych209/Readings/LakeEtAlBBS.pdf'>\n            Building Machines That Learn and Think Like People\n        </Entry>\n        <Entry href='https://arxiv.org/abs/1911.09421'>\n            The Linear Algebra Mapping Problem\n        </Entry>\n        <Entry href='https://arxiv.org/abs/1905.02175'>\n            Adversarial Examples Are Not Bugs, They Are Features\n        </Entry>\n        <Entry href='https://arxiv.org/abs/2003.03384'>\n            AutoML-Zero: Evolving Machine Learning Algorithms From Scratch\n        </Entry>\n      </>\n    );\n}\n\nexport default Page;\n","import React from 'react';\nimport Button from '@material-ui/core/Button';\nimport '../App.css';\n\n\nfunction Page() {\n    return (\n      <>\n        <p>\n          <img src='./normalizing-flows.png' style={{ borderRadius: '10px', width: '100%' }} />\n        </p>\n        <p>\n          <img src='./hoover.gif' style={{ borderRadius: '10px', width: '100%' }} />\n        </p>\n      </>\n    );\n}\n\nexport default Page;\n","import React from 'react';\nimport 'katex/dist/katex.min.css';\nimport { InlineMath, BlockMath } from 'react-katex';\n\n\nfunction Post() {\n    return (\n      <>\n        <h2>\n          Machine Unlearning\n        </h2>\n        <p>\n          Here{\"\\'\"}s an interesting problem: let{\"\\'\"}s imagine your data was used to train some machine learning model. Now, you want to request that your data be <em>unlearned</em> from the model. That is, the model is updated so that its parameters and outputs have no knowledge of you anymore.\n        </p>\n        <p>\n          For <InlineMath math=\"k\"/>-nearest neighbors, the problem is trivial - simply remove the point from the data. For something more complex like a neural network, it{\"\\'\"}s not immediately clear what you would do. Is their an interesting middle-ground?\n        </p>\n        <p>\n          First, let{\"\\'\"}s define the problem. Consider the two following scenarios:\n        </p>\n        <ul>\n          <li>\n          <em>\n            Case A (Real): We train the model on the full dataset containing <InlineMath math=\"x\"/>, we remove <InlineMath math=\"x\"/> from the model parameters, and then we publish the model to the public.\n          </em>\n          </li>\n          <li>\n          <em>\n            Case B (Imaginary): The point <InlineMath math=\"x\"/> was never in the dataset, we train the model on the dataset (which doesn{\"\\'\"}t contain <InlineMath math=\"x\"/>), and then publish the model to the public.\n          </em>\n          </li>\n        </ul>\n        <p>\n          If we can find a procedure for Case A which yields an indistinguishable outcome to Case B, then we will have achieved our goal. In this respect, I will detail the <a href=\"https://arxiv.org/abs/2007.02923\">descent-to-delete</a> approach by Neel et al.\n        </p>\n        <p>\n          For the purposes of this post, we are going to tackle the case of a convex parametric model. For example, this would include logistic regression with binary cross entropy loss and <InlineMath math=\"\\ell_2\" /> regularization.\n        </p>\n        <p>\n          To be concrete, let{\"\\'\"}s say our loss function is:\n        </p>\n        <BlockMath math=\"\\ell(\\theta) = - \\frac{1}{n} \\left( \\sum_{i \\in [n]} y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right) + \\lambda ||\\theta||_2\"/>\n        <p>\n          Then we would have that our loss function is <InlineMath math=\"m\"/>-strongly convex and <InlineMath math=\"M\"/>-smooth where <InlineMath math=\"m = \\lambda\"/> and <InlineMath math=\"M = 4 - \\lambda\"/>.\n        </p>\n        <p>\n          Now, the great part about working in this setting is that we have convergence guarantees. That is, as long as we know how far we are at most from the global optimum, if we run gradient descent, we know we will be at most some distance after some number of steps we take.\n        </p>\n        <em>\n          Let <InlineMath math=\"\\ell(\\theta)\" /> be <InlineMath math=\"m\" />-strongly convex and M-smooth, and let <InlineMath math=\"\\theta^* = argmin_{\\theta \\in \\Theta} \\ell(\\theta)\" />. We have that after <InlineMath math=\"T\" /> steps of gradient descent with step size <InlineMath math=\"\\eta = \\frac{2}{m + M}\" />:\n        </em>\n        <BlockMath math=\"||\\theta_T - \\theta^*||_2 \\leq \\left( \\frac{M - m}{M + m} \\right)^T || \\theta_0 - \\theta^*||_2\" />\n        <p>\n          Now, for sake of intuition, I will give a geometric argument for showing how we can get some notion of machine unlearning to work.\n        </p>\n\n        <center style={{ 'background-color': 'white', 'padding': '20px', 'border-radius': '20px' }}>\n          <img src=\"../machine-unlearning/image2.svg\" width=\"50%\"/>\n        </center>\n\n        <br/><p>\n          We begin with Case A, where we perform gradient descent on the full dataset for <InlineMath math=\"t\"/> steps until <InlineMath math=\"\\theta_0\"/> becomes <InlineMath math=\"\\theta_t\"/>. The dotted line represents how far away <InlineMath math=\"\\theta_t\"/> can possibly be from <InlineMath math=\"\\theta^D\"/> due to our convergence guarantee.\n        </p>\n\n        <center style={{ 'background-color': 'white', 'padding': '20px', 'border-radius': '20px' }}>\n          <img src=\"../machine-unlearning/image1.svg\" width=\"50%\"/>\n        </center>\n\n        <br/><p>\n          Now, let{\"\\'\"}s say that this point in training we receive a deletion request for point <InlineMath math=\"x\"/>. To handle this, we{\"\\'\"}ll need to establish the notion of <em>sensitivity</em>. That is, if we have some global optimum across the entire dataset <InlineMath math=\"\\theta^D\"/>, then the <em>sensitivity</em> is the furthest away the global minimum can move due to the removal of a single point. When we remove <InlineMath math=\"x\"/>, we know that the resulting global minimum <InlineMath math=\"\\theta^{D \\setminus x}\"/> can{\"\\'\"}t be <em>too</em> far away.\n        </p>\n\n        <center style={{ 'background-color': 'white', 'padding': '20px', 'border-radius': '20px' }}>\n          <img src=\"../machine-unlearning/image3.svg\" width=\"50%\"/>\n        </center>\n\n        <br/><p>\n          Given this, we know how far away <InlineMath math=\"\\theta_t\"/> can be from <InlineMath math=\"\\theta^D\"/> and how far <InlineMath math=\"\\theta^D\"/> can be from <InlineMath math=\"\\theta^{D \\setminus x}\"/>. Therefore, we know how far <InlineMath math=\"\\theta_t\"/> could possibly be from <InlineMath math=\"\\theta^{D \\setminus x}\"/>. Given this information, we can again apply the convergence guarantee and perform gradient descent for some required number of steps in the direction of <InlineMath math=\"\\theta^{D \\setminus x}\"/> until we know we{\"\\'\"}re some distance away, resulting in <InlineMath math=\"\\theta_T\"/>. This finishes Case A, where we{\"\\'\"}ve gone from an initial point <InlineMath math=\"\\theta_0\"/> and gotten close to <InlineMath math=\"\\theta^{D \\setminus x}\"/> without knowing <InlineMath math=\"x\"/> beforehand.\n        </p>\n\n        <center style={{ 'background-color': 'white', 'padding': '20px', 'border-radius': '20px' }}>\n          <img src=\"../machine-unlearning/image4.svg\" width=\"50%\"/>\n        </center>\n\n        <br/><p>\n          Now we consider alternative reality, Case B. That is, we start from our initial point and train regularly but <InlineMath math=\"x\"/> was never in the dataset. Given our convergence guarantee, again we can know after some number of steps that we are at least within some distance of the optimum.\n        </p>\n\n        <center style={{ 'background-color': 'white', 'padding': '20px', 'border-radius': '20px' }}>\n          <img src=\"../machine-unlearning/image5.svg\" width=\"50%\"/>\n        </center>\n\n        <br/><p>\n          Now given that we can guarantee a certain distance from <InlineMath math=\"\\theta^{D \\setminus x}\"/> in either case, we can guarantee that <InlineMath math=\"\\theta_T\"/> and <InlineMath math=\"\\theta\"/> are within some distance of one another.\n        </p>\n\n        <center style={{ 'background-color': 'white', 'padding': '20px', 'border-radius': '20px' }}>\n          <img src=\"../machine-unlearning/image6.svg\" width=\"50%\"/>\n        </center>\n\n        <br/><p>\n          Finally, in either case, we publish the models according to the same publishing scheme, namely an injection of Gaussian noise to the model parameters scaled to how far <InlineMath math=\"\\theta_T\"/> and <InlineMath math=\"\\theta\"/> could possibly be from one another. All of this entails that both outcomes are statistically indistinguishable from one another.\n        </p>\n\n        <h3>\n          Conlusion\n        </h3>\n        <p>\n          Hopefully this was a useful introduction into the problem of machine unlearning and how it can be addressed. For further reading, make sure to check out the amazing blog post about an alternative approach, <a href=\"http://www.cleverhans.io/2020/07/20/unlearning.html\">SISA</a>, proposed by Papernot et al.\n        </p>\n      </>\n    );\n}\n\nexport default Post;\n","import React from 'react';\nimport 'katex/dist/katex.min.css';\nimport { InlineMath, BlockMath } from 'react-katex';\n\n/*\n        <h3>\n          Private Aggregation of Teacher Ensembles\n        </h3>\n        <p>\n          Private Aggregation of Teacher Ensembles or PATE is an alternative method to DPSGD for conducting differentially private learning. The key idea is to, rather than train a single strong model which captures a complex criterion in a differentially private manner, train a set of weaker, non-private models on partitions of the data and then perform a noisy aggregation of their predictions. Overall the method has been shown to be quite effective at the expense of some assumptions about the training procedure. There has even been an application of this technique to GANs via PATE-GAN.\n        </p>\n\n        <h3>\n          Differentially Private Federated Learning\n        </h3>\n\n        <h3>\n          Proper Development\n        </h3>\n        <p>\n          There are a number of common gotchas involved with the proper development and deployment of differentially private deep learning models.\n        </p>\n        <p>\n          The biggest is hyperparameter search and model selection. You might think intuitively that, as long as you implement and execute something like DP-SGD, you can sleep soundly at night knowing your model is privacy preserving. Although, typically many models are trained in the process of during the hyperparameter . Although in theory, the hyperparameters you{\"\\'\"}re selecting are technically data-inspired, and hence indirectly leak information. Frankly this detail is swept under the rug in research contexts. But in industry, this absolutely needs to be taken into consideration, or at the very least consciously acknowledged as a risk.\n        </p>\n        <p>\n          The second is proper evaluation. It does not suffice to simply train a single model and report its accuracy. Recall that\n        </p>\n*/\n\n\nfunction Post() {\n    return (\n      <>\n        <h2>\n          A Guide to Differentially Private Deep Learning\n        </h2>\n        <p>\n          In this post we tackle the topic of privacy-preserving deep learning. This commentary will be less so concerned with precision and proof, and moreso geared towards convincing a deep learning practitioner what privacy should mean, why its important, and how it can be achieved.\n        </p>\n\n        <h3>\n          Differential Privacy\n        </h3>\n        <p>\n          Before talking about anything else, we need to define what privacy is. There{\"\\'\"}s a near infinite continuum of what privacy <em>could</em> mean, and naturally some definitions are less vacuous than others.\n        </p>\n        <p>\n          One such definition is <em>differential privacy</em>. Differential privacy is concerned with <em>algorithms</em>, namely functions responsible for mapping a given dataset to some output space, e.g., linear regression mapping a dataset to its coefficients <InlineMath math=\"w\" /> and <InlineMath math=\"b\" />.\n        </p>\n        <p>\n          On a high level, such an algorithm would \"preserve privacy\" under the notion of differential privacy if it were to behave (approximately) the same regardless of whether you removed any individual point from the dataset. If you could achieve this property, then you <em>should</em> be convinced that this algorithm is privacy-preserving.\n        </p>\n        <p>\n          Why? Well, let{\"\\'\"}s consider how you would feel if one of these points actually corresponded to you. By the definition put forth, you wouldn{\"\\'\"}t have grounds to care whether your data is given to the algorithm - the outcome will be the same regardless. In other words, <em>you should feel like this algorithm preserves your privacy because its outputs look the same whether or not your data is given to it</em>. This is the core idea.\n        </p>\n        <p>\n          Formally, we can express this notion via the following<sup><a href=\"https://stephentu.github.io/writeups/6885-lec20-b.pdf\">1</a></sup>:\n        </p>\n        <p><em>\n          Let <InlineMath math=\"A : \\mathcal{D} \\rightarrow \\mathcal{Y}\" /> be a randomized algorithm. We call <InlineMath math=\"A\" /> \"<InlineMath math=\"\\varepsilon\" />-differentially private\" if for all <InlineMath math=\"D_1, D_2 \\in \\mathcal{D}\" /> differing in exactly one entry, and for all outputs <InlineMath math=\"y \\in \\mathcal{Y}\" />, we have that:\n        <BlockMath math=\"e^{-\\varepsilon} \\leq \\frac{\\Pr[\\mathcal{A(D_1) = y}]}{\\Pr[\\mathcal{A(D_2) = y}]} \\leq e^\\varepsilon\" />\n        </em></p>\n        <p>\n          Upon inspection, you{\"\\'\"}ll notice that this is saying exactly what we established earlier. Namely, the probability of a particular outcome occuring is about the same whether or not you include any particular individual in the dataset.\n        </p>\n\n        <h3>\n          Deep Learning\n        </h3>\n        <p>\n          It goes without saying that deep learning has become an extremely popular form of statistical analysis. And conveniently, the algorithms of concern in the context of deep learning align perfect with the interface prescribed by differential privacy, namely in mapping provided datasets to some output space, in this context the model parameters.\n        </p>\n        <p>\n          One of the most pervasive approaches to this is the process of stochastic gradient descent. When conducting stochastic gradient descent, we iteratively update the parameters of a model by repeatedly sampling data and taking small steps over the parameters in the direction which minimizes our loss function. In other words, we repeatedly follow something resembling the following steps:\n        </p>\n        <BlockMath math=\"\\{ x^{(1)}, x^{(1)}, \\ldots x^{(b)} \\} \\sim sample(X, b)\" />\n        <BlockMath math=\"\\ell(\\theta) = \\frac{1}{b} \\sum_i \\ell(x^{(i)} ; \\theta)\" />\n        <BlockMath math=\"\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\ell(\\theta)\" />\n        <p>\n          It then begs the question as to how one would have to augment this procedure to achieve differential privacy, if at all possible.\n        </p>\n\n        <h3>\n          Differentially Private Stochastic Gradient Descent\n        </h3>\n        <p>\n          <a href=\"https://arxiv.org/abs/1607.00133\">Abadi et al.</a> detail the differentially private stochastic gradient descent (DP-SGD) algorithm to make traditional SGD yield a differential privacy guarantee. To describe it, we need to introduce a number of augmentations.\n        </p>\n        <p>\n          First, we have to augment our typical method for sampling. In the context of non-private deep learning, sampling is often achieved by shuffling the dataset and running through partitions of size <InlineMath math=\"b\" /> such that each example is viewed by the model exactly once per epoch. Although in the context of DP-SGD, we must opt for either Poisson<sup>2</sup> or uniform<sup>3</sup> subsampling if we want to retain an actual differential privacy guarantee.\n        </p>\n        <p>\n          Second, we need to augment the gradient calculatoin. In particular, we need to introduce a clipping parameter <InlineMath math=\"C\" />, an upper bound on the <InlineMath math=\"\\ell_2\" />-norm of each per-example gradient, as well as the noise multiplier <InlineMath math=\"\\sigma\" />, which scales the variance of the Gaussian noise applied to each gradient update after clipping. All of this together, we execute the following augmented training loop:\n        </p>\n        <BlockMath math=\"\\{ x^{(1)}, x^{(1)}, \\ldots x^{(b)} \\} \\sim sample(X, b)\" />\n        <BlockMath math=\"g^{(i)} \\leftarrow \\nabla_{\\theta} \\ell(x^{(i)} ; \\theta)\" />\n        <BlockMath math=\"\\bar{g}^{(i)} \\leftarrow g^{(i)} / \\max\\{ 1, ||g^{(i)}||_2 / C\\}\" />\n        <BlockMath math=\"\\tilde{g} \\leftarrow \\frac{1}{b} (\\sum_i \\bar{g}^{(i)} + \\sigma \\cdot C \\cdot \\mathcal{N}(0, I))\" />\n        <BlockMath math=\"\\theta \\leftarrow \\theta - \\eta \\tilde{g}\" />\n        <p>\n          In order to calculate the privacy loss corresponding to <InlineMath math=\"t\" /> executions of the above update rule, Abadi et al. also details the <em>moments accountant</em>, a specialized analysis capable of reporting the privacy loss over time. A full deep dive into tools used in the moments accountant is beyond the scope of this post, but it can be understood as simply a black box which takes in your training loop parameters (<InlineMath math=\"C\" />, <InlineMath math=\"\\sigma\" />, <InlineMath math=\"t\" />, etc.) and gives you <InlineMath math=\"\\varepsilon\" />. If interested in learning more, there exists a corresponding implementation within Tensorflow Privacy.\n        </p>\n\n        <h3>\n          Conlusion\n        </h3>\n        <p>\n          Hopefully this was a useful tutorial outlining differential privacy and its applications to deep learning in plain English. If you have any questions or have caught any errors, feel free to reach out!\n        </p>\n      </>\n    );\n}\n\nexport default Post;\n","import React from 'react';\n\nfunction Post() {\n  return (\n    <div>\n      <h2>\n          How Differential Privacy (Could) Fit Into Industry\n      </h2>\n      <p>\n          In the context of differential privacy, there{\"\\'\"}s typically some data curator (e.g. some silicon valley tech giant) and the outside world. Supposedly the curator has an incentive to release some data analysis result, but in such a way where the privacy leakage associated with this analysis is trackable.\n      </p>\n      <p>\n          Although, there are several practical issues with this. One which is common is this issue of an <em>infinite horizon</em>. That is, say this silicon valley tech giant wants to adhere to a strict privacy budget for the rest of eternity. When they want to release the result of an analysis, what epsilon should they choose? 1? 0.1? 0.0001? It’s unclear, especially if their goal is to stick around for eternity, and we assume that the privacy leakage associated with a given analysis is permanant.\n      </p>\n      <p>\n          What will happen? Well, once they’ve inevitably creeped up on their limit, should we expect them to seriously consider halting the release of further results forever? No - more than likely, they’ll just raise the limit. And inevitably they’ll do it again. This is to say that, in the context of entities which have no foreseeable horizon, the apparent practicality of differential privacy is of concern because the privacy expenditure is a monotonically increasing value over time.\n      </p>\n      <p>\n          This is not a problem I have a general solution to. I’m a big fan of differentially private synthetic datasets, and in certain contexts they can help in this regard. Although, not every undertaking can be easily framed as a synthetic data problem, especially if new relevant data comes in consistently.\n      </p>\n      <p>\n          Although, the core point here I’d like to make is that <em>the continual public release of results to analyses may not be the most interesting or useful context to evaluate the utility of differential privacy within</em>.\n      </p>\n      <p>\n          Instead, consider the case where an engineer at the aforementioned silicon valley tech giant accidentally leaves their laptop in the car and it gets stolen, and say they were doing some work concerning sensitive data. Can you begin to quantify the amount of damage done to the individuals included in the data they were working with? Not by default, but naturally if the results the engineer was working with were computed with differential privacy in mind, then you could actually start to get some form of a guarantee.\n      </p>\n      <p>\n          So, the slight distinction I’m making here is that maybe the utility of differential privacy is not as pronounced in contexts where the forefront goal is information release. Maybe a more useful context for differential privacy is actually behind the walls of your organization.\n      </p>\n      <p>\n          That is, the incorporation of differential privacy might be best served as a means for protection against the worst case scenario, where a data leakage happens against your will. Now the conversation shifts from saying “silicon valley tech giant, use differential privacy so that your public analyses don’t reveal too much about your users”, and it becomes “have your employees speak through the lens of differential privacy, so we know how much damage has been done in the worst case where information is leaked.”\n      </p>\n      <p>\n          This reformulation of the problem setting, in a bit of a roundabout matter, highlights the utility of differential privacy by dampening issues concerning infinite horizons, stemming from the inherent nature of data leakages. Namely, they are <em>sparse</em> and <em>unintended</em>.\n      </p>\n      <p>\n          Given that data leakages are canonically sparse, this allows you to talk about a global privacy budget per individual which might actually be useful. That is, you could actually get away with something like an epsilon of 1.0 per user over a very long timespan if data release occurs every ten years, not every day.\n      </p>\n      <p>\n          In addition, it doesn’t make sense to complain about the limitations of differential privacy with respect to its monotonically increasing nature if data release is unintended by definition - if it’s going to happen regardless, you don’t have to worry as much about the number of releases you intend to perform forever onwards because that’s not a variable you can control in the first place.\n      </p>\n      <p>\n          There are just my thoughts in isolation, and this has been said before by others. But hopefully it sparks additional discussion on the topic, on where differential privacy will make the most sense to be deployed in the real world in years to come.\n      </p>\n    </div>\n  );\n}\n\nexport default Post;\n","import React from 'react';\nimport 'katex/dist/katex.min.css';\nimport { InlineMath, BlockMath } from 'react-katex';\n\nfunction Post() {\n  return (\n    <>\n      <h2>\n          Risk-Aware Reinforcement Learning\n      </h2>\n\n      <p>\n          In this post, we will investigate the notion of risk in the context of reinforcement learning.\n      </p>\n\n      <h3>\n        Actor-Critic\n      </h3>\n      <p>\n        Actor critic methods, in my opinion, are often described in strangely confusing ways despite not being a particularly complex topic. Here we{\"\\'\"}ll offer a brief introduction.\n      </p>\n      <p>\n        First we have a notion of a critic <InlineMath math=\"\\hat{Q}: \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}\" /> which takes in a state <InlineMath math=\"s\" /> and an action <InlineMath math=\"a\" /> and produces an estimate <InlineMath math=\"\\hat{Q}(s, a)\" /> of the expected long-term reward of taking action <InlineMath math=\"a\" /> in state <InlineMath math=\"s\" />, and following the policy afterwards. To learn a good estimate of this function, we compare this estimate against a target <InlineMath math=\"Q(s, a) = r + \\gamma \\sum_{s'} p(s' | s, a) \\hat{Q}(s', a')\" /> during training, which partially incorporates ground-truth information in the form of an observed reward combined with a bootstrapped estimate. We then simply take a gradient step in the direction minimizing the difference between our computed estimate and our target.\n      </p>\n      <BlockMath math=\"\\theta_Q \\leftarrow \\theta_Q - \\eta \\nabla_{\\theta_Q} (\\hat{Q}(s, a) - Q(s, a))^2\" />\n      <p>\n        Second we introduce a notion of an actor <InlineMath math=\"A: \\mathcal{S} \\rightarrow \\mathcal{A}\" /> which simply takes in a state <InlineMath math=\"s\" /> and outputs an action <InlineMath math=\"a\" />. To quantify the quality of this predicted action, we pass <InlineMath math=\"s\" /> and <InlineMath math=\"a\" /> into the critic, which then outputs the expected long-term reward of taking <InlineMath math=\"a\" /> in <InlineMath math=\"s\" />. Assuming this estimate is decent, we would then want to update the actor in such a manner so as to maximize this value, so we take a gradient step in the direction achieving this.\n      </p>\n      <BlockMath math=\"\\theta_A \\leftarrow \\theta_A + \\eta \\nabla_{\\theta_A} \\hat{Q}(s, A(s))\" />\n      <p>\n        After alternating between training the actor and training the critic while exploring the environment, we should eventually converge on a good value function and a good actor characterizing the policy of the agent.\n      </p>\n\n      <h3>\n        What is Risk?\n      </h3>\n      <p>\n        Now, imagine that our critic didn{\"\\'\"}t just estimate the expected long-term reward, but it estimated the entire <em>distribution</em> of long term rewards. Maybe we could assume that this distribution is a Gaussian, and predict its mean and variance. Assuming we could do that, then how might we define risk?\n      </p>\n      <p>\n        Well, one way you could think about risk is that it{\"\\'\"}s <em> the degree to which you are concerned about worst-case outcomes</em>. If you are risk-averse, that means you <em>only</em> care about worst case outcomes. If you are risk-willing, then you might only care about average-case outcomes.\n      </p>\n      <p>\n        A way to formalize this notion is through <em>conditional value at risk (CVaR)</em>. Given a distribution, it is defined as the expectation of the distribution up to the <InlineMath math=\"\\alpha\" />-th percentile. If <InlineMath math=\"\\alpha \\approx 0\" />, then you essentially only care about the worst case. If <InlineMath math=\"\\alpha = 1\" />, then you only care about expected case (which would be regular actor-critic!) Anything in between is simply some domain-specific tradeoff.\n      </p>\n      <p>\n          This is the idea followed in the paper <em><a href=\"https://arxiv.org/abs/1911.03618\">Worst Case Policy Gradient</a></em> by Tang et al., and the basis for this post.\n      </p>\n\n      <h3>\n        Training the Critic\n      </h3>\n      <p>\n          Getting the estimated means and variances from your model is fairly straightforward - just feedforward your critic model! Give it a state and an action and it will spit out the estimated mean and variance.\n      </p>\n      <BlockMath math=\"\\{ \\hat{Q}(s, a), \\hat{\\Upsilon}(s, a) \\} = critic(s, a)\" />\n      <p>\n          Now how will we attain good estimates for <InlineMath math=\"\\hat{Q}(s, a)\" /> and <InlineMath math=\"\\hat{\\Upsilon}(s, a)\" />? By the same idea of computing targets given observed reward information. Fist, the mean is actually quite easy because it has precisely the same semantic interpretation of the regular Q-value, hence going unchanged.\n      </p>\n      <BlockMath math=\"Q(s, a) = r + \\gamma \\sum_{s'} p(s' | s, a) \\hat{Q}(s', a')\" />\n      <p>\n          Now we just need to sort out what the proper variance target should look like. By simply expanding the definition of variance, you should end up with an expression resembling the following.\n      </p>\n      <BlockMath math=\"\\Upsilon(s, a) \\leftarrow r^2 + 2 \\gamma r \\sum_{s} p(s' | s, a) \\hat{Q}(s', a')\" />\n      <BlockMath math=\"+ \\gamma^2 \\sum_{s'} p(s' | s, a) \\hat{\\Upsilon}(s', a') + \\gamma^2 \\sum_{s'} p(s' | s, a) \\hat{Q}(s', a')^2 - \\hat{Q}(s, a)^2\" />\n      <p>\n          Now we have our <em>estimates</em>, namely <InlineMath math=\"\\hat{Q}(s, a)\"/> and <InlineMath math=\"\\hat{\\Upsilon}(s, a)\"/>, as well as our <em>targets</em>, namely <InlineMath math=\"Q(s, a)\"/> and <InlineMath math=\"\\Upsilon(s, a)\"/>.\n      </p>\n      <p>\n        Now we have to construct a loss function which will quantify how good our estimates are. Why not define our loss function as some statistical distance metric between the Gaussian distributions characterized by our estimate and target? It turns out, the Wasserstein distance between the two Gaussians can be characterized by the following expression.\n      </p>\n      <BlockMath math=\"\\ell(\\theta) = (\\hat{Q}(s, a) - Q(s, a))^2\" />\n      <BlockMath math=\"+ \\hat{\\Upsilon}(s, a) + \\Upsilon(s, a) - 2\\sqrt{\\hat{\\Upsilon}(s, a) \\Upsilon(s, a)}\" />\n      <p>\n        This expression should satisfy our intuition - the loss is zero when <InlineMath math=\"\\hat{Q}(s, a) = Q(s, a)\" /> and <InlineMath math=\"\\hat{\\Upsilon}(s, a) = \\Upsilon(s, a)\" />, and positive otherwise.\n      </p>\n\n      <h3>\n        Training the Actor\n      </h3>\n      <p>\n        Now how should we train our actor? This part is easy assuming we have a trained critic. Just as before, we pass in a state <InlineMath math=\"s\" /> to our actor which will give us an action <InlineMath math=\"a\" />. This action is then passed into the critic, which yields a mean <InlineMath math=\"\\hat{Q}(s, a)\" /> and a variance <InlineMath math=\"\\hat{\\Upsilon}(s, a)\" />. Given this mean and variance, we can directly calculate the <emph>CVaR</emph> metric as the following where <InlineMath math=\"\\phi(\\alpha)\" /> is the PDF of the Gaussian distribution evaluated at <InlineMath math=\"\\alpha\" /> and <InlineMath math=\"\\Phi(\\alpha)\" /> is the CDF.\n      </p>\n      <BlockMath math=\"\\Gamma(s, a, \\alpha) = \\hat{Q}(s, a) - \\frac{\\phi(\\alpha)}{\\Phi(\\alpha)}\\sqrt{\\hat{\\Upsilon}(s, a)}\" />\n      <p>\n        This is the scalar we want! Now, the actor just takes a gradient step in the direction <em>maximizing</em> this value. Super simple!\n      </p>\n\n      <h3>\n        Conclusion\n      </h3>\n      <p>\n        Hopefully this acted as a simple introduction to risk in the context of reinforcement learning. If you have any questions, feel free to shoot them my way!\n      </p>\n    </>\n  );\n}\n\nexport default Post;\n","import React from 'react';\nimport 'katex/dist/katex.min.css';\nimport { InlineMath, BlockMath } from 'react-katex';\nimport InstagramEmbed from 'react-instagram-embed';\n\nfunction Post() {\n  return (\n    <>\n      <h2>\n          Deaths of Despair\n      </h2>\n      <p>\n      <em>\n          Ecclesiastes 1:9. What has been will be again, what has been done will be done again; there is nothing new under the sun.\n      </em>\n      </p>\n      <p>\n          I believe this is true in one sense, but false in another. Have these things been echoed in other forms in the past? Absolutely. But are these manifestations becoming increasingly more potent? I would argue so.\n      </p>\n      <p>\n          In the past, you at least had some degree of separation between you and the elite. You can realize this in a few ways, but the way I think the most clearly about is the following. That is, on average, the amount of time these individuals occupy your direct attention, with some implicit weight on subconscious attention.\n      </p>\n      <p>\n          Let’s take the median American who makes around $33,706 a year (2018). Now, convince yourself there exists some probability, no matter how small, that such a person exhibits a chance of breaking down upon seeing such a photo after a long and hard day at work.\n      </p>\n      <p>\n          How many times a day do you think that coin is flipped? There are 350 million americans. Wouldn’t you like to be him? He looks well rested, and if he were to go up to any of his million fans, he would be undoubtedly praised like a living god.\n      </p>\n\n      <h3>\n          Wealth Inequality\n      </h3>\n      <p>\n          I see Amazon as a useful analog. If you had to distill America{\"\\'\"}s problem, it could be physically manifested as Amazon{\"\\'\"}s structure. Essentially, we have a clear balkanization and class structure. Which is, we have the elite which make decisions and live way more well off than necessary, the engineers, the lower class who are treated unreasonably poorly, and the hyper-elite Jeff Bezos money who is more valuable than is really coprehensible.\n      </p>\n      <p>\n          At the time of writing this, Jeff Bezos is worth approximately 180 billion dollars. To conceptualize this, consider a billionaire. To become a billionaire, you would have had to have had a 4 million dollar salary every year throughout all of American history, which is an absolutely ridiculous salary to have persisted through the civil war, the invention of the cotton gin, throught the civil rights movement, until now. That is poverty in relation to Jeff Bezos.\n      </p>\n      <p>\n          Now, the problem per say clearly isn{\"\\'\"}t that automation or wealth inequality is new in some sense, this is clearly false. But I would argue that a problem which rapidly exacerbates is indistinguishable from a new problem, and that this principle applies in this context. Of course, every time a new technology comes around it ripples throughout the economy. But the problem now is that the proportions of the population our innovations are having are growing, and the skills they undermine require more and more prior investment. Of course, when the cotton gin came around people surely lost their jobs, but finding other blue collar work was an alternative and learning the upkeep a cotton gin, which required more skill than picking the cotton itself, took an investment on the order of days. Now, we{\"\\'\"}re talking about technology which could potentially undermine manual labor all together, and using methods which require so much more skill, at an unprecedented speed.\n      </p>\n      <p>\n          The key difference is the following key metric. What proportion of outsourced workers were able to actually contribute or play a part in the technology which outsourced them? With the cotton gin, I would argue that that number was modestly high. That is, the skills you had to be a picker of cotton weren{\"\\'\"}t too far off from maintaining a cotton gin or thinking of new ideas to improve it, it was a mechanical contraption after all. Of course not everyone would exhibit that effort and might simply switch to another industry, but the possibility was there. What proportion of truck drivers outsourced will be able to contribute or play a part in self-driving car technology? Literally zero percent.\n      </p>\n      <p>\n          So lets say you think well get past the self driving truck issue, which id agree we probably will at some cost. Well, again, this is getting worse. Whats the next large scale technological innovation to be had? Almost surey the end of menial white collar work requiring basic linguistic ability and reasoning skills, aka lawyers, accountants and the like. What are we going to tell 40 year old men with kids and a mortgage? Especially when the maintenance of this software requires a team of three engineers? Im sorry but theres no jobs available in this context - midway through your career you might just need to learn how to be a pharmacologist. Its almost in some sense like our efficiency and problem solving ability has become too low of a barrier to entry for our own good.\n      </p>\n      <p>\n          Again, my argument is not that self driving cars will be the end of civilization. Its that the implications of technological innovation are getting worse. If you think this round of innovation isn{\"\\'\"}t sufficient, wait 100 years. What will we be outsourcing then?\n      </p>\n      <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/G1UpFHsbOf0?start=1745\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n\n      <h3>\n          Social Media\n      </h3>\n\n      <h4>\n          Twitch\n      </h4>\n      <p>\n        I can{\"\\'\"}t say my response to such incidents better than it was said by /u/Thopterthallid in <a href=\"https://www.reddit.com/r/AskReddit/comments/ca0gm6/people_who_ordered_belle_delphines_bathwater_why/et5pesf?utm_source=share&utm_medium=web2x\">this</a> comment.\n      </p>\n\n      <p>\n        <details>\n        <summary><i>I want to try and break \"Gamer girl water\" down for people...</i></summary>\n        <i>\n        ...coming from someone who had a dark time in my life where I might have been tempted to buy some \"gamer girl bath water\". It{\"\\'\"}s easy to say \"Oh she{\"\\'\"}s just tricking idiots out of their money\" but it{\"\\'\"}s a little more complicated than that.<br/>\n        <br/>\n          <b>When you{\"\\'\"}re approaching your mid to late 20s and still haven{\"\\'\"}t enjoyed a healthy and intimate relationship, it really does start to fuck with you on a scary level.</b> Mental illness, depression, anxiety, and such seem to be a much more prevalent nowadays since it{\"\\'\"}s so easy to get \"quick fix\" social interaction through the internet. The internet is amazing, but it doesn{\"\\'\"}t make you better at actually being a social human being in real life. You get guys who had quick and easy access to a hundred friends on MSN/AIM/Skype/Discord/Telegram grow increasingly dependant on online interactions for their social needs. I can confirm it gets to the point where you just don{\"\\'\"}t even really want real interactions anymore because pornhub handles your libido, twitch gives you someone to listen to, instant messaging gives you people to talk to, etc. The only thing missing is the sexual physical touch of a woman, and there{\"\\'\"}s a part of you that might think that water that{\"\\'\"}s touched your Twitch crush will satisfy that. It won{\"\\'\"}t of course.<br/>\n        <br/>\n          <b>The way that the most popular Youtubers, and Twitch Streamers interact with their audience is that of being a friend, family, or lover.</b> How many streamers call their viewers some kind of pet name? Like, the [Streamer name] Family, or the [Streamer name] Army. When your livelihood comes from donations and views, you need to be especially charismatic. The biggest names in online video makers always talk directly to their audience. They look into the camera, they call you their friends and family, they want you to hang out with them at conventions, and they want to hear your comments, they want to read your messages in chat, they offer life advice, they tell you they care about you, they tell you they{\"\\'\"}re thankful for you, they want you to take care of yourself, and often times they{\"\\'\"}ll open up their soul and talk about their lives to you. It{\"\\'\"}s all about charisma and being a good host and it{\"\\'\"}s a great talent to have. For people in the previous point, this can actually be super harmful. You spend enough time in their streams and watching their videos, and you really do start to subconsciously believe that Markiplier, Pewdiepie, and Summit are your friends. Obviously it{\"\\'\"}s not the case, but there was a time in my life where some of my favourite Youtubers were appearing in my dreams and were asking me to hang out, and my brain didn{\"\\'\"}t even question that it was something we just did all the time. I recall a few years ago some kids and their mom showed up to a famous Twitch streamer{\"\\'\"}s address and were confused as to why he felt that was a breach of privacy. They expected to hang out with him, get autographs, play games with him, and all around expect that he{\"\\'\"}d be thrilled to see them. Obviously, this is a huge breach of boundaries, and perfectly sane people just forget that these people don{\"\\'\"}t have a 1:1 friendship with them.<br/>\n        <br/>\n          <b>When you{\"\\'\"}re 25+ and haven{\"\\'\"}t had meaningful relationships with women, either romantic or otherwise, you start to get weird ideas about what women are, especially if you talk to the wrong groups.</b> When you get to the point where you{\"\\'\"}ve craved intimate love and never had it for over ten years, you start getting weird ideas. Some men lash out at women and get incel-ish. Some men put women on these pedestals as divine trophies to be won. Some men just assume that women will never like them. But the longer you go without ever knowing love, the easier it is to think of women as being a totally different species.<br/>\n        <br/>\n          <b>When every woman you{\"\\'\"}ve ever known either hates video games or are casual/novice gamers at best, meeting a woman who seems to idolize gaming as much as you do triggers volatile emotions.</b> Sometimes it comes in the form of disdain and skepticism, and end up making a toxic environment for women in gaming. If not that, it comes out as excessive idolization. You{\"\\'\"}re at a point in your life where any woman who would even pay attention to you is a dream, and the thought of a girl who would share your favourite hobby with you may as well be the second coming of Christ. You crush on her hard, you buy all her merch, you send her donations so that she{\"\\'\"}ll start to recognize your name and tell you \"thank you\" and read your messages. I{\"\\'\"}m not saying that women shouldn{\"\\'\"}t be allowed to be successful gaming streamers, but I do think that taking advantage of clinically depressed and lonely fans like this is awful.<br/>\n        <br/>\n          <b>This whole, selling bathwater thing to thirsty fans may seem like memes and trolling, but in reality it{\"\\'\"}s just highlighting a very, very sad reality.</b> There{\"\\'\"}s guys out there who would buy this in a futile attempt just to feel closer to the one girl that ever paid them any attention. I was dangerously close to being one of those guys at one point and I know just how scary the spiral goes. People on my path would either end up doing things like buying this bathwater or becoming misogynistic incels.<br/>\n        <br/>\n          It all sounds so stupid and pathetic, but it{\"\\'\"}s a very true reality for some unfortunate guys suffering from severe depression.\n        </i>\n        </details>\n        <br/>\n      </p>\n\n      <h4>\n          Tinder\n      </h4>\n\n      <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/2O-iLk1G_ng?start=3103\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n\n      <InstagramEmbed url='https://instagr.am/p/Bfw2OPinqme/' maxWidth={320} hideCaption={true} />\n      <InstagramEmbed url='https://instagr.am/p/BoPAPVKAnS0/' maxWidth={320} hideCaption={true} />\n\n      <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/PMotykw0SIk?start=1282\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n    </>\n  );\n}\n\nexport default Post;\n","import React from 'react';\nimport 'katex/dist/katex.min.css';\nimport { InlineMath, BlockMath } from 'react-katex';\n\nfunction Post() {\n  return (\n    <>\n      <h2>\n        Where We Are Now\n      </h2>\n      <h3>\n        The Past\n      </h3>\n      <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/WYaluOHcATU?start=88\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n      <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/l6i-gYRAwM0\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n\n      <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/NRCWbFFRpnY\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n\n      <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Rm6xL0klXcQ\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n\n      <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/iTACH1eVIaA\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n\n      <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/ec9P3C1OXqE\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n\n      <p>\n        <em>\n          \"He's got a great personality. He's a funny guy, he's very smart, he's a great negotiator. He loves his people, not that I'm surprised by that. I think that we have the start of an amazing deal.\" - Donald Trump\n        </em>\n      </p>\n\n      <p>\n        <em>\n          \"I've known Jeff for 15 years. Terrific guy. He's a lot of fun to be with. It is even said that he likes beautiful women as much as I do, and many of them are on the younger side.\" - Donald Trump\n        </em>\n      </p>\n    </>\n  );\n}\n\nexport default Post;\n","import './App.css';\n\nimport Page from './components/page';\nimport React from 'react';\nimport { Switch, Route, BrowserRouter } from 'react-router-dom';\n\nimport About from './pages/about';\nimport Research from './pages/research';\nimport Writing from './pages/writing';\nimport Reading from './pages/reading';\nimport Design from './pages/design';\n\nimport MachineUnlearning from './posts/machine-unlearning';\nimport DifferentiallyPrivateDeepLearning from './posts/differentially-private-deep-learning';\nimport HowDifferentialPrivacyFitsIntoIndustry from './posts/how-differential-privacy-fits-into-industry';\nimport RiskAwareReinforcementLearning from './posts/risk-aware-reinforcement-learning';\nimport DeathsOfDespair from './posts/deaths-of-despair';\nimport WhereWeAreNow from './posts/where-we-are-now';\n\nfunction App() {\n  return (\n    <Page>\n      <Route exact path='/' component={About}/>\n\n      <Route path='/about' component={About}/>\n      <Route path='/research' component={Research}/>\n      <Route path='/writing' component={Writing}/>\n      <Route path='/reading' component={Reading}/>\n      <Route path='/design' component={Design}/>\n\n      <Route path='/deaths-of-despair' component={DeathsOfDespair} />\n      <Route path='/where-we-are-now' component={WhereWeAreNow} />\n      <Route path='/how-differential-privacy-fits-into-industry' component={HowDifferentialPrivacyFitsIntoIndustry} />\n      <Route path='/differentially-private-deep-learning' component={DifferentiallyPrivateDeepLearning} />\n      <Route path='/risk-aware-reinforcement-learning' component={RiskAwareReinforcementLearning} />\n      <Route path='/machine-unlearning' component={MachineUnlearning} />\n    </Page>\n  );\n}\n\nexport default App;\n","import './index.css';\nimport * as serviceWorker from './serviceWorker';\nimport App from './App';\nimport React from 'react';\nimport ReactDOM from 'react-dom';\nimport { HashRouter } from 'react-router-dom';\n\nReactDOM.render((\n  <HashRouter>\n    <App/>\n  </HashRouter>\n  ), document.getElementById('root')\n);\n\n// If you want your app to work offline and load faster, you can change\n// unregister() to register() below. Note this comes with some pitfalls.\n// Learn more about service workers: https://bit.ly/CRA-PWA\nserviceWorker.unregister();\n"],"sourceRoot":""}