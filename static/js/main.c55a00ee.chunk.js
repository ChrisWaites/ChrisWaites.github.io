(this["webpackJsonppersonal-site"]=this["webpackJsonppersonal-site"]||[]).push([[0],{13:function(e,t,a){},28:function(e,t,a){e.exports=a(41)},29:function(e,t,a){},41:function(e,t,a){"use strict";a.r(t);a(29),Boolean("localhost"===window.location.hostname||"[::1]"===window.location.hostname||window.location.hostname.match(/^127(?:\.(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)){3}$/));a(13);var n=a(0),i=a.n(n),r=a(23),o=a(10);var l=function(e){return i.a.createElement("div",{style:{display:"flex",alignItems:"flex-start",justifyContent:"center"}},i.a.createElement("div",{style:{padding:"60px",width:"40%",textAlign:"left"}},e.children),i.a.createElement(r.a,{style:{paddingTop:"60px",textAlign:"right"}},i.a.createElement("table",null,i.a.createElement("tr",null,i.a.createElement("td",null,i.a.createElement("strong",null,i.a.createElement(o.b,{className:"link",to:"/"},"Chris Waites")))),i.a.createElement("tr",null,i.a.createElement("td",null,i.a.createElement(o.b,{className:"link",to:"/research"},"Research"))),i.a.createElement("tr",null,i.a.createElement("td",null,i.a.createElement(o.b,{className:"link",to:"/writing"},"Writing"))),i.a.createElement("tr",null,i.a.createElement("td",null,i.a.createElement(o.b,{className:"link",to:"/reading"},"Reading"))),i.a.createElement("tr",null,i.a.createElement("td",null,i.a.createElement(o.b,{className:"link",to:"/design"},"Design"))),i.a.createElement("tr",null,i.a.createElement("td",null,i.a.createElement("a",{href:"https://github.com/ChrisWaites"},"Github"))))))},s=a(3),h=a(54);function c(e){return i.a.createElement("p",null,i.a.createElement(h.a,{variant:"outlined",href:e.href},e.children))}var m=function(){return i.a.createElement(i.a.Fragment,null,i.a.createElement(c,{href:"https://invertibleworkshop.github.io/accepted_papers/pdfs/41.pdf"},i.a.createElement("em",null,"Differentially Private Normalizing Flows for Privacy-Preserving Density Estimation"),i.a.createElement("br",null),i.a.createElement("hr",null),"Chris Waites and Rachel Cummings.",i.a.createElement("br",null),i.a.createElement("em",null,"ICML Workshop on Invertible Neural Networks, Normalizing Flows, and Explicit Likelihood Models.")),i.a.createElement(c,{href:"https://arxiv.org/abs/1912.03250"},i.a.createElement("em",null,"Differentially Private Mixed-Type Data Generation For Unsupervised Learning"),i.a.createElement("br",null),i.a.createElement("hr",null),"Uthaipon Tantipongpipat*, Chris Waites*, Digvijay Boob, Amaresh Ankit Siva, and Rachel Cummings.",i.a.createElement("br",null),i.a.createElement("em",null,"NeurIPS 2020.")))};var u=function(){return i.a.createElement(i.a.Fragment,null,i.a.createElement("p",null,i.a.createElement("img",{src:"./academic-banner.jpeg",style:{borderRadius:"10px",width:"100%"}})),i.a.createElement("p",null,"Hi! Im Chris."),i.a.createElement("p",null,"Currently, I","'","m doing my M.S. in Computer Science at ",i.a.createElement("a",{href:"https://www.stanford.edu/"},"Stanford University"),". Previously, I did my B.S. in Computer Science at the Georgia Institute of Technology."),i.a.createElement("h3",null,"Research"),i.a.createElement(m,null))};function d(e){return i.a.createElement("p",null,i.a.createElement(h.a,{variant:"outlined",href:e.href},i.a.createElement("em",null,e.children)))}var p=function(){return i.a.createElement(i.a.Fragment,null,i.a.createElement(d,{href:"/writing/deaths-of-despair"},"Deaths of Despair"),i.a.createElement(d,{href:"/writing/risk-aware-reinforcement-learning"},"Risk-Aware Reinforcement Learning"),i.a.createElement(d,{href:"/writing/how-differential-privacy-fits-into-industry"},"How Differential Privacy (Could) Fit Into Industry"),i.a.createElement(d,{href:"/writing/differentially-private-deep-learning"},"A Guide to Differentially Private Deep Learning"))};function f(e){return i.a.createElement("p",null,i.a.createElement(h.a,{variant:"outlined",href:e.href},i.a.createElement("em",null,e.children)))}var y=function(){return i.a.createElement(i.a.Fragment,null,i.a.createElement(f,{href:"https://law.stanford.edu/wp-content/uploads/2019/01/Bellovin_20190129.pdf"},"Privacy and Synthetic Datasets"),i.a.createElement(f,{href:"https://cset.georgetown.edu/wp-content/uploads/Keeping-Top-AI-Talent-in-the-United-States.pdf"},"Keeping Top AI Talent in the United States"),i.a.createElement(f,{href:"http://web.stanford.edu/class/psych209/Readings/LakeEtAlBBS.pdf"},"Building Machines That Learn and Think Like People"),i.a.createElement(f,{href:"https://arxiv.org/abs/1911.09421"},"The Linear Algebra Mapping Problem"),i.a.createElement(f,{href:"https://arxiv.org/abs/1905.02175"},"Adversarial Examples Are Not Bugs, They Are Features"),i.a.createElement(f,{href:"https://arxiv.org/abs/2003.03384"},"AutoML-Zero: Evolving Machine Learning Algorithms From Scratch"))};var g=function(){return i.a.createElement(i.a.Fragment,null,i.a.createElement("p",null,i.a.createElement("img",{src:"./normalizing-flows.png",style:{borderRadius:"10px",width:"100%"}})),i.a.createElement("p",null,i.a.createElement("img",{src:"./hoover.gif",style:{borderRadius:"10px",width:"100%"}})))},w=(a(17),a(2));var b=function(){return i.a.createElement(i.a.Fragment,null,i.a.createElement("h2",null,"A Guide to Differentially Private Deep Learning"),i.a.createElement("p",null,"In this post we review differential privacy, the task of deep learning, what privacy means in this context, and reference current results in this area."),i.a.createElement("h3",null,"Differential Privacy"),i.a.createElement("p",null,"Finding a definition which fully satisfies one\u2019s intuitive understanding of privacy is surprisingly tricky. One such definition is ",i.a.createElement("em",null,"differential privacy"),". On a high level, the idea behind differential privacy is that given a randomized algorithm which performs some statistical task on subsets of a dataset, such an algorithm would \u201cpreserve privacy\u201d if it behaved approximately the same regardless of the inclusion or exclusion of any individual in the subset it was acting on. That is, the data of each entry would be thought to be hidden since the behavior of the algorithm closely resembles every possible case where the entry would not have been included."),i.a.createElement("p",null,"Formally, this is expressed as the following",i.a.createElement("sup",null,i.a.createElement("a",{href:"https://stephentu.github.io/writeups/6885-lec20-b.pdf"},"1")),":"),i.a.createElement("h3",null,"Deep Learning"),i.a.createElement("p",null,"Deep learning is currently one of the most predominant forms of statistical analysis used today and has been shown to be remarkably effective for a variety of tasks. Deep neural networks, in their standard form, define a function composed of a sequence of layers where each layer represents an operation to be performed on the output of the previous layer. Typically the goal associated with such models is to find the set of parameters which map a set of inputs to a set of outputs in a way which minimizes some function, referred to as the loss function."),i.a.createElement(w.BlockMath,{math:"\\{ x^{(1)}, x^{(1)}, \\ldots x^{(b)} \\} \\sim sample(X, b)"}),i.a.createElement(w.BlockMath,{math:"\\ell(\\theta) = \\frac{1}{b} \\sum_i \\ell(x^{(i)} ; \\theta)"}),i.a.createElement(w.BlockMath,{math:"\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\ell(\\theta)"}),i.a.createElement("p",null,"A popular method for finding such parameters is via a process of stochastic gradient descent. When conducting stochastic gradient descent, one iteratively updates the parameters of the model by sampling an individual input-output pair from the dataset and partially applying their values to the error function so that the gradient of the error with respect to the parameters of the model can be computed. Then, one would update the parameters of the model in the direction opposite of the gradient, in turn minimizing the error function with respect to that example. Formally, if we let \u03b80 be the randomly initialized parameters of the model, \u03b8t be the parameters of the model at iteration t, (xt, yt) be our sampled input-output pair, L be our error function, and \u03b7t be the learning rate, we iteratively apply the following update rule:"),i.a.createElement("p",null,"Although, it\u2019s more common in practice to opt for minibatch gradient descent. Rather than calculating gradients with respect to individual examples, one uniformly samples a subset of B examples without replacement, calculates the gradient with respect to each example, and applies the average of the gradients to the model. This corresponds to the following update rule:"),i.a.createElement("h3",null,"Differentially Private Stochastic Gradient Descent"),i.a.createElement("p",null,i.a.createElement("a",{href:"https://arxiv.org/abs/1607.00133"},"Abadi et al.")," detail the differentially private stochastic gradient descent (DP-SGD) algorithm to make traditional SGD yield a differential privacy guarantee. To describe it, we need to introduce a number of augmentations."),i.a.createElement("p",null,"First, we have to augment our typical method for sampling examples from the dataset. In the context of non-private deep learning, sampling is often achieved by shuffling the dataset and running through partitions of size ",i.a.createElement(w.InlineMath,{math:"b"})," such that each example is viewed by the model exactly once per epoch. In the context of DP-SGD, we have to opt for either Poisson",i.a.createElement("sup",null,"2")," or uniform",i.a.createElement("sup",null,"3")," subsampling if we want a differential privacy guarantee."),i.a.createElement("p",null,"Second, we need to introduce a number of additional training parameters. In particular, we need to introduce a clipping parameter ",i.a.createElement(w.InlineMath,{math:"C"}),", an upper bound on the ",i.a.createElement(w.InlineMath,{math:"\\ell_2"}),"-norm of each per-example gradient, as well as the noise multiplier ",i.a.createElement(w.InlineMath,{math:"\\sigma"}),", which acts as the ratio between the clipping parameter and variance of the Gaussian noise applied to each gradient update after clipping."),i.a.createElement(w.BlockMath,{math:"\\{ x^{(1)}, x^{(1)}, \\ldots x^{(b)} \\} \\sim sample(X, b)"}),i.a.createElement(w.BlockMath,{math:"g^{(i)} \\leftarrow \\nabla_{\\theta} \\ell(x^{(i)} ; \\theta)"}),i.a.createElement(w.BlockMath,{math:"\\bar{g}^{(i)} \\leftarrow g^{(i)} / \\max\\{ 1, ||g^{(i)}||_2 / C\\}"}),i.a.createElement(w.BlockMath,{math:"\\tilde{g} \\leftarrow \\frac{1}{b} (\\sum_i \\bar{g}^{(i)} + \\sigma C \\mathcal{N}(0, I))"}),i.a.createElement(w.BlockMath,{math:"\\theta \\leftarrow \\theta - \\eta \\tilde{g}"}),i.a.createElement("p",null,"In order to calculate the privacy loss corresponding to k executions of the above update rule, Abadi et al. detail the moments accountant as a method to report privacy loss over time. A full deep dive into the foundations backing the moments accountant are likely outside the scope of this post, but on a high level it can be thought of as a black box which takes in values which characterize your training loop (sampling probabilities, number of minibatches, delta, etc.) and outputs epsilon. But importantly, their method yields much tighter bounds on the privacy loss achieved than what is reported via the strong composition theorem. If interested in learning more, the algorithm was originally introduced in Abadi et al. and has a corresponding implementation within Tensorflow Privacy."),i.a.createElement("h3",null,"Private Aggregation of Teacher Ensembles"),i.a.createElement("p",null,"Private Aggregation of Teacher Ensembles or PATE is an alternative method to DPSGD for conducting differentially private learning. The key idea is to, rather than train a single strong model which captures a complex criterion in a differentially private manner, train a set of weaker, non-private models on partitions of the data and then perform a noisy aggregation of their predictions. Overall the method has been shown to be quite effective at the expense of some assumptions about the training procedure. There has even been an application of this technique to GANs via PATE-GAN."),i.a.createElement("h3",null,"Differentially Private Federated Learning"),i.a.createElement("h3",null,"Proper Development"),i.a.createElement("p",null,"There are a number of common gotchas involved with the proper development and deployment of differentially private deep learning models."),i.a.createElement("p",null,"The biggest is hyperparameter search and model selection. You might think intuitively that, as long as you implement and execute something like DP-SGD, you can sleep soundly at night knowing your model is privacy preserving. Although, typically many models are trained in the process of during the hyperparameter . Although in theory, the hyperparameters you","'","re selecting are technically data-inspired, and hence indirectly leak information. Frankly this detail is swept under the rug in research contexts. But in industry, this absolutely needs to be taken into consideration, or at the very least consciously acknowledged as a risk."),i.a.createElement("p",null,"The second is proper evaluation. It does not suffice to simply train a single model and report its accuracy. Recall that"))};var v=function(){return i.a.createElement("div",null,i.a.createElement("h2",null,"How Differential Privacy (Could) Fit Into Industry"),i.a.createElement("p",null,"In the context of differential privacy, there","'","s typically some data curator (e.g. some silicon valley tech giant) and the outside world. Supposedly the curator has an incentive to release some data analysis result, but in such a way where the privacy leakage associated with this analysis is trackable."),i.a.createElement("p",null,"Although, there are several practical issues with this. One which is common is this issue of an ",i.a.createElement("em",null,"infinite horizon"),". That is, say this silicon valley tech giant wants to adhere to a strict privacy budget for the rest of eternity. When they want to release the result of an analysis, what epsilon should they choose? 1? 0.1? 0.0001? It\u2019s unclear, especially if their goal is to stick around for eternity, and we assume that the privacy leakage associated with a given analysis is permanant."),i.a.createElement("p",null,"What will happen? Well, once they\u2019ve inevitably creeped up on their limit, should we expect them to seriously consider halting the release of further results forever? No - more than likely, they\u2019ll just raise the limit. And inevitably they\u2019ll do it again. This is to say that, in the context of entities which have no foreseeable horizon, the apparent practicality of differential privacy is of concern because the privacy expenditure is a monotonically increasing value over time."),i.a.createElement("p",null,"This is not a problem I have a general solution to. I\u2019m a big fan of differentially private synthetic datasets, and in certain contexts they can help in this regard. Although, not every undertaking can be easily framed as a synthetic data problem, especially if new relevant data comes in consistently."),i.a.createElement("p",null,"Although, the core point here I\u2019d like to make is that ",i.a.createElement("em",null,"the continual public release of results to analyses may not be the most interesting or useful context to evaluate the utility of differential privacy within"),"."),i.a.createElement("p",null,"Instead, consider the case where an engineer at the aforementioned silicon valley tech giant accidentally leaves their laptop in the car and it gets stolen, and say they were doing some work concerning sensitive data. Can you begin to quantify the amount of damage done to the individuals included in the data they were working with? Not by default, but naturally if the results the engineer was working with were computed with differential privacy in mind, then you could actually start to get some form of a guarantee."),i.a.createElement("p",null,"So, the slight distinction I\u2019m making here is that maybe the utility of differential privacy is not as pronounced in contexts where the forefront goal is information release. Maybe a more useful context for differential privacy is actually behind the walls of your organization."),i.a.createElement("p",null,"That is, the incorporation of differential privacy might be best served as a means for protection against the worst case scenario, where a data leakage happens against your will. Now the conversation shifts from saying \u201csilicon valley tech giant, use differential privacy so that your public analyses don\u2019t reveal too much about your users\u201d, and it becomes \u201chave your employees speak through the lens of differential privacy, so we know how much damage has been done in the worst case where information is leaked.\u201d"),i.a.createElement("p",null,"This reformulation of the problem setting, in a bit of a roundabout matter, highlights the utility of differential privacy by dampening issues concerning infinite horizons, stemming from the inherent nature of data leakages. Namely, they are ",i.a.createElement("em",null,"sparse")," and ",i.a.createElement("em",null,"unintended"),"."),i.a.createElement("p",null,"Given that data leakages are canonically sparse, this allows you to talk about a global privacy budget per individual which might actually be useful. That is, you could actually get away with something like an epsilon of 1.0 per user over a very long timespan if data release occurs every ten years, not every day."),i.a.createElement("p",null,"In addition, it doesn\u2019t make sense to complain about the limitations of differential privacy with respect to its monotonically increasing nature if data release is unintended by definition - if it\u2019s going to happen regardless, you don\u2019t have to worry as much about the number of releases you intend to perform forever onwards because that\u2019s not a variable you can control in the first place."),i.a.createElement("p",null,"There are just my thoughts in isolation, and this has been said before by others. But hopefully it sparks additional discussion on the topic, on where differential privacy will make the most sense to be deployed in the real world in years to come."))};var E=function(){return i.a.createElement(i.a.Fragment,null,i.a.createElement("h2",null,"Risk-Aware Reinforcement Learning"),i.a.createElement("p",null,"In this post, we will investigate the notion of risk in the context of reinforcement learning."),i.a.createElement("h3",null,"Actor-Critic"),i.a.createElement("p",null,"Actor critic methods, in my opinion, are often described in strangely confusing ways despite not being a particularly complex topic. Here we","'","ll offer a brief introduction."),i.a.createElement("p",null,"First we have a notion of a critic ",i.a.createElement(w.InlineMath,{math:"\\hat{Q}: \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}"})," which takes in a state ",i.a.createElement(w.InlineMath,{math:"s"})," and an action ",i.a.createElement(w.InlineMath,{math:"a"})," and produces an estimate ",i.a.createElement(w.InlineMath,{math:"\\hat{Q}(s, a)"})," of the expected long-term reward of taking action ",i.a.createElement(w.InlineMath,{math:"a"})," in state ",i.a.createElement(w.InlineMath,{math:"s"}),", and following the policy afterwards. To learn a good estimate of this function, we compare this estimate against a target ",i.a.createElement(w.InlineMath,{math:"Q(s, a) = r + \\gamma \\sum_{s'} p(s' | s, a) \\hat{Q}(s', a')"})," during training, which partially incorporates ground-truth information in the form of an observed reward combined with a bootstrapped estimate. We then simply take a gradient step in the direction minimizing the difference between our computed estimate and our target."),i.a.createElement(w.BlockMath,{math:"\\theta_Q \\leftarrow \\theta_Q - \\eta \\nabla_{\\theta_Q} (\\hat{Q}(s, a) - Q(s, a))^2"}),i.a.createElement("p",null,"Second we introduce a notion of an actor ",i.a.createElement(w.InlineMath,{math:"A: \\mathcal{S} \\rightarrow \\mathcal{A}"})," which simply takes in a state ",i.a.createElement(w.InlineMath,{math:"s"})," and outputs an action ",i.a.createElement(w.InlineMath,{math:"a"}),". To quantify the quality of this predicted action, we pass ",i.a.createElement(w.InlineMath,{math:"s"})," and ",i.a.createElement(w.InlineMath,{math:"a"})," into the critic, which then outputs the expected long-term reward of taking ",i.a.createElement(w.InlineMath,{math:"a"})," in ",i.a.createElement(w.InlineMath,{math:"s"}),". Assuming this estimate is decent, we would then want to update the actor in such a manner so as to maximize this value, so we take a gradient step in the direction achieving this."),i.a.createElement(w.BlockMath,{math:"\\theta_A \\leftarrow \\theta_A + \\eta \\nabla_{\\theta_A} \\hat{Q}(s, A(s))"}),i.a.createElement("p",null,"After alternating between training the actor and training the critic while exploring the environment, we should eventually converge on a good value function and a good actor characterizing the policy of the agent."),i.a.createElement("h3",null,"What is Risk?"),i.a.createElement("p",null,"Now, imagine that our critic didn","'","t just estimate the expected long-term reward, but it estimated the entire ",i.a.createElement("em",null,"distribution")," of long term rewards. Maybe we could assume that this distribution is a Gaussian, and predict its mean and variance. Assuming we could do that, then how might we define risk?"),i.a.createElement("p",null,"Well, one way you could think about risk is that it","'","s ",i.a.createElement("em",null," the degree to which you are concerned about worst-case outcomes"),". If you are risk-averse, that means you ",i.a.createElement("em",null,"only")," care about worst case outcomes. If you are risk-willing, then you might only care about average-case outcomes."),i.a.createElement("p",null,"A way to formalize this notion is through ",i.a.createElement("em",null,"conditional value at risk (CVaR)"),". Given a distribution, it is defined as the expectation of the distribution up to the ",i.a.createElement(w.InlineMath,{math:"\\alpha"}),"-th percentile. If ",i.a.createElement(w.InlineMath,{math:"\\alpha \\approx 0"}),", then you essentially only care about the worst case. If ",i.a.createElement(w.InlineMath,{math:"\\alpha = 1"}),", then you only care about expected case (which would be regular actor-critic!) Anything in between is simply some domain-specific tradeoff."),i.a.createElement("p",null,"This is the idea followed in the paper ",i.a.createElement("em",null,i.a.createElement("a",{href:"https://arxiv.org/abs/1911.03618"},"Worst Case Policy Gradient"))," by Tang et al., and the basis for this post."),i.a.createElement("h3",null,"Training the Critic"),i.a.createElement("p",null,"Getting the estimated means and variances from your model is fairly straightforward - just feedforward your critic model! Give it a state and an action and it will spit out the estimated mean and variance."),i.a.createElement(w.BlockMath,{math:"\\{ \\hat{Q}(s, a), \\hat{\\Upsilon}(s, a) \\} = critic(s, a)"}),i.a.createElement("p",null,"Now how will we attain good estimates for ",i.a.createElement(w.InlineMath,{math:"\\hat{Q}(s, a)"})," and ",i.a.createElement(w.InlineMath,{math:"\\hat{\\Upsilon}(s, a)"}),"? By the same idea of computing targets given observed reward information. Fist, the mean is actually quite easy because it has precisely the same semantic interpretation of the regular Q-value, hence going unchanged."),i.a.createElement(w.BlockMath,{math:"Q(s, a) = r + \\gamma \\sum_{s'} p(s' | s, a) \\hat{Q}(s', a')"}),i.a.createElement("p",null,"Now we just need to sort out what the proper variance target should look like. By simply expanding the definition of variance, you should end up with an expression resembling the following."),i.a.createElement(w.BlockMath,{math:"\\Upsilon(s, a) \\leftarrow r^2 + 2 \\gamma r \\sum_{s} p(s' | s, a) \\hat{Q}(s', a')"}),i.a.createElement(w.BlockMath,{math:"+ \\gamma^2 \\sum_{s'} p(s' | s, a) \\hat{\\Upsilon}(s', a') + \\gamma^2 \\sum_{s'} p(s' | s, a) \\hat{Q}(s', a')^2 - \\hat{Q}(s, a)^2"}),i.a.createElement("p",null,"Now we have our ",i.a.createElement("em",null,"estimates"),", namely ",i.a.createElement(w.InlineMath,{math:"\\hat{Q}(s, a)"})," and ",i.a.createElement(w.InlineMath,{math:"\\hat{\\Upsilon}(s, a)"}),", as well as our ",i.a.createElement("em",null,"targets"),", namely ",i.a.createElement(w.InlineMath,{math:"Q(s, a)"})," and ",i.a.createElement(w.InlineMath,{math:"\\Upsilon(s, a)"}),"."),i.a.createElement("p",null,"Now we have to construct a loss function which will quantify how good our estimates are. Why not define our loss function as some statistical distance metric between the Gaussian distributions characterized by our estimate and target? It turns out, the Wasserstein distance between the two Gaussians can be characterized by the following expression."),i.a.createElement(w.BlockMath,{math:"\\ell(\\theta) = (\\hat{Q}(s, a) - Q(s, a))^2"}),i.a.createElement(w.BlockMath,{math:"+ \\hat{\\Upsilon}(s, a) + \\Upsilon(s, a) - 2\\sqrt{\\hat{\\Upsilon}(s, a) \\Upsilon(s, a)}"}),i.a.createElement("p",null,"This expression should satisfy our intuition - the loss is zero when ",i.a.createElement(w.InlineMath,{math:"\\hat{Q}(s, a) = Q(s, a)"})," and ",i.a.createElement(w.InlineMath,{math:"\\hat{\\Upsilon}(s, a) = \\Upsilon(s, a)"}),", and positive otherwise."),i.a.createElement("h3",null,"Training the Actor"),i.a.createElement("p",null,"Now how should we train our actor? This part is easy assuming we have a trained critic. Just as before, we pass in a state ",i.a.createElement(w.InlineMath,{math:"s"})," to our actor which will give us an action ",i.a.createElement(w.InlineMath,{math:"a"}),". This action is then passed into the critic, which yields a mean ",i.a.createElement(w.InlineMath,{math:"\\hat{Q}(s, a)"})," and a variance ",i.a.createElement(w.InlineMath,{math:"\\hat{\\Upsilon}(s, a)"}),". Given this mean and variance, we can directly calculate the ",i.a.createElement("emph",null,"CVaR")," metric as the following where ",i.a.createElement(w.InlineMath,{math:"\\phi(\\alpha)"})," is the PDF of the Gaussian distribution evaluated at ",i.a.createElement(w.InlineMath,{math:"\\alpha"})," and ",i.a.createElement(w.InlineMath,{math:"\\Phi(\\alpha)"})," is the CDF."),i.a.createElement(w.BlockMath,{math:"\\Gamma(s, a, \\alpha) = \\hat{Q}(s, a) - \\frac{\\phi(\\alpha)}{\\Phi(\\alpha)}\\sqrt{\\hat{\\Upsilon}(s, a)}"}),i.a.createElement("p",null,"This is the scalar we want! Now, the actor just takes a gradient step in the direction ",i.a.createElement("em",null,"maximizing")," this value. Super simple!"),i.a.createElement("h3",null,"Conclusion"),i.a.createElement("p",null,"Hopefully this acted as a simple introduction to risk in the context of reinforcement learning. If you have any questions, feel free to shoot them my way!"))},k=a(20);var I=function(){return i.a.createElement(i.a.Fragment,null,i.a.createElement("h2",null,"Deaths of Despair"),i.a.createElement("p",null,i.a.createElement("em",null,"Ecclesiastes 1:9. What has been will be again, what has been done will be done again; there is nothing new under the sun.")),i.a.createElement("p",null,"I believe this is true in one sense, but false in another. Have these things been echoed in other forms in the past? Absolutely. But are these manifestations becoming increasingly more potent? I would argue so."),i.a.createElement("p",null,"In the past, you at least had some degree of separation between you and the elite. You can realize this in a few ways, but the way I think the most clearly about is the following. That is, on average, the amount of time these individuals occupy your direct attention, with some implicit weight on subconscious attention."),i.a.createElement("p",null,"Let\u2019s take the median American who makes around $33,706 a year (2018). Now, convince yourself there exists some probability, no matter how small, that such a person exhibits a chance of breaking down upon seeing such a photo after a long and hard day at work."),i.a.createElement("p",null,"How many times a day do you think that coin is flipped? There are 350 million americans. Wouldn\u2019t you like to be him? He looks well rested, and if he were to go up to any of his million fans, he would be undoubtedly praised like a living god."),i.a.createElement("h3",null,"Wealth Inequality"),i.a.createElement("p",null,"I see Amazon as a useful analog. If you had to distill America","'","s problem, it could be physically manifested as Amazon","'","s structure. Essentially, we have a clear balkanization and class structure. Which is, we have the elite which make decisions and live way more well off than necessary, the engineers, the lower class who are treated unreasonably poorly, and the hyper-elite Jeff Bezos money who is more valuable than is really coprehensible."),i.a.createElement("p",null,"At the time of writing this, Jeff Bezos is worth approximately 180 billion dollars. To conceptualize this, consider a billionaire. To become a billionaire, you would have had to have had a 4 million dollar salary every year throughout all of American history, which is an absolutely ridiculous salary to have persisted through the civil war, the invention of the cotton gin, throught the civil rights movement, until now. That is poverty in relation to Jeff Bezos."),i.a.createElement("p",null,"Now, the problem per say clearly isn","'","t that automation or wealth inequality is new in some sense, this is clearly false. But I would argue that a problem which rapidly exacerbates is indistinguishable from a new problem, and that this principle applies in this context. Of course, every time a new technology comes around it ripples throughout the economy. But the problem now is that the proportions of the population our innovations are having are growing, and the skills they undermine require more and more prior investment. Of course, when the cotton gin came around people surely lost their jobs, but finding other blue collar work was an alternative and learning the upkeep a cotton gin, which required more skill than picking the cotton itself, took an investment on the order of days. Now, we","'","re talking about technology which could potentially undermine manual labor all together, and using methods which require so much more skill, at an unprecedented speed."),i.a.createElement("p",null,"The key difference is the following key metric. What proportion of outsourced workers were able to actually contribute or play a part in the technology which outsourced them? With the cotton gin, I would argue that that number was modestly high. That is, the skills you had to be a picker of cotton weren","'","t too far off from maintaining a cotton gin or thinking of new ideas to improve it, it was a mechanical contraption after all. Of course not everyone would exhibit that effort and might simply switch to another industry, but the possibility was there. What proportion of truck drivers outsourced will be able to contribute or play a part in self-driving car technology? Literally zero percent."),i.a.createElement("p",null,"So lets say you think well get past the self driving truck issue, which id agree we probably will at some cost. Well, again, this is getting worse. Whats the next large scale technological innovation to be had? Almost surey the end of menial white collar work requiring basic linguistic ability and reasoning skills, aka lawyers, accountants and the like. What are we going to tell 40 year old men with kids and a mortgage? Especially when the maintenance of this software requires a team of three engineers? Im sorry but theres no jobs available in this context - midway through your career you might just need to learn how to be a pharmacologist. Its almost in some sense like our efficiency and problem solving ability has become too low of a barrier to entry for our own good."),i.a.createElement("p",null,"Again, my argument is not that self driving cars will be the end of civilization. Its that the implications of technological innovation are getting worse. If you think this round of innovation isn","'","t sufficient, wait 100 years. What will we be outsourcing then?"),i.a.createElement("iframe",{width:"560",height:"315",src:"https://www.youtube.com/embed/G1UpFHsbOf0?start=1745",frameborder:"0",allow:"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture",allowfullscreen:!0}),i.a.createElement("h3",null,"Social Media"),i.a.createElement("h4",null,"Twitch"),i.a.createElement("p",null,"I can","'","t say my response to such incidents better than it was said by /u/Thopterthallid in ",i.a.createElement("a",{href:"https://www.reddit.com/r/AskReddit/comments/ca0gm6/people_who_ordered_belle_delphines_bathwater_why/et5pesf?utm_source=share&utm_medium=web2x"},"this")," comment."),i.a.createElement("p",null,i.a.createElement("details",null,i.a.createElement("summary",null,i.a.createElement("i",null,'I want to try and break "Gamer girl water" down for people...')),i.a.createElement("i",null,'...coming from someone who had a dark time in my life where I might have been tempted to buy some "gamer girl bath water". It',"'",'s easy to say "Oh she',"'",'s just tricking idiots out of their money" but it',"'","s a little more complicated than that.",i.a.createElement("br",null),i.a.createElement("br",null),i.a.createElement("b",null,"When you","'","re approaching your mid to late 20s and still haven","'","t enjoyed a healthy and intimate relationship, it really does start to fuck with you on a scary level.")," Mental illness, depression, anxiety, and such seem to be a much more prevalent nowadays since it","'",'s so easy to get "quick fix" social interaction through the internet. The internet is amazing, but it doesn',"'","t make you better at actually being a social human being in real life. You get guys who had quick and easy access to a hundred friends on MSN/AIM/Skype/Discord/Telegram grow increasingly dependant on online interactions for their social needs. I can confirm it gets to the point where you just don","'","t even really want real interactions anymore because pornhub handles your libido, twitch gives you someone to listen to, instant messaging gives you people to talk to, etc. The only thing missing is the sexual physical touch of a woman, and there","'","s a part of you that might think that water that","'","s touched your Twitch crush will satisfy that. It won","'","t of course.",i.a.createElement("br",null),i.a.createElement("br",null),i.a.createElement("b",null,"The way that the most popular Youtubers, and Twitch Streamers interact with their audience is that of being a friend, family, or lover.")," How many streamers call their viewers some kind of pet name? Like, the [Streamer name] Family, or the [Streamer name] Army. When your livelihood comes from donations and views, you need to be especially charismatic. The biggest names in online video makers always talk directly to their audience. They look into the camera, they call you their friends and family, they want you to hang out with them at conventions, and they want to hear your comments, they want to read your messages in chat, they offer life advice, they tell you they care about you, they tell you they","'","re thankful for you, they want you to take care of yourself, and often times they","'","ll open up their soul and talk about their lives to you. It","'","s all about charisma and being a good host and it","'","s a great talent to have. For people in the previous point, this can actually be super harmful. You spend enough time in their streams and watching their videos, and you really do start to subconsciously believe that Markiplier, Pewdiepie, and Summit are your friends. Obviously it","'","s not the case, but there was a time in my life where some of my favourite Youtubers were appearing in my dreams and were asking me to hang out, and my brain didn","'","t even question that it was something we just did all the time. I recall a few years ago some kids and their mom showed up to a famous Twitch streamer","'","s address and were confused as to why he felt that was a breach of privacy. They expected to hang out with him, get autographs, play games with him, and all around expect that he","'","d be thrilled to see them. Obviously, this is a huge breach of boundaries, and perfectly sane people just forget that these people don","'","t have a 1:1 friendship with them.",i.a.createElement("br",null),i.a.createElement("br",null),i.a.createElement("b",null,"When you","'","re 25+ and haven","'","t had meaningful relationships with women, either romantic or otherwise, you start to get weird ideas about what women are, especially if you talk to the wrong groups.")," When you get to the point where you","'","ve craved intimate love and never had it for over ten years, you start getting weird ideas. Some men lash out at women and get incel-ish. Some men put women on these pedestals as divine trophies to be won. Some men just assume that women will never like them. But the longer you go without ever knowing love, the easier it is to think of women as being a totally different species.",i.a.createElement("br",null),i.a.createElement("br",null),i.a.createElement("b",null,"When every woman you","'","ve ever known either hates video games or are casual/novice gamers at best, meeting a woman who seems to idolize gaming as much as you do triggers volatile emotions.")," Sometimes it comes in the form of disdain and skepticism, and end up making a toxic environment for women in gaming. If not that, it comes out as excessive idolization. You","'","re at a point in your life where any woman who would even pay attention to you is a dream, and the thought of a girl who would share your favourite hobby with you may as well be the second coming of Christ. You crush on her hard, you buy all her merch, you send her donations so that she","'",'ll start to recognize your name and tell you "thank you" and read your messages. I',"'","m not saying that women shouldn","'","t be allowed to be successful gaming streamers, but I do think that taking advantage of clinically depressed and lonely fans like this is awful.",i.a.createElement("br",null),i.a.createElement("br",null),i.a.createElement("b",null,"This whole, selling bathwater thing to thirsty fans may seem like memes and trolling, but in reality it","'","s just highlighting a very, very sad reality.")," There","'","s guys out there who would buy this in a futile attempt just to feel closer to the one girl that ever paid them any attention. I was dangerously close to being one of those guys at one point and I know just how scary the spiral goes. People on my path would either end up doing things like buying this bathwater or becoming misogynistic incels.",i.a.createElement("br",null),i.a.createElement("br",null),"It all sounds so stupid and pathetic, but it","'","s a very true reality for some unfortunate guys suffering from severe depression.")),i.a.createElement("br",null)),i.a.createElement("h4",null,"Tinder"),i.a.createElement(k.a,{url:"https://instagr.am/p/Bfw2OPinqme/",maxWidth:320,hideCaption:!0}),i.a.createElement(k.a,{url:"https://instagr.am/p/BoPAPVKAnS0/",maxWidth:320,hideCaption:!0}),i.a.createElement("iframe",{width:"560",height:"315",src:"https://www.youtube.com/embed/PMotykw0SIk?start=1282",frameborder:"0",allow:"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture",allowfullscreen:!0}))};var x=function(){return i.a.createElement(l,null,i.a.createElement(s.c,null,i.a.createElement(s.a,{exact:!0,path:"/",component:u}),i.a.createElement(s.a,{exact:!0,path:"/about",component:u}),i.a.createElement(s.a,{exact:!0,path:"/research",component:m}),i.a.createElement(s.a,{exact:!0,path:"/writing",component:p}),i.a.createElement(s.a,{exact:!0,path:"/reading",component:y}),i.a.createElement(s.a,{exact:!0,path:"/design",component:g}),i.a.createElement(s.a,{exact:!0,path:"/writing/deaths-of-despair",component:I}),i.a.createElement(s.a,{exact:!0,path:"/writing/how-differential-privacy-fits-into-industry",component:v}),i.a.createElement(s.a,{exact:!0,path:"/writing/differentially-private-deep-learning",component:b}),i.a.createElement(s.a,{exact:!0,path:"/writing/risk-aware-reinforcement-learning",component:E})))},M=a(12);a.n(M).a.render(i.a.createElement(o.a,null,i.a.createElement(x,null)),document.getElementById("root")),"serviceWorker"in navigator&&navigator.serviceWorker.ready.then((function(e){e.unregister()})).catch((function(e){console.error(e.message)}))}},[[28,1,2]]]);
//# sourceMappingURL=main.c55a00ee.chunk.js.map