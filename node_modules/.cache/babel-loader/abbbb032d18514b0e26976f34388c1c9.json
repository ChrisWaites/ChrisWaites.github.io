{"ast":null,"code":"var _jsxFileName = \"/Users/chriswaites/Documents/projects/ChrisWaites.github.io/src/posts/differentially-private-deep-learning.js\";\nimport React from 'react';\nimport 'katex/dist/katex.min.css';\nimport { InlineMath, BlockMath } from 'react-katex';\n\nfunction Post() {\n  return /*#__PURE__*/React.createElement(React.Fragment, null, /*#__PURE__*/React.createElement(\"h2\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 9,\n      columnNumber: 9\n    }\n  }, \"A Guide to Differentially Private Deep Learning\"), /*#__PURE__*/React.createElement(\"p\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 12,\n      columnNumber: 9\n    }\n  }, \"In this post we tackle the topic of privacy-preserving deep learning. This commentary will be less so concerned with precision and proof, and moreso geared towards convincing a deep learning practitioner what privacy should mean, why its important, and how it can be achieved.\"), /*#__PURE__*/React.createElement(\"h3\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 16,\n      columnNumber: 9\n    }\n  }, \"Differential Privacy\"), /*#__PURE__*/React.createElement(\"p\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 19,\n      columnNumber: 9\n    }\n  }, \"Before talking about anything else, we need to define what privacy is. There\", \"\\'\", \"s a near infinite continuum of what privacy \", /*#__PURE__*/React.createElement(\"em\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 20,\n      columnNumber: 137\n    }\n  }, \"could\"), \" mean, and naturally some definitions are less vacuous than others.\"), /*#__PURE__*/React.createElement(\"p\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 22,\n      columnNumber: 9\n    }\n  }, \"One such definition is \", /*#__PURE__*/React.createElement(\"em\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 23,\n      columnNumber: 34\n    }\n  }, \"differential privacy\"), \". Differential privacy is concerned with \", /*#__PURE__*/React.createElement(\"em\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 23,\n      columnNumber: 104\n    }\n  }, \"algorithms\"), \", namely functions responsible for mapping a given dataset to some output space, e.g., linear regression mapping a dataset to its coefficients \", /*#__PURE__*/React.createElement(InlineMath, {\n    math: \"w\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 23,\n      columnNumber: 266\n    }\n  }), \" and \", /*#__PURE__*/React.createElement(InlineMath, {\n    math: \"b\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 23,\n      columnNumber: 294\n    }\n  }), \".\"), /*#__PURE__*/React.createElement(\"p\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 25,\n      columnNumber: 9\n    }\n  }, \"On a high level, such an algorithm would \\\"preserve privacy\\\" under the notion of differential privacy if it were to behave (approximately) the same regardless of whether you removed any individual point from the dataset. If you could achieve this property, then you \", /*#__PURE__*/React.createElement(\"em\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 26,\n      columnNumber: 276\n    }\n  }, \"should\"), \" be convinced that this algorithm is privacy-preserving.\"), /*#__PURE__*/React.createElement(\"p\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 28,\n      columnNumber: 9\n    }\n  }, \"Why? Well, let\", \"\\'\", \"s consider how you would feel if one of these points actually corresponded to you. By the definition put forth, you wouldn\", \"\\'\", \"t have grounds to care whether your data is given to the algorithm - the outcome will be the same regardless. In other words, \", /*#__PURE__*/React.createElement(\"em\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 29,\n      columnNumber: 285\n    }\n  }, \"you should feel like this algorithm preserves your privacy because its outputs look the same whether or not your data is given to it\"), \". This is the core idea.\"), /*#__PURE__*/React.createElement(\"p\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 31,\n      columnNumber: 9\n    }\n  }, \"Formally, we can express this notion via the following\", /*#__PURE__*/React.createElement(\"sup\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 32,\n      columnNumber: 65\n    }\n  }, /*#__PURE__*/React.createElement(\"a\", {\n    href: \"https://stephentu.github.io/writeups/6885-lec20-b.pdf\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 32,\n      columnNumber: 70\n    }\n  }, \"1\")), \":\"), /*#__PURE__*/React.createElement(\"p\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 34,\n      columnNumber: 9\n    }\n  }, /*#__PURE__*/React.createElement(\"em\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 34,\n      columnNumber: 12\n    }\n  }, \"Let \", /*#__PURE__*/React.createElement(InlineMath, {\n    math: \"A : \\\\mathcal{D} \\\\rightarrow \\\\mathcal{Y}\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 35,\n      columnNumber: 15\n    }\n  }), \" be a randomized algorithm. We call \", /*#__PURE__*/React.createElement(InlineMath, {\n    math: \"A\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 35,\n      columnNumber: 112\n    }\n  }), \" \\\"\", /*#__PURE__*/React.createElement(InlineMath, {\n    math: \"\\\\varepsilon\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 35,\n      columnNumber: 137\n    }\n  }), \"-differentially private\\\" if for all \", /*#__PURE__*/React.createElement(InlineMath, {\n    math: \"D_1, D_2 \\\\in \\\\mathcal{D}\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 35,\n      columnNumber: 206\n    }\n  }), \" differing in exactly one entry, and for all outputs \", /*#__PURE__*/React.createElement(InlineMath, {\n    math: \"y \\\\in \\\\mathcal{Y}\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 35,\n      columnNumber: 305\n    }\n  }), \", we have that:\", /*#__PURE__*/React.createElement(BlockMath, {\n    math: \"e^{-\\\\varepsilon} \\\\leq \\\\frac{\\\\Pr[\\\\mathcal{A(D_1) = y}]}{\\\\Pr[\\\\mathcal{A(D_2) = y}]} \\\\leq e^\\\\varepsilon\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 36,\n      columnNumber: 9\n    }\n  }))), /*#__PURE__*/React.createElement(\"p\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 38,\n      columnNumber: 9\n    }\n  }, \"Upon inspection, you\", \"\\'\", \"ll notice that this is saying exactly what we established earlier. Namely, the probability of a particular outcome occuring is about the same whether or not you include any particular individual in the dataset.\"), /*#__PURE__*/React.createElement(\"h3\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 42,\n      columnNumber: 9\n    }\n  }, \"Deep Learning\"), /*#__PURE__*/React.createElement(\"p\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 45,\n      columnNumber: 9\n    }\n  }, \"It goes without saying that deep learning has become an extremely popular form of statistical analysis. And conveniently, the algorithms of concern in the context of deep learning align perfect with the interface prescribed by differential privacy, namely in mapping provided datasets to some output space, in this context the model parameters.\"), /*#__PURE__*/React.createElement(\"p\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 48,\n      columnNumber: 9\n    }\n  }, \"One of the most pervasive approaches to this is the process of stochastic gradient descent. When conducting stochastic gradient descent, we iteratively update the parameters of a model by repeatedly sampling data and taking small steps over the parameters in the direction which minimizes our loss function. In other words, we repeatedly follow something resembling the following steps:\"), /*#__PURE__*/React.createElement(BlockMath, {\n    math: \"\\\\{ x^{(1)}, x^{(1)}, \\\\ldots x^{(b)} \\\\} \\\\sim sample(X, b)\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 51,\n      columnNumber: 9\n    }\n  }), /*#__PURE__*/React.createElement(BlockMath, {\n    math: \"\\\\ell(\\\\theta) = \\\\frac{1}{b} \\\\sum_i \\\\ell(x^{(i)} ; \\\\theta)\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 52,\n      columnNumber: 9\n    }\n  }), /*#__PURE__*/React.createElement(BlockMath, {\n    math: \"\\\\theta \\\\leftarrow \\\\theta - \\\\eta \\\\nabla_\\\\theta \\\\ell(\\\\theta)\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 53,\n      columnNumber: 9\n    }\n  }), /*#__PURE__*/React.createElement(\"p\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 54,\n      columnNumber: 9\n    }\n  }, \"It then begs the question as to how one would have to augment this procedure to achieve differential privacy, if at all possible.\"), /*#__PURE__*/React.createElement(\"h3\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 58,\n      columnNumber: 9\n    }\n  }, \"Differentially Private Stochastic Gradient Descent\"), /*#__PURE__*/React.createElement(\"p\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 61,\n      columnNumber: 9\n    }\n  }, /*#__PURE__*/React.createElement(\"a\", {\n    href: \"https://arxiv.org/abs/1607.00133\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 62,\n      columnNumber: 11\n    }\n  }, \"Abadi et al.\"), \" detail the differentially private stochastic gradient descent (DP-SGD) algorithm to make traditional SGD yield a differential privacy guarantee. To describe it, we need to introduce a number of augmentations.\"), /*#__PURE__*/React.createElement(\"p\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 64,\n      columnNumber: 9\n    }\n  }, \"First, we have to augment our typical method for sampling. In the context of non-private deep learning, sampling is often achieved by shuffling the dataset and running through partitions of size \", /*#__PURE__*/React.createElement(InlineMath, {\n    math: \"b\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 65,\n      columnNumber: 206\n    }\n  }), \" such that each example is viewed by the model exactly once per epoch. Although in the context of DP-SGD, we must opt for either Poisson\", /*#__PURE__*/React.createElement(\"sup\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 65,\n      columnNumber: 365\n    }\n  }, \"2\"), \" or uniform\", /*#__PURE__*/React.createElement(\"sup\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 65,\n      columnNumber: 388\n    }\n  }, \"3\"), \" subsampling if we want to achieve a differential privacy guarantee.\"), /*#__PURE__*/React.createElement(\"p\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 67,\n      columnNumber: 9\n    }\n  }, \"Second, we need to introduce a number of additional training parameters. In particular, we need to introduce a clipping parameter \", /*#__PURE__*/React.createElement(InlineMath, {\n    math: \"C\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 68,\n      columnNumber: 141\n    }\n  }), \", an upper bound on the \", /*#__PURE__*/React.createElement(InlineMath, {\n    math: \"\\\\ell_2\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 68,\n      columnNumber: 188\n    }\n  }), \"-norm of each per-example gradient, as well as the noise multiplier \", /*#__PURE__*/React.createElement(InlineMath, {\n    math: \"\\\\sigma\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 68,\n      columnNumber: 284\n    }\n  }), \", which acts as the ratio between the clipping parameter and variance of the Gaussian noise applied to each gradient update after clipping.\"), /*#__PURE__*/React.createElement(BlockMath, {\n    math: \"\\\\{ x^{(1)}, x^{(1)}, \\\\ldots x^{(b)} \\\\} \\\\sim sample(X, b)\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 70,\n      columnNumber: 9\n    }\n  }), /*#__PURE__*/React.createElement(BlockMath, {\n    math: \"g^{(i)} \\\\leftarrow \\\\nabla_{\\\\theta} \\\\ell(x^{(i)} ; \\\\theta)\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 71,\n      columnNumber: 9\n    }\n  }), /*#__PURE__*/React.createElement(BlockMath, {\n    math: \"\\\\bar{g}^{(i)} \\\\leftarrow g^{(i)} / \\\\max\\\\{ 1, ||g^{(i)}||_2 / C\\\\}\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 72,\n      columnNumber: 9\n    }\n  }), /*#__PURE__*/React.createElement(BlockMath, {\n    math: \"\\\\tilde{g} \\\\leftarrow \\\\frac{1}{b} (\\\\sum_i \\\\bar{g}^{(i)} + \\\\sigma \\\\cdot C \\\\cdot \\\\mathcal{N}(0, I))\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 73,\n      columnNumber: 9\n    }\n  }), /*#__PURE__*/React.createElement(BlockMath, {\n    math: \"\\\\theta \\\\leftarrow \\\\theta - \\\\eta \\\\tilde{g}\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 74,\n      columnNumber: 9\n    }\n  }), /*#__PURE__*/React.createElement(\"p\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 75,\n      columnNumber: 9\n    }\n  }, \"In order to calculate the privacy loss corresponding to k executions of the above update rule, Abadi et al. detail the moments accountant as a method to report privacy loss over time. A full deep dive into the foundations backing the moments accountant are likely outside the scope of this post, but on a high level it can be thought of as a black box which takes in values which characterize your training loop (sampling probabilities, number of minibatches, delta, etc.) and outputs epsilon. But importantly, their method yields much tighter bounds on the privacy loss achieved than what is reported via the strong composition theorem. If interested in learning more, the algorithm was originally introduced in Abadi et al. and has a corresponding implementation within Tensorflow Privacy.\"), /*#__PURE__*/React.createElement(\"h3\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 79,\n      columnNumber: 9\n    }\n  }, \"Private Aggregation of Teacher Ensembles\"), /*#__PURE__*/React.createElement(\"p\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 82,\n      columnNumber: 9\n    }\n  }, \"Private Aggregation of Teacher Ensembles or PATE is an alternative method to DPSGD for conducting differentially private learning. The key idea is to, rather than train a single strong model which captures a complex criterion in a differentially private manner, train a set of weaker, non-private models on partitions of the data and then perform a noisy aggregation of their predictions. Overall the method has been shown to be quite effective at the expense of some assumptions about the training procedure. There has even been an application of this technique to GANs via PATE-GAN.\"), /*#__PURE__*/React.createElement(\"h3\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 86,\n      columnNumber: 9\n    }\n  }, \"Differentially Private Federated Learning\"), /*#__PURE__*/React.createElement(\"h3\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 90,\n      columnNumber: 9\n    }\n  }, \"Proper Development\"), /*#__PURE__*/React.createElement(\"p\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 93,\n      columnNumber: 9\n    }\n  }, \"There are a number of common gotchas involved with the proper development and deployment of differentially private deep learning models.\"), /*#__PURE__*/React.createElement(\"p\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 96,\n      columnNumber: 9\n    }\n  }, \"The biggest is hyperparameter search and model selection. You might think intuitively that, as long as you implement and execute something like DP-SGD, you can sleep soundly at night knowing your model is privacy preserving. Although, typically many models are trained in the process of during the hyperparameter . Although in theory, the hyperparameters you\", \"\\'\", \"re selecting are technically data-inspired, and hence indirectly leak information. Frankly this detail is swept under the rug in research contexts. But in industry, this absolutely needs to be taken into consideration, or at the very least consciously acknowledged as a risk.\"), /*#__PURE__*/React.createElement(\"p\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 99,\n      columnNumber: 9\n    }\n  }, \"The second is proper evaluation. It does not suffice to simply train a single model and report its accuracy. Recall that\"));\n}\n\nexport default Post;","map":{"version":3,"sources":["/Users/chriswaites/Documents/projects/ChrisWaites.github.io/src/posts/differentially-private-deep-learning.js"],"names":["React","InlineMath","BlockMath","Post"],"mappings":";AAAA,OAAOA,KAAP,MAAkB,OAAlB;AACA,OAAO,0BAAP;AACA,SAASC,UAAT,EAAqBC,SAArB,QAAsC,aAAtC;;AAGA,SAASC,IAAT,GAAgB;AACZ,sBACE,uDACE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,uDADF,eAIE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,4RAJF,eAQE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,4BARF,eAWE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,qFAC+E,IAD/E,+DACgI;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,aADhI,wEAXF,eAcE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,6CACyB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,4BADzB,4DAC+F;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,kBAD/F,kKACiQ,oBAAC,UAAD;AAAY,IAAA,IAAI,EAAC,GAAjB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IADjQ,wBAC6R,oBAAC,UAAD;AAAY,IAAA,IAAI,EAAC,GAAjB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IAD7R,MAdF,eAiBE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,iSAC2Q;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,cAD3Q,6DAjBF,eAoBE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,uBACiB,IADjB,gIACiJ,IADjJ,iJACoR;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,4IADpR,6BApBF,eAuBE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,4EACwD;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,kBAAK;AAAG,IAAA,IAAI,EAAC,uDAAR;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,SAAL,CADxD,MAvBF,eA0BE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,kBAAG;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,0BACG,oBAAC,UAAD;AAAY,IAAA,IAAI,EAAC,4CAAjB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IADH,uDACoG,oBAAC,UAAD;AAAY,IAAA,IAAI,EAAC,GAAjB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IADpG,sBAC6H,oBAAC,UAAD;AAAY,IAAA,IAAI,EAAC,cAAjB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IAD7H,wDACkM,oBAAC,UAAD;AAAY,IAAA,IAAI,EAAC,4BAAjB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IADlM,wEACqS,oBAAC,UAAD;AAAY,IAAA,IAAI,EAAC,qBAAjB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IADrS,kCAEH,oBAAC,SAAD;AAAW,IAAA,IAAI,EAAC,+GAAhB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IAFG,CAAH,CA1BF,eA8BE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,6BACuB,IADvB,uNA9BF,eAkCE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,qBAlCF,eAqCE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,gWArCF,eAwCE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,0YAxCF,eA2CE,oBAAC,SAAD;AAAW,IAAA,IAAI,EAAC,8DAAhB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IA3CF,eA4CE,oBAAC,SAAD;AAAW,IAAA,IAAI,EAAC,gEAAhB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IA5CF,eA6CE,oBAAC,SAAD;AAAW,IAAA,IAAI,EAAC,oEAAhB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IA7CF,eA8CE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,yIA9CF,eAkDE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,0DAlDF,eAqDE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,kBACE;AAAG,IAAA,IAAI,EAAC,kCAAR;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,oBADF,sNArDF,eAwDE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,yNACqM,oBAAC,UAAD;AAAY,IAAA,IAAI,EAAC,GAAjB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IADrM,2JACoW;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,SADpW,8BAC2X;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,SAD3X,yEAxDF,eA2DE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,wJACoI,oBAAC,UAAD;AAAY,IAAA,IAAI,EAAC,GAAjB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IADpI,2CACmL,oBAAC,UAAD;AAAY,IAAA,IAAI,EAAC,SAAjB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IADnL,uFACmR,oBAAC,UAAD;AAAY,IAAA,IAAI,EAAC,SAAjB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IADnR,gJA3DF,eA8DE,oBAAC,SAAD;AAAW,IAAA,IAAI,EAAC,8DAAhB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IA9DF,eA+DE,oBAAC,SAAD;AAAW,IAAA,IAAI,EAAC,gEAAhB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IA/DF,eAgEE,oBAAC,SAAD;AAAW,IAAA,IAAI,EAAC,uEAAhB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IAhEF,eAiEE,oBAAC,SAAD;AAAW,IAAA,IAAI,EAAC,2GAAhB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IAjEF,eAkEE,oBAAC,SAAD;AAAW,IAAA,IAAI,EAAC,gDAAhB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IAlEF,eAmEE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,+xBAnEF,eAuEE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,gDAvEF,eA0EE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,glBA1EF,eA8EE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,iDA9EF,eAkFE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,0BAlFF,eAqFE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,gJArFF,eAwFE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,+WACyW,IADzW,wRAxFF,eA2FE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,gIA3FF,CADF;AAiGH;;AAED,eAAeA,IAAf","sourcesContent":["import React from 'react';\nimport 'katex/dist/katex.min.css';\nimport { InlineMath, BlockMath } from 'react-katex';\n\n\nfunction Post() {\n    return (\n      <>\n        <h2>\n          A Guide to Differentially Private Deep Learning\n        </h2>\n        <p>\n          In this post we tackle the topic of privacy-preserving deep learning. This commentary will be less so concerned with precision and proof, and moreso geared towards convincing a deep learning practitioner what privacy should mean, why its important, and how it can be achieved.\n        </p>\n\n        <h3>\n          Differential Privacy\n        </h3>\n        <p>\n          Before talking about anything else, we need to define what privacy is. There{\"\\'\"}s a near infinite continuum of what privacy <em>could</em> mean, and naturally some definitions are less vacuous than others.\n        </p>\n        <p>\n          One such definition is <em>differential privacy</em>. Differential privacy is concerned with <em>algorithms</em>, namely functions responsible for mapping a given dataset to some output space, e.g., linear regression mapping a dataset to its coefficients <InlineMath math=\"w\" /> and <InlineMath math=\"b\" />.\n        </p>\n        <p>\n          On a high level, such an algorithm would \"preserve privacy\" under the notion of differential privacy if it were to behave (approximately) the same regardless of whether you removed any individual point from the dataset. If you could achieve this property, then you <em>should</em> be convinced that this algorithm is privacy-preserving.\n        </p>\n        <p>\n          Why? Well, let{\"\\'\"}s consider how you would feel if one of these points actually corresponded to you. By the definition put forth, you wouldn{\"\\'\"}t have grounds to care whether your data is given to the algorithm - the outcome will be the same regardless. In other words, <em>you should feel like this algorithm preserves your privacy because its outputs look the same whether or not your data is given to it</em>. This is the core idea.\n        </p>\n        <p>\n          Formally, we can express this notion via the following<sup><a href=\"https://stephentu.github.io/writeups/6885-lec20-b.pdf\">1</a></sup>:\n        </p>\n        <p><em>\n          Let <InlineMath math=\"A : \\mathcal{D} \\rightarrow \\mathcal{Y}\" /> be a randomized algorithm. We call <InlineMath math=\"A\" /> \"<InlineMath math=\"\\varepsilon\" />-differentially private\" if for all <InlineMath math=\"D_1, D_2 \\in \\mathcal{D}\" /> differing in exactly one entry, and for all outputs <InlineMath math=\"y \\in \\mathcal{Y}\" />, we have that:\n        <BlockMath math=\"e^{-\\varepsilon} \\leq \\frac{\\Pr[\\mathcal{A(D_1) = y}]}{\\Pr[\\mathcal{A(D_2) = y}]} \\leq e^\\varepsilon\" />\n        </em></p>\n        <p>\n          Upon inspection, you{\"\\'\"}ll notice that this is saying exactly what we established earlier. Namely, the probability of a particular outcome occuring is about the same whether or not you include any particular individual in the dataset.\n        </p>\n\n        <h3>\n          Deep Learning\n        </h3>\n        <p>\n          It goes without saying that deep learning has become an extremely popular form of statistical analysis. And conveniently, the algorithms of concern in the context of deep learning align perfect with the interface prescribed by differential privacy, namely in mapping provided datasets to some output space, in this context the model parameters.\n        </p>\n        <p>\n          One of the most pervasive approaches to this is the process of stochastic gradient descent. When conducting stochastic gradient descent, we iteratively update the parameters of a model by repeatedly sampling data and taking small steps over the parameters in the direction which minimizes our loss function. In other words, we repeatedly follow something resembling the following steps:\n        </p>\n        <BlockMath math=\"\\{ x^{(1)}, x^{(1)}, \\ldots x^{(b)} \\} \\sim sample(X, b)\" />\n        <BlockMath math=\"\\ell(\\theta) = \\frac{1}{b} \\sum_i \\ell(x^{(i)} ; \\theta)\" />\n        <BlockMath math=\"\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\ell(\\theta)\" />\n        <p>\n          It then begs the question as to how one would have to augment this procedure to achieve differential privacy, if at all possible.\n        </p>\n\n        <h3>\n          Differentially Private Stochastic Gradient Descent\n        </h3>\n        <p>\n          <a href=\"https://arxiv.org/abs/1607.00133\">Abadi et al.</a> detail the differentially private stochastic gradient descent (DP-SGD) algorithm to make traditional SGD yield a differential privacy guarantee. To describe it, we need to introduce a number of augmentations.\n        </p>\n        <p>\n          First, we have to augment our typical method for sampling. In the context of non-private deep learning, sampling is often achieved by shuffling the dataset and running through partitions of size <InlineMath math=\"b\" /> such that each example is viewed by the model exactly once per epoch. Although in the context of DP-SGD, we must opt for either Poisson<sup>2</sup> or uniform<sup>3</sup> subsampling if we want to achieve a differential privacy guarantee.\n        </p>\n        <p>\n          Second, we need to introduce a number of additional training parameters. In particular, we need to introduce a clipping parameter <InlineMath math=\"C\" />, an upper bound on the <InlineMath math=\"\\ell_2\" />-norm of each per-example gradient, as well as the noise multiplier <InlineMath math=\"\\sigma\" />, which acts as the ratio between the clipping parameter and variance of the Gaussian noise applied to each gradient update after clipping.\n        </p>\n        <BlockMath math=\"\\{ x^{(1)}, x^{(1)}, \\ldots x^{(b)} \\} \\sim sample(X, b)\" />\n        <BlockMath math=\"g^{(i)} \\leftarrow \\nabla_{\\theta} \\ell(x^{(i)} ; \\theta)\" />\n        <BlockMath math=\"\\bar{g}^{(i)} \\leftarrow g^{(i)} / \\max\\{ 1, ||g^{(i)}||_2 / C\\}\" />\n        <BlockMath math=\"\\tilde{g} \\leftarrow \\frac{1}{b} (\\sum_i \\bar{g}^{(i)} + \\sigma \\cdot C \\cdot \\mathcal{N}(0, I))\" />\n        <BlockMath math=\"\\theta \\leftarrow \\theta - \\eta \\tilde{g}\" />\n        <p>\n          In order to calculate the privacy loss corresponding to k executions of the above update rule, Abadi et al. detail the moments accountant as a method to report privacy loss over time. A full deep dive into the foundations backing the moments accountant are likely outside the scope of this post, but on a high level it can be thought of as a black box which takes in values which characterize your training loop (sampling probabilities, number of minibatches, delta, etc.) and outputs epsilon. But importantly, their method yields much tighter bounds on the privacy loss achieved than what is reported via the strong composition theorem. If interested in learning more, the algorithm was originally introduced in Abadi et al. and has a corresponding implementation within Tensorflow Privacy.\n        </p>\n\n        <h3>\n          Private Aggregation of Teacher Ensembles\n        </h3>\n        <p>\n          Private Aggregation of Teacher Ensembles or PATE is an alternative method to DPSGD for conducting differentially private learning. The key idea is to, rather than train a single strong model which captures a complex criterion in a differentially private manner, train a set of weaker, non-private models on partitions of the data and then perform a noisy aggregation of their predictions. Overall the method has been shown to be quite effective at the expense of some assumptions about the training procedure. There has even been an application of this technique to GANs via PATE-GAN.\n        </p>\n\n        <h3>\n          Differentially Private Federated Learning\n        </h3>\n\n        <h3>\n          Proper Development\n        </h3>\n        <p>\n          There are a number of common gotchas involved with the proper development and deployment of differentially private deep learning models.\n        </p>\n        <p>\n          The biggest is hyperparameter search and model selection. You might think intuitively that, as long as you implement and execute something like DP-SGD, you can sleep soundly at night knowing your model is privacy preserving. Although, typically many models are trained in the process of during the hyperparameter . Although in theory, the hyperparameters you{\"\\'\"}re selecting are technically data-inspired, and hence indirectly leak information. Frankly this detail is swept under the rug in research contexts. But in industry, this absolutely needs to be taken into consideration, or at the very least consciously acknowledged as a risk.\n        </p>\n        <p>\n          The second is proper evaluation. It does not suffice to simply train a single model and report its accuracy. Recall that\n        </p>\n      </>\n    );\n}\n\nexport default Post;\n"]},"metadata":{},"sourceType":"module"}