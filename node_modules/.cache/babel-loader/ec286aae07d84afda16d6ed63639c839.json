{"ast":null,"code":"import React from'react';import'katex/dist/katex.min.css';import{InlineMath,BlockMath}from'react-katex';function Post(){return/*#__PURE__*/React.createElement(React.Fragment,null,/*#__PURE__*/React.createElement(\"h2\",null,\"Deleting Data from Machine Learning Models\"),/*#__PURE__*/React.createElement(\"p\",null,\"Here\",\"\\'\",\"s an interesting problem: let\",\"\\'\",\"s imagine your data was used to train some machine learning model. Now, you want to request that your data be \",/*#__PURE__*/React.createElement(\"em\",null,\"unlearned\"),\" from the model. Roughly speaking, the model is updated so that its parameters and outputs have no knowledge of your data anymore.\"),/*#__PURE__*/React.createElement(\"p\",null,\"For \",/*#__PURE__*/React.createElement(InlineMath,{math:\"k\"}),\"-nearest neighbors, this problem is trivial - simply remove the point from the dataset. For something more complex like a neural network, it\",\"\\'\",\"s not immediately clear what you would do. Is there some interesting middle-ground?\"),/*#__PURE__*/React.createElement(\"p\",null,\"First, let\",\"\\'\",\"s define the problem. Consider the two following scenarios:\"),/*#__PURE__*/React.createElement(\"ul\",null,/*#__PURE__*/React.createElement(\"li\",null,/*#__PURE__*/React.createElement(\"em\",null,\"Case A (Real): We train the model on the full dataset containing \",/*#__PURE__*/React.createElement(InlineMath,{math:\"x\"}),\", we remove \",/*#__PURE__*/React.createElement(InlineMath,{math:\"x\"}),\" from the model parameters, and then we publish the model to the public.\")),/*#__PURE__*/React.createElement(\"li\",null,/*#__PURE__*/React.createElement(\"em\",null,\"Case B (Imaginary): The point \",/*#__PURE__*/React.createElement(InlineMath,{math:\"x\"}),\" was never in the dataset, we train the model on the dataset (which doesn\",\"\\'\",\"t contain \",/*#__PURE__*/React.createElement(InlineMath,{math:\"x\"}),\"), and then we publish the model to the public.\"))),/*#__PURE__*/React.createElement(\"p\",null,\"Here\",\"\\'\",\"s what it will mean to be able to \\\"unlearn\\\": if we can find a procedure for Case A which yields an indistinguishable outcome to Case B, then we will have achieved our goal. Meaning, if we can train and then delete, and the outcome of this is indistinguishable from the case where the point was never in the dataset at all, that would be an effective unlearning algorithm.\"),/*#__PURE__*/React.createElement(\"p\",null,\"Believe it or not, this is possible in certain cases! For the purposes of this post, I will detail the \",/*#__PURE__*/React.createElement(\"a\",{href:\"https://arxiv.org/abs/2007.02923\"},\"descent-to-delete\"),\" approach by Neel et al. To achieve this, we are going to tackle the case of a convex parametric model, e.g., logistic regression with binary cross entropy loss and \",/*#__PURE__*/React.createElement(InlineMath,{math:\"\\\\ell_2\"}),\" regularization.\"),/*#__PURE__*/React.createElement(\"p\",null,\"To be concrete, let\",\"\\'\",\"s say our loss function is:\"),/*#__PURE__*/React.createElement(BlockMath,{math:\"\\\\ell(\\\\theta) = - \\\\frac{1}{n} \\\\left( \\\\sum_{i \\\\in [n]} y_i \\\\log(\\\\hat{y}_i) + (1 - y_i) \\\\log(1 - \\\\hat{y}_i) \\\\right) + \\\\lambda ||\\\\theta||_2\"}),/*#__PURE__*/React.createElement(\"p\",null,\"This loss function is \",/*#__PURE__*/React.createElement(InlineMath,{math:\"m\"}),\"-strongly convex and \",/*#__PURE__*/React.createElement(InlineMath,{math:\"M\"}),\"-smooth, where \",/*#__PURE__*/React.createElement(InlineMath,{math:\"m = \\\\lambda\"}),\" and \",/*#__PURE__*/React.createElement(InlineMath,{math:\"M = 4 - \\\\lambda\"}),\".\"),/*#__PURE__*/React.createElement(\"p\",null,\"Now, the great part about working in this setting is that we have provable convergence guarantees. That is, as long as we know how far we are at most from the global optimum, if we run (projected) gradient descent for some number of steps, we know we will be at most some distance away. To formalize this:\"),/*#__PURE__*/React.createElement(\"em\",null,\"Let \",/*#__PURE__*/React.createElement(InlineMath,{math:\"\\\\ell(\\\\theta)\"}),\" be \",/*#__PURE__*/React.createElement(InlineMath,{math:\"m\"}),\"-strongly convex and M-smooth, and let \",/*#__PURE__*/React.createElement(InlineMath,{math:\"\\\\theta^* = argmin_{\\\\theta \\\\in \\\\Theta} \\\\ell(\\\\theta)\"}),\". We have that after \",/*#__PURE__*/React.createElement(InlineMath,{math:\"T\"}),\" steps of gradient descent with step size \",/*#__PURE__*/React.createElement(InlineMath,{math:\"\\\\eta = \\\\frac{2}{m + M}\"}),\":\"),/*#__PURE__*/React.createElement(BlockMath,{math:\"||\\\\theta_T - \\\\theta^*||_2 \\\\leq \\\\left( \\\\frac{M - m}{M + m} \\\\right)^T || \\\\theta_0 - \\\\theta^*||_2\"}),/*#__PURE__*/React.createElement(\"p\",null,\"Now I\",\"\\'\",\"ll introduce the algorithm.\"),/*#__PURE__*/React.createElement(\"br\",null),/*#__PURE__*/React.createElement(\"p\",null,\"We begin with Case A, where we perform gradient descent on the full dataset for \",/*#__PURE__*/React.createElement(InlineMath,{math:\"t\"}),\" steps until \",/*#__PURE__*/React.createElement(InlineMath,{math:\"\\\\theta_0\"}),\" becomes \",/*#__PURE__*/React.createElement(InlineMath,{math:\"\\\\theta_t\"}),\". The dotted line represents how far away \",/*#__PURE__*/React.createElement(InlineMath,{math:\"\\\\theta_t\"}),\" can possibly be from \",/*#__PURE__*/React.createElement(InlineMath,{math:\"\\\\theta^D\"}),\" due to our convergence guarantee.\"),/*#__PURE__*/React.createElement(\"center\",{style:{'background-color':'white','padding':'20px','border-radius':'20px'}},/*#__PURE__*/React.createElement(\"img\",{src:\"../machine-unlearning/image2.svg\",width:\"50%\"})),/*#__PURE__*/React.createElement(\"br\",null),/*#__PURE__*/React.createElement(\"p\",null,\"Now, let\",\"\\'\",\"s say that at this point in training, we receive a deletion request for some point \",/*#__PURE__*/React.createElement(InlineMath,{math:\"x\"}),\". To handle this, we\",\"\\'\",\"ll first need to establish the notion of \",/*#__PURE__*/React.createElement(\"em\",null,\"sensitivity\"),\". If we have some global optimum, then the \",/*#__PURE__*/React.createElement(\"em\",null,\"sensitivity\"),\" is the furthest away the global minimum can move due to the removal of a single point. When we remove \",/*#__PURE__*/React.createElement(InlineMath,{math:\"x\"}),\", we know that the resulting global minimum \",/*#__PURE__*/React.createElement(InlineMath,{math:\"\\\\theta^{D \\\\setminus x}\"}),\" can\",\"\\'\",\"t be \",/*#__PURE__*/React.createElement(\"em\",null,\"too\"),\" far away.\"),/*#__PURE__*/React.createElement(\"center\",{style:{'background-color':'white','padding':'20px','border-radius':'20px'}},/*#__PURE__*/React.createElement(\"img\",{src:\"../machine-unlearning/image1.svg\",width:\"50%\"})),/*#__PURE__*/React.createElement(\"br\",null),/*#__PURE__*/React.createElement(\"p\",null,\"Given this, we know how far away \",/*#__PURE__*/React.createElement(InlineMath,{math:\"\\\\theta_t\"}),\" can be from \",/*#__PURE__*/React.createElement(InlineMath,{math:\"\\\\theta^D\"}),\" and how far \",/*#__PURE__*/React.createElement(InlineMath,{math:\"\\\\theta^D\"}),\" can be from \",/*#__PURE__*/React.createElement(InlineMath,{math:\"\\\\theta^{D \\\\setminus x}\"}),\". Therefore, we know how far \",/*#__PURE__*/React.createElement(InlineMath,{math:\"\\\\theta_t\"}),\" could possibly be from \",/*#__PURE__*/React.createElement(InlineMath,{math:\"\\\\theta^{D \\\\setminus x}\"}),\". Given this information, we can again apply the convergence guarantee and perform gradient descent for some required number of steps in the direction of \",/*#__PURE__*/React.createElement(InlineMath,{math:\"\\\\theta^{D \\\\setminus x}\"}),\" until we know we\",\"\\'\",\"re some distance away, resulting in \",/*#__PURE__*/React.createElement(InlineMath,{math:\"\\\\theta_T\"}),\". This finishes Case A, where we\",\"\\'\",\"ve gone from an initial point \",/*#__PURE__*/React.createElement(InlineMath,{math:\"\\\\theta_0\"}),\" and gotten close to \",/*#__PURE__*/React.createElement(InlineMath,{math:\"\\\\theta^{D \\\\setminus x}\"}),\" without knowing \",/*#__PURE__*/React.createElement(InlineMath,{math:\"x\"}),\" beforehand.\"),/*#__PURE__*/React.createElement(\"center\",{style:{'background-color':'white','padding':'20px','border-radius':'20px'}},/*#__PURE__*/React.createElement(\"img\",{src:\"../machine-unlearning/image3.svg\",width:\"50%\"})),/*#__PURE__*/React.createElement(\"br\",null),/*#__PURE__*/React.createElement(\"p\",null,\"Now we consider alternative reality, Case B. That is, we start from our initial point and train regularly but \",/*#__PURE__*/React.createElement(InlineMath,{math:\"x\"}),\" was never in the dataset. Given our convergence guarantee, again we can know after some number of steps that we are at least within some distance of the optimum.\"),/*#__PURE__*/React.createElement(\"center\",{style:{'background-color':'white','padding':'20px','border-radius':'20px'}},/*#__PURE__*/React.createElement(\"img\",{src:\"../machine-unlearning/image4.svg\",width:\"50%\"})),/*#__PURE__*/React.createElement(\"br\",null),/*#__PURE__*/React.createElement(\"p\",null,\"Now given that we can guarantee a certain distance from \",/*#__PURE__*/React.createElement(InlineMath,{math:\"\\\\theta^{D \\\\setminus x}\"}),\" in either case, we can guarantee that \",/*#__PURE__*/React.createElement(InlineMath,{math:\"\\\\theta_T\"}),\" and \",/*#__PURE__*/React.createElement(InlineMath,{math:\"\\\\theta\"}),\" are within some distance of one another.\"),/*#__PURE__*/React.createElement(\"center\",{style:{'background-color':'white','padding':'20px','border-radius':'20px'}},/*#__PURE__*/React.createElement(\"img\",{src:\"../machine-unlearning/image5.svg\",width:\"50%\"})),/*#__PURE__*/React.createElement(\"br\",null),/*#__PURE__*/React.createElement(\"p\",null,\"Finally, in either case, we publish the models according to the same publishing scheme, namely an injection of Gaussian noise to the model parameters scaled to how far \",/*#__PURE__*/React.createElement(InlineMath,{math:\"\\\\theta_T\"}),\" and \",/*#__PURE__*/React.createElement(InlineMath,{math:\"\\\\theta\"}),\" could possibly be from one another. All of this entails that both outcomes are statistically indistinguishable from one another, in the sense that the probability distributions over both models are \\\"approximately identical\\\".\"),/*#__PURE__*/React.createElement(\"center\",{style:{'background-color':'white','padding':'20px','border-radius':'20px'}},/*#__PURE__*/React.createElement(\"img\",{src:\"../machine-unlearning/image6.svg\",width:\"50%\"})),/*#__PURE__*/React.createElement(\"h3\",null,\"Conlusion\"),/*#__PURE__*/React.createElement(\"p\",null,\"Hopefully this was a useful introduction into the problem of machine unlearning and how it can be addressed. For further reading, make sure to check out the blog post detailing \",/*#__PURE__*/React.createElement(\"a\",{href:\"http://www.cleverhans.io/2020/07/20/unlearning.html\"},\"SISA\"),\", an alternativ approach proposed by Papernot et al.\"));}export default Post;","map":{"version":3,"sources":["/Users/chriswaites/Documents/projects/ChrisWaites.github.io/src/posts/machine-unlearning.js"],"names":["React","InlineMath","BlockMath","Post"],"mappings":"AAAA,MAAOA,CAAAA,KAAP,KAAkB,OAAlB,CACA,MAAO,0BAAP,CACA,OAASC,UAAT,CAAqBC,SAArB,KAAsC,aAAtC,CAGA,QAASC,CAAAA,IAAT,EAAgB,CACZ,mBACE,qDACE,2EADF,cAIE,oCACO,IADP,iCAC0C,IAD1C,+HAC6J,0CAD7J,sIAJF,cAOE,iDACM,oBAAC,UAAD,EAAY,IAAI,CAAC,GAAjB,EADN,gJACyK,IADzK,uFAPF,cAUE,0CACa,IADb,+DAVF,cAaE,2CACE,2CACA,+GACmE,oBAAC,UAAD,EAAY,IAAI,CAAC,GAAjB,EADnE,6BACqG,oBAAC,UAAD,EAAY,IAAI,CAAC,GAAjB,EADrG,4EADA,CADF,cAME,2CACA,4EACgC,oBAAC,UAAD,EAAY,IAAI,CAAC,GAAjB,EADhC,6EACgI,IADhI,2BAC+I,oBAAC,UAAD,EAAY,IAAI,CAAC,GAAjB,EAD/I,mDADA,CANF,CAbF,cAyBE,oCACO,IADP,yXAzBF,cA4BE,oJACyG,yBAAG,IAAI,CAAC,kCAAR,sBADzG,sLAC8U,oBAAC,UAAD,EAAY,IAAI,CAAC,SAAjB,EAD9U,oBA5BF,cA+BE,mDACsB,IADtB,+BA/BF,cAkCE,oBAAC,SAAD,EAAW,IAAI,CAAC,sJAAhB,EAlCF,cAmCE,mEACwB,oBAAC,UAAD,EAAY,IAAI,CAAC,GAAjB,EADxB,sCACmE,oBAAC,UAAD,EAAY,IAAI,CAAC,GAAjB,EADnE,gCACwG,oBAAC,UAAD,EAAY,IAAI,CAAC,cAAjB,EADxG,sBAC6I,oBAAC,UAAD,EAAY,IAAI,CAAC,kBAAjB,EAD7I,KAnCF,cAsCE,iVAtCF,cAyCE,kDACM,oBAAC,UAAD,EAAY,IAAI,CAAC,gBAAjB,EADN,qBAC4C,oBAAC,UAAD,EAAY,IAAI,CAAC,GAAjB,EAD5C,wDAC0G,oBAAC,UAAD,EAAY,IAAI,CAAC,0DAAjB,EAD1G,sCACuM,oBAAC,UAAD,EAAY,IAAI,CAAC,GAAjB,EADvM,2DACwQ,oBAAC,UAAD,EAAY,IAAI,CAAC,0BAAjB,EADxQ,KAzCF,cA4CE,oBAAC,SAAD,EAAW,IAAI,CAAC,wGAAhB,EA5CF,cA6CE,qCACQ,IADR,+BA7CF,cAiDE,8BAjDF,cAiDO,6HAC6E,oBAAC,UAAD,EAAY,IAAI,CAAC,GAAjB,EAD7E,8BACgH,oBAAC,UAAD,EAAY,IAAI,CAAC,WAAjB,EADhH,0BACsJ,oBAAC,UAAD,EAAY,IAAI,CAAC,WAAjB,EADtJ,2DAC6N,oBAAC,UAAD,EAAY,IAAI,CAAC,WAAjB,EAD7N,uCACgR,oBAAC,UAAD,EAAY,IAAI,CAAC,WAAjB,EADhR,sCAjDP,cAqDE,8BAAQ,KAAK,CAAE,CAAE,mBAAoB,OAAtB,CAA+B,UAAW,MAA1C,CAAkD,gBAAiB,MAAnE,CAAf,eACE,2BAAK,GAAG,CAAC,kCAAT,CAA4C,KAAK,CAAC,KAAlD,EADF,CArDF,cAyDE,8BAzDF,cAyDO,wCACM,IADN,oGAC8F,oBAAC,UAAD,EAAY,IAAI,CAAC,GAAjB,EAD9F,wBACyI,IADzI,0DACuL,4CADvL,4DACsP,4CADtP,wHACiX,oBAAC,UAAD,EAAY,IAAI,CAAC,GAAjB,EADjX,6DACmb,oBAAC,UAAD,EAAY,IAAI,CAAC,0BAAjB,EADnb,QACme,IADne,sBAC6e,oCAD7e,cAzDP,cA6DE,8BAAQ,KAAK,CAAE,CAAE,mBAAoB,OAAtB,CAA+B,UAAW,MAA1C,CAAkD,gBAAiB,MAAnE,CAAf,eACE,2BAAK,GAAG,CAAC,kCAAT,CAA4C,KAAK,CAAC,KAAlD,EADF,CA7DF,cAiEE,8BAjEF,cAiEO,8EAC8B,oBAAC,UAAD,EAAY,IAAI,CAAC,WAAjB,EAD9B,8BACwE,oBAAC,UAAD,EAAY,IAAI,CAAC,WAAjB,EADxE,8BACkH,oBAAC,UAAD,EAAY,IAAI,CAAC,WAAjB,EADlH,8BAC4J,oBAAC,UAAD,EAAY,IAAI,CAAC,0BAAjB,EAD5J,8CACoO,oBAAC,UAAD,EAAY,IAAI,CAAC,WAAjB,EADpO,yCACyR,oBAAC,UAAD,EAAY,IAAI,CAAC,0BAAjB,EADzR,2KAC8d,oBAAC,UAAD,EAAY,IAAI,CAAC,0BAAjB,EAD9d,qBAC2hB,IAD3hB,qDACokB,oBAAC,UAAD,EAAY,IAAI,CAAC,WAAjB,EADpkB,oCACkoB,IADloB,+CACqqB,oBAAC,UAAD,EAAY,IAAI,CAAC,WAAjB,EADrqB,sCACutB,oBAAC,UAAD,EAAY,IAAI,CAAC,0BAAjB,EADvtB,kCACmxB,oBAAC,UAAD,EAAY,IAAI,CAAC,GAAjB,EADnxB,gBAjEP,cAqEE,8BAAQ,KAAK,CAAE,CAAE,mBAAoB,OAAtB,CAA+B,UAAW,MAA1C,CAAkD,gBAAiB,MAAnE,CAAf,eACE,2BAAK,GAAG,CAAC,kCAAT,CAA4C,KAAK,CAAC,KAAlD,EADF,CArEF,cAyEE,8BAzEF,cAyEO,2JAC2G,oBAAC,UAAD,EAAY,IAAI,CAAC,GAAjB,EAD3G,sKAzEP,cA6EE,8BAAQ,KAAK,CAAE,CAAE,mBAAoB,OAAtB,CAA+B,UAAW,MAA1C,CAAkD,gBAAiB,MAAnE,CAAf,eACE,2BAAK,GAAG,CAAC,kCAAT,CAA4C,KAAK,CAAC,KAAlD,EADF,CA7EF,cAiFE,8BAjFF,cAiFO,qGACqD,oBAAC,UAAD,EAAY,IAAI,CAAC,0BAAjB,EADrD,wDACuI,oBAAC,UAAD,EAAY,IAAI,CAAC,WAAjB,EADvI,sBACyK,oBAAC,UAAD,EAAY,IAAI,CAAC,SAAjB,EADzK,6CAjFP,cAqFE,8BAAQ,KAAK,CAAE,CAAE,mBAAoB,OAAtB,CAA+B,UAAW,MAA1C,CAAkD,gBAAiB,MAAnE,CAAf,eACE,2BAAK,GAAG,CAAC,kCAAT,CAA4C,KAAK,CAAC,KAAlD,EADF,CArFF,cAyFE,8BAzFF,cAyFO,qNACqK,oBAAC,UAAD,EAAY,IAAI,CAAC,WAAjB,EADrK,sBACuM,oBAAC,UAAD,EAAY,IAAI,CAAC,SAAjB,EADvM,uOAzFP,cA6FE,8BAAQ,KAAK,CAAE,CAAE,mBAAoB,OAAtB,CAA+B,UAAW,MAA1C,CAAkD,gBAAiB,MAAnE,CAAf,eACE,2BAAK,GAAG,CAAC,kCAAT,CAA4C,KAAK,CAAC,KAAlD,EADF,CA7FF,cAiGE,0CAjGF,cAoGE,8NACmL,yBAAG,IAAI,CAAC,qDAAR,SADnL,wDApGF,CADF,CA0GH,CAED,cAAeA,CAAAA,IAAf","sourcesContent":["import React from 'react';\nimport 'katex/dist/katex.min.css';\nimport { InlineMath, BlockMath } from 'react-katex';\n\n\nfunction Post() {\n    return (\n      <>\n        <h2>\n          Deleting Data from Machine Learning Models\n        </h2>\n        <p>\n          Here{\"\\'\"}s an interesting problem: let{\"\\'\"}s imagine your data was used to train some machine learning model. Now, you want to request that your data be <em>unlearned</em> from the model. Roughly speaking, the model is updated so that its parameters and outputs have no knowledge of your data anymore.\n        </p>\n        <p>\n          For <InlineMath math=\"k\"/>-nearest neighbors, this problem is trivial - simply remove the point from the dataset. For something more complex like a neural network, it{\"\\'\"}s not immediately clear what you would do. Is there some interesting middle-ground?\n        </p>\n        <p>\n          First, let{\"\\'\"}s define the problem. Consider the two following scenarios:\n        </p>\n        <ul>\n          <li>\n          <em>\n            Case A (Real): We train the model on the full dataset containing <InlineMath math=\"x\"/>, we remove <InlineMath math=\"x\"/> from the model parameters, and then we publish the model to the public.\n          </em>\n          </li>\n          <li>\n          <em>\n            Case B (Imaginary): The point <InlineMath math=\"x\"/> was never in the dataset, we train the model on the dataset (which doesn{\"\\'\"}t contain <InlineMath math=\"x\"/>), and then we publish the model to the public.\n          </em>\n          </li>\n        </ul>\n        <p>\n          Here{\"\\'\"}s what it will mean to be able to \"unlearn\": if we can find a procedure for Case A which yields an indistinguishable outcome to Case B, then we will have achieved our goal. Meaning, if we can train and then delete, and the outcome of this is indistinguishable from the case where the point was never in the dataset at all, that would be an effective unlearning algorithm.\n        </p>\n        <p>\n          Believe it or not, this is possible in certain cases! For the purposes of this post, I will detail the <a href=\"https://arxiv.org/abs/2007.02923\">descent-to-delete</a> approach by Neel et al. To achieve this, we are going to tackle the case of a convex parametric model, e.g., logistic regression with binary cross entropy loss and <InlineMath math=\"\\ell_2\" /> regularization.\n        </p>\n        <p>\n          To be concrete, let{\"\\'\"}s say our loss function is:\n        </p>\n        <BlockMath math=\"\\ell(\\theta) = - \\frac{1}{n} \\left( \\sum_{i \\in [n]} y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right) + \\lambda ||\\theta||_2\"/>\n        <p>\n          This loss function is <InlineMath math=\"m\"/>-strongly convex and <InlineMath math=\"M\"/>-smooth, where <InlineMath math=\"m = \\lambda\"/> and <InlineMath math=\"M = 4 - \\lambda\"/>.\n        </p>\n        <p>\n          Now, the great part about working in this setting is that we have provable convergence guarantees. That is, as long as we know how far we are at most from the global optimum, if we run (projected) gradient descent for some number of steps, we know we will be at most some distance away. To formalize this:\n        </p>\n        <em>\n          Let <InlineMath math=\"\\ell(\\theta)\" /> be <InlineMath math=\"m\" />-strongly convex and M-smooth, and let <InlineMath math=\"\\theta^* = argmin_{\\theta \\in \\Theta} \\ell(\\theta)\" />. We have that after <InlineMath math=\"T\" /> steps of gradient descent with step size <InlineMath math=\"\\eta = \\frac{2}{m + M}\" />:\n        </em>\n        <BlockMath math=\"||\\theta_T - \\theta^*||_2 \\leq \\left( \\frac{M - m}{M + m} \\right)^T || \\theta_0 - \\theta^*||_2\" />\n        <p>\n          Now I{\"\\'\"}ll introduce the algorithm.\n        </p>\n\n        <br/><p>\n          We begin with Case A, where we perform gradient descent on the full dataset for <InlineMath math=\"t\"/> steps until <InlineMath math=\"\\theta_0\"/> becomes <InlineMath math=\"\\theta_t\"/>. The dotted line represents how far away <InlineMath math=\"\\theta_t\"/> can possibly be from <InlineMath math=\"\\theta^D\"/> due to our convergence guarantee.\n        </p>\n\n        <center style={{ 'background-color': 'white', 'padding': '20px', 'border-radius': '20px' }}>\n          <img src=\"../machine-unlearning/image2.svg\" width=\"50%\"/>\n        </center>\n\n        <br/><p>\n          Now, let{\"\\'\"}s say that at this point in training, we receive a deletion request for some point <InlineMath math=\"x\"/>. To handle this, we{\"\\'\"}ll first need to establish the notion of <em>sensitivity</em>. If we have some global optimum, then the <em>sensitivity</em> is the furthest away the global minimum can move due to the removal of a single point. When we remove <InlineMath math=\"x\"/>, we know that the resulting global minimum <InlineMath math=\"\\theta^{D \\setminus x}\"/> can{\"\\'\"}t be <em>too</em> far away.\n        </p>\n\n        <center style={{ 'background-color': 'white', 'padding': '20px', 'border-radius': '20px' }}>\n          <img src=\"../machine-unlearning/image1.svg\" width=\"50%\"/>\n        </center>\n\n        <br/><p>\n          Given this, we know how far away <InlineMath math=\"\\theta_t\"/> can be from <InlineMath math=\"\\theta^D\"/> and how far <InlineMath math=\"\\theta^D\"/> can be from <InlineMath math=\"\\theta^{D \\setminus x}\"/>. Therefore, we know how far <InlineMath math=\"\\theta_t\"/> could possibly be from <InlineMath math=\"\\theta^{D \\setminus x}\"/>. Given this information, we can again apply the convergence guarantee and perform gradient descent for some required number of steps in the direction of <InlineMath math=\"\\theta^{D \\setminus x}\"/> until we know we{\"\\'\"}re some distance away, resulting in <InlineMath math=\"\\theta_T\"/>. This finishes Case A, where we{\"\\'\"}ve gone from an initial point <InlineMath math=\"\\theta_0\"/> and gotten close to <InlineMath math=\"\\theta^{D \\setminus x}\"/> without knowing <InlineMath math=\"x\"/> beforehand.\n        </p>\n\n        <center style={{ 'background-color': 'white', 'padding': '20px', 'border-radius': '20px' }}>\n          <img src=\"../machine-unlearning/image3.svg\" width=\"50%\"/>\n        </center>\n\n        <br/><p>\n          Now we consider alternative reality, Case B. That is, we start from our initial point and train regularly but <InlineMath math=\"x\"/> was never in the dataset. Given our convergence guarantee, again we can know after some number of steps that we are at least within some distance of the optimum.\n        </p>\n\n        <center style={{ 'background-color': 'white', 'padding': '20px', 'border-radius': '20px' }}>\n          <img src=\"../machine-unlearning/image4.svg\" width=\"50%\"/>\n        </center>\n\n        <br/><p>\n          Now given that we can guarantee a certain distance from <InlineMath math=\"\\theta^{D \\setminus x}\"/> in either case, we can guarantee that <InlineMath math=\"\\theta_T\"/> and <InlineMath math=\"\\theta\"/> are within some distance of one another.\n        </p>\n\n        <center style={{ 'background-color': 'white', 'padding': '20px', 'border-radius': '20px' }}>\n          <img src=\"../machine-unlearning/image5.svg\" width=\"50%\"/>\n        </center>\n\n        <br/><p>\n          Finally, in either case, we publish the models according to the same publishing scheme, namely an injection of Gaussian noise to the model parameters scaled to how far <InlineMath math=\"\\theta_T\"/> and <InlineMath math=\"\\theta\"/> could possibly be from one another. All of this entails that both outcomes are statistically indistinguishable from one another, in the sense that the probability distributions over both models are \"approximately identical\".\n        </p>\n\n        <center style={{ 'background-color': 'white', 'padding': '20px', 'border-radius': '20px' }}>\n          <img src=\"../machine-unlearning/image6.svg\" width=\"50%\"/>\n        </center>\n\n        <h3>\n          Conlusion\n        </h3>\n        <p>\n          Hopefully this was a useful introduction into the problem of machine unlearning and how it can be addressed. For further reading, make sure to check out the blog post detailing <a href=\"http://www.cleverhans.io/2020/07/20/unlearning.html\">SISA</a>, an alternativ approach proposed by Papernot et al.\n        </p>\n      </>\n    );\n}\n\nexport default Post;\n"]},"metadata":{},"sourceType":"module"}