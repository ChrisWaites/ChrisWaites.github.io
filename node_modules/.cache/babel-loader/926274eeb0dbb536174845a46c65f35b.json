{"ast":null,"code":"var _jsxFileName = \"/Users/chriswaites/Documents/projects/ChrisWaites.github.io/src/posts/machine-unlearning.js\";\nimport React from 'react';\nimport 'katex/dist/katex.min.css';\nimport { InlineMath, BlockMath } from 'react-katex';\n\nfunction Post() {\n  return /*#__PURE__*/React.createElement(React.Fragment, null, /*#__PURE__*/React.createElement(\"h2\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 9,\n      columnNumber: 9\n    }\n  }, \"Deleting Data from Machine Learning Models\"), /*#__PURE__*/React.createElement(BlockMath, {\n    math: \"\\\\theta \\\\leftarrow \\\\theta - \\\\eta \\\\tilde{g}\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 12,\n      columnNumber: 9\n    }\n  }), /*#__PURE__*/React.createElement(\"p\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 13,\n      columnNumber: 9\n    }\n  }, \"Let \", /*#__PURE__*/React.createElement(InlineMath, {\n    math: \"\\\\ell\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 14,\n      columnNumber: 15\n    }\n  }), \" be m-strongly convex and M-smooth, and let \", /*#__PURE__*/React.createElement(InlineMath, {\n    math: \"\\\\theta^* = argmin_{\\\\theta \\\\in \\\\Theta} \\\\ell(\\\\theta)\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 14,\n      columnNumber: 85\n    }\n  }), \". We have that after \", /*#__PURE__*/React.createElement(InlineMath, {\n    math: \"T\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 14,\n      columnNumber: 178\n    }\n  }), \" steps of gradient descent with step size \", /*#__PURE__*/React.createElement(InlineMath, {\n    math: \"\\\\eta = \\\\frac{2}{m + M}\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 14,\n      columnNumber: 243\n    }\n  }), \",\"), /*#__PURE__*/React.createElement(BlockMath, {\n    math: \"||\\\\theta_T - \\\\theta^*||_2 \\\\leq \\\\left( \\\\frac{M - m}{M + m}^T \\\\right) || \\\\theta_0 - \\\\theta^*||_2\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 16,\n      columnNumber: 9\n    }\n  }), /*#__PURE__*/React.createElement(\"h3\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 18,\n      columnNumber: 9\n    }\n  }, \"Conlusion\"), /*#__PURE__*/React.createElement(\"p\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 21,\n      columnNumber: 9\n    }\n  }, \"Hopefully this was a useful tutorial outlining differential privacy and its applications to deep learning in plain English. If you have any questions or have caught any errors, feel free to reach out!\"));\n}\n\nexport default Post;","map":{"version":3,"sources":["/Users/chriswaites/Documents/projects/ChrisWaites.github.io/src/posts/machine-unlearning.js"],"names":["React","InlineMath","BlockMath","Post"],"mappings":";AAAA,OAAOA,KAAP,MAAkB,OAAlB;AACA,OAAO,0BAAP;AACA,SAASC,UAAT,EAAqBC,SAArB,QAAsC,aAAtC;;AAGA,SAASC,IAAT,GAAgB;AACZ,sBACE,uDACE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,kDADF,eAIE,oBAAC,SAAD;AAAW,IAAA,IAAI,EAAC,gDAAhB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IAJF,eAKE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,0BACM,oBAAC,UAAD;AAAY,IAAA,IAAI,EAAC,OAAjB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IADN,+DAC4E,oBAAC,UAAD;AAAY,IAAA,IAAI,EAAC,0DAAjB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IAD5E,wCACyK,oBAAC,UAAD;AAAY,IAAA,IAAI,EAAC,GAAjB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IADzK,6DAC0O,oBAAC,UAAD;AAAY,IAAA,IAAI,EAAC,0BAAjB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IAD1O,MALF,eAQE,oBAAC,SAAD;AAAW,IAAA,IAAI,EAAC,wGAAhB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IARF,eAUE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,iBAVF,eAaE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,gNAbF,CADF;AAmBH;;AAED,eAAeA,IAAf","sourcesContent":["import React from 'react';\nimport 'katex/dist/katex.min.css';\nimport { InlineMath, BlockMath } from 'react-katex';\n\n\nfunction Post() {\n    return (\n      <>\n        <h2>\n          Deleting Data from Machine Learning Models\n        </h2>\n        <BlockMath math=\"\\theta \\leftarrow \\theta - \\eta \\tilde{g}\" />\n        <p>\n          Let <InlineMath math=\"\\ell\" /> be m-strongly convex and M-smooth, and let <InlineMath math=\"\\theta^* = argmin_{\\theta \\in \\Theta} \\ell(\\theta)\" />. We have that after <InlineMath math=\"T\" /> steps of gradient descent with step size <InlineMath math=\"\\eta = \\frac{2}{m + M}\" />,\n        </p>\n        <BlockMath math=\"||\\theta_T - \\theta^*||_2 \\leq \\left( \\frac{M - m}{M + m}^T \\right) || \\theta_0 - \\theta^*||_2\" />\n\n        <h3>\n          Conlusion\n        </h3>\n        <p>\n          Hopefully this was a useful tutorial outlining differential privacy and its applications to deep learning in plain English. If you have any questions or have caught any errors, feel free to reach out!\n        </p>\n      </>\n    );\n}\n\nexport default Post;\n"]},"metadata":{},"sourceType":"module"}