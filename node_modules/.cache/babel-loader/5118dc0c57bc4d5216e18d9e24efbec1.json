{"ast":null,"code":"import React from'react';import'katex/dist/katex.min.css';import{InlineMath,BlockMath}from'react-katex';function Post(){return/*#__PURE__*/React.createElement(React.Fragment,null,/*#__PURE__*/React.createElement(\"h2\",null,\"Machine Unlearning\"),/*#__PURE__*/React.createElement(\"p\",null,\"Here\",\"\\'\",\"s an interesting problem: let\",\"\\'\",\"s imagine your data was used to train some machine learning model. Now, you want to request that your data be \",/*#__PURE__*/React.createElement(\"em\",null,\"unlearned\"),\" from the model. That is, the model is updated so that its parameters and outputs have no knowledge of you anymore.\"),/*#__PURE__*/React.createElement(\"p\",null,\"For \",/*#__PURE__*/React.createElement(InlineMath,{math:\"k\"}),\"-nearest neighbors, the problem is trivial - simply remove the point from the data. For something more complex like a neural network, it\",\"\\'\",\"s not immediately clear what you would do. Is their an interesting middle-ground?\"),/*#__PURE__*/React.createElement(\"p\",null,\"First, let\",\"\\'\",\"s define the problem. Consider the two following scenarios:\"),/*#__PURE__*/React.createElement(\"ul\",null,/*#__PURE__*/React.createElement(\"li\",null,/*#__PURE__*/React.createElement(\"em\",null,\"Case A (Real): We train the model on the full dataset containing \",/*#__PURE__*/React.createElement(InlineMath,{math:\"x\"}),\", we remove \",/*#__PURE__*/React.createElement(InlineMath,{math:\"x\"}),\" from the model parameters, and then we publish the model to the public.\")),/*#__PURE__*/React.createElement(\"li\",null,/*#__PURE__*/React.createElement(\"em\",null,\"Case B (Imaginary): The point \",/*#__PURE__*/React.createElement(InlineMath,{math:\"x\"}),\" was never in the dataset, we train the model on the dataset (which doesn\",\"\\'\",\"t contain \",/*#__PURE__*/React.createElement(InlineMath,{math:\"x\"}),\"), and then publish the model to the public.\"))),/*#__PURE__*/React.createElement(\"p\",null,\"If we can find a procedure for Case A which yields an indistinguishable outcome to Case B, then we will have achieved our goal. In this respect, I will detail the \",/*#__PURE__*/React.createElement(\"a\",{href:\"https://arxiv.org/abs/2007.02923\"},\"descent-to-delete\"),\" approach by Neel et al.\"),/*#__PURE__*/React.createElement(\"p\",null,\"For the purposes of this post, we are going to tackle the case of a convex parametric model. For example, this would include logistic regression with binary cross entropy loss and \",/*#__PURE__*/React.createElement(InlineMath,{math:\"\\\\ell_2\"}),\" regularization.\"),/*#__PURE__*/React.createElement(\"p\",null,\"To be concrete, let\",\"\\'\",\"s say our loss function is:\"),/*#__PURE__*/React.createElement(BlockMath,{math:\"\\\\ell(\\\\theta) = - \\\\frac{1}{n} \\\\left( \\\\sum_{i \\\\in [n]} y_i \\\\log(\\\\hat{y}_i) + (1 - y_i) \\\\log(1 - \\\\hat{y}_i) \\\\right) + \\\\lambda ||\\\\theta||_2\"}),/*#__PURE__*/React.createElement(\"p\",null,\"Then we would have that our loss function is \",/*#__PURE__*/React.createElement(InlineMath,{math:\"m\"}),\"-strongly convex and \",/*#__PURE__*/React.createElement(InlineMath,{math:\"M\"}),\"-smooth where \",/*#__PURE__*/React.createElement(InlineMath,{math:\"m = \\\\lambda\"}),\" and \",/*#__PURE__*/React.createElement(InlineMath,{math:\"M = 4 - \\\\lambda\"}),\".\"),/*#__PURE__*/React.createElement(\"p\",null,\"Now, the great part about working in this setting is that we have convergence guarantees. That is, as long as we know how far we are at most from the global optimum, if we run gradient descent, we know we will be at most some distance after some number of steps we take.\"),/*#__PURE__*/React.createElement(\"em\",null,\"Let \",/*#__PURE__*/React.createElement(InlineMath,{math:\"\\\\ell(\\\\theta)\"}),\" be \",/*#__PURE__*/React.createElement(InlineMath,{math:\"m\"}),\"-strongly convex and M-smooth, and let \",/*#__PURE__*/React.createElement(InlineMath,{math:\"\\\\theta^* = argmin_{\\\\theta \\\\in \\\\Theta} \\\\ell(\\\\theta)\"}),\". We have that after \",/*#__PURE__*/React.createElement(InlineMath,{math:\"T\"}),\" steps of gradient descent with step size \",/*#__PURE__*/React.createElement(InlineMath,{math:\"\\\\eta = \\\\frac{2}{m + M}\"}),\":\"),/*#__PURE__*/React.createElement(BlockMath,{math:\"||\\\\theta_T - \\\\theta^*||_2 \\\\leq \\\\left( \\\\frac{M - m}{M + m} \\\\right)^T || \\\\theta_0 - \\\\theta^*||_2\"}),/*#__PURE__*/React.createElement(\"p\",null,\"Now, for sake of intuition, I will give a geometric argument for showing how we can get some notion of machine unlearning to work.\"),/*#__PURE__*/React.createElement(\"center\",{style:{'background-color':'white','padding':'20px','border-radius':'20px'}},/*#__PURE__*/React.createElement(\"img\",{src:\"../machine-unlearning/image2.svg\",width:\"50%\"})),/*#__PURE__*/React.createElement(\"br\",null),/*#__PURE__*/React.createElement(\"p\",null,\"We begin with Case A, where we perform gradient descent on the full dataset for \",/*#__PURE__*/React.createElement(InlineMath,{math:\"t\"}),\" steps until \",/*#__PURE__*/React.createElement(InlineMath,{math:\"\\\\theta_0\"}),\" becomes \",/*#__PURE__*/React.createElement(InlineMath,{math:\"\\\\theta_t\"}),\". The dotted line represents how far away \",/*#__PURE__*/React.createElement(InlineMath,{math:\"\\\\theta_t\"}),\" can possibly be from \",/*#__PURE__*/React.createElement(InlineMath,{math:\"\\\\theta^D\"}),\" due to our convergence guarantee.\"),/*#__PURE__*/React.createElement(\"center\",{style:{'background-color':'white','padding':'20px','border-radius':'20px'}},/*#__PURE__*/React.createElement(\"img\",{src:\"../machine-unlearning/image1.svg\",width:\"50%\"})),/*#__PURE__*/React.createElement(\"br\",null),/*#__PURE__*/React.createElement(\"p\",null,\"Now, let\",\"\\'\",\"s say that this point in training we receive a deletion request for point \",/*#__PURE__*/React.createElement(InlineMath,{math:\"x\"}),\". To handle this, we\",\"\\'\",\"ll need to establish the notion of \",/*#__PURE__*/React.createElement(\"em\",null,\"sensitivity\"),\". That is, if we have some global optimum across the entire dataset \",/*#__PURE__*/React.createElement(InlineMath,{math:\"\\\\theta^D\"}),\", then the \",/*#__PURE__*/React.createElement(\"em\",null,\"sensitivity\"),\" is the furthest away the global minimum can move due to the removal of a single point. When we remove \",/*#__PURE__*/React.createElement(InlineMath,{math:\"x\"}),\", we know that the resulting global minimum \",/*#__PURE__*/React.createElement(InlineMath,{math:\"\\\\theta^{D \\\\setminus x}\"}),\" can\",\"\\'\",\"t be \",/*#__PURE__*/React.createElement(\"em\",null,\"too\"),\" far away.\"),/*#__PURE__*/React.createElement(\"center\",{style:{'background-color':'white','padding':'20px','border-radius':'20px'}},/*#__PURE__*/React.createElement(\"img\",{src:\"../machine-unlearning/image3.svg\",width:\"50%\"})),/*#__PURE__*/React.createElement(\"br\",null),/*#__PURE__*/React.createElement(\"p\",null,\"Given this, we know how far away \",/*#__PURE__*/React.createElement(InlineMath,{math:\"\\\\theta_t\"}),\" can be from \",/*#__PURE__*/React.createElement(InlineMath,{math:\"\\\\theta^D\"}),\" and how far \",/*#__PURE__*/React.createElement(InlineMath,{math:\"\\\\theta^D\"}),\" can be from \",/*#__PURE__*/React.createElement(InlineMath,{math:\"\\\\theta^{D \\\\setminus x}\"}),\". Therefore, we know how far \",/*#__PURE__*/React.createElement(InlineMath,{math:\"\\\\theta_t\"}),\" could possibly be from \",/*#__PURE__*/React.createElement(InlineMath,{math:\"\\\\theta^{D \\\\setminus x}\"}),\". Given this information, we can again apply the convergence guarantee and perform gradient descent for some required number of steps in the direction of \",/*#__PURE__*/React.createElement(InlineMath,{math:\"\\\\theta^{D \\\\setminus x}\"}),\" until we know we\",\"\\'\",\"re some distance away, resulting in \",/*#__PURE__*/React.createElement(InlineMath,{math:\"\\\\theta_T\"}),\". This finishes Case A, where we\",\"\\'\",\"ve gone from an initial point \",/*#__PURE__*/React.createElement(InlineMath,{math:\"\\\\theta_0\"}),\" and gotten close to \",/*#__PURE__*/React.createElement(InlineMath,{math:\"\\\\theta^{D \\\\setminus x}\"}),\" without knowing \",/*#__PURE__*/React.createElement(InlineMath,{math:\"x\"}),\" beforehand.\"),/*#__PURE__*/React.createElement(\"center\",{style:{'background-color':'white','padding':'20px','border-radius':'20px'}},/*#__PURE__*/React.createElement(\"img\",{src:\"../machine-unlearning/image4.svg\",width:\"50%\"})),/*#__PURE__*/React.createElement(\"br\",null),/*#__PURE__*/React.createElement(\"p\",null,\"Now we consider alternative reality, Case B. That is, we start from our initial point and train regularly but \",/*#__PURE__*/React.createElement(InlineMath,{math:\"x\"}),\" was never in the dataset. Given our convergence guarantee, again we can know after some number of steps that we are at least within some distance of the optimum.\"),/*#__PURE__*/React.createElement(\"center\",{style:{'background-color':'white','padding':'20px','border-radius':'20px'}},/*#__PURE__*/React.createElement(\"img\",{src:\"../machine-unlearning/image5.svg\",width:\"50%\"})),/*#__PURE__*/React.createElement(\"br\",null),/*#__PURE__*/React.createElement(\"p\",null,\"Now given that we can guarantee a certain distance from \",/*#__PURE__*/React.createElement(InlineMath,{math:\"\\\\theta^{D \\\\setminus x}\"}),\" in either case, we can guarantee that \",/*#__PURE__*/React.createElement(InlineMath,{math:\"\\\\theta_T\"}),\" and \",/*#__PURE__*/React.createElement(InlineMath,{math:\"\\\\theta\"}),\" are within some distance of one another.\"),/*#__PURE__*/React.createElement(\"center\",{style:{'background-color':'white','padding':'20px','border-radius':'20px'}},/*#__PURE__*/React.createElement(\"img\",{src:\"../machine-unlearning/image6.svg\",width:\"50%\"})),/*#__PURE__*/React.createElement(\"br\",null),/*#__PURE__*/React.createElement(\"p\",null,\"Finally, in either case, we publish the models according to the same publishing scheme, namely an injection of Gaussian noise to the model parameters scaled to how far \",/*#__PURE__*/React.createElement(InlineMath,{math:\"\\\\theta_T\"}),\" and \",/*#__PURE__*/React.createElement(InlineMath,{math:\"\\\\theta\"}),\" could possibly be from one another. All of this entails that both outcomes are statistically indistinguishable from one another.\"),/*#__PURE__*/React.createElement(\"h3\",null,\"Conlusion\"),/*#__PURE__*/React.createElement(\"p\",null,\"Hopefully this was a useful introduction into the problem of machine unlearning and how it can be addressed. For further reading, make sure to check out the amazing blog post about an alternative approach, \",/*#__PURE__*/React.createElement(\"a\",{href:\"http://www.cleverhans.io/2020/07/20/unlearning.html\"},\"SISA\"),\", proposed by Papernot et al.\"));}export default Post;","map":{"version":3,"sources":["/Users/chriswaites/Documents/projects/ChrisWaites.github.io/src/posts/machine-unlearning.js"],"names":["React","InlineMath","BlockMath","Post"],"mappings":"AAAA,MAAOA,CAAAA,KAAP,KAAkB,OAAlB,CACA,MAAO,0BAAP,CACA,OAASC,UAAT,CAAqBC,SAArB,KAAsC,aAAtC,CAGA,QAASC,CAAAA,IAAT,EAAgB,CACZ,mBACE,qDACE,mDADF,cAIE,oCACO,IADP,iCAC0C,IAD1C,+HAC6J,0CAD7J,uHAJF,cAOE,iDACM,oBAAC,UAAD,EAAY,IAAI,CAAC,GAAjB,EADN,4IACqK,IADrK,qFAPF,cAUE,0CACa,IADb,+DAVF,cAaE,2CACE,2CACA,+GACmE,oBAAC,UAAD,EAAY,IAAI,CAAC,GAAjB,EADnE,6BACqG,oBAAC,UAAD,EAAY,IAAI,CAAC,GAAjB,EADrG,4EADA,CADF,cAME,2CACA,4EACgC,oBAAC,UAAD,EAAY,IAAI,CAAC,GAAjB,EADhC,6EACgI,IADhI,2BAC+I,oBAAC,UAAD,EAAY,IAAI,CAAC,GAAjB,EAD/I,gDADA,CANF,CAbF,cAyBE,gNACqK,yBAAG,IAAI,CAAC,kCAAR,sBADrK,4BAzBF,cA4BE,iOACsL,oBAAC,UAAD,EAAY,IAAI,CAAC,SAAjB,EADtL,oBA5BF,cA+BE,mDACsB,IADtB,+BA/BF,cAkCE,oBAAC,SAAD,EAAW,IAAI,CAAC,sJAAhB,EAlCF,cAmCE,0FAC+C,oBAAC,UAAD,EAAY,IAAI,CAAC,GAAjB,EAD/C,sCAC0F,oBAAC,UAAD,EAAY,IAAI,CAAC,GAAjB,EAD1F,+BAC8H,oBAAC,UAAD,EAAY,IAAI,CAAC,cAAjB,EAD9H,sBACmK,oBAAC,UAAD,EAAY,IAAI,CAAC,kBAAjB,EADnK,KAnCF,cAsCE,8SAtCF,cAyCE,kDACM,oBAAC,UAAD,EAAY,IAAI,CAAC,gBAAjB,EADN,qBAC4C,oBAAC,UAAD,EAAY,IAAI,CAAC,GAAjB,EAD5C,wDAC0G,oBAAC,UAAD,EAAY,IAAI,CAAC,0DAAjB,EAD1G,sCACuM,oBAAC,UAAD,EAAY,IAAI,CAAC,GAAjB,EADvM,2DACwQ,oBAAC,UAAD,EAAY,IAAI,CAAC,0BAAjB,EADxQ,KAzCF,cA4CE,oBAAC,SAAD,EAAW,IAAI,CAAC,wGAAhB,EA5CF,cA6CE,kKA7CF,cAiDE,8BAAQ,KAAK,CAAE,CAAE,mBAAoB,OAAtB,CAA+B,UAAW,MAA1C,CAAkD,gBAAiB,MAAnE,CAAf,eACE,2BAAK,GAAG,CAAC,kCAAT,CAA4C,KAAK,CAAC,KAAlD,EADF,CAjDF,cAqDE,8BArDF,cAqDO,6HAC6E,oBAAC,UAAD,EAAY,IAAI,CAAC,GAAjB,EAD7E,8BACgH,oBAAC,UAAD,EAAY,IAAI,CAAC,WAAjB,EADhH,0BACsJ,oBAAC,UAAD,EAAY,IAAI,CAAC,WAAjB,EADtJ,2DAC6N,oBAAC,UAAD,EAAY,IAAI,CAAC,WAAjB,EAD7N,uCACgR,oBAAC,UAAD,EAAY,IAAI,CAAC,WAAjB,EADhR,sCArDP,cAyDE,8BAAQ,KAAK,CAAE,CAAE,mBAAoB,OAAtB,CAA+B,UAAW,MAA1C,CAAkD,gBAAiB,MAAnE,CAAf,eACE,2BAAK,GAAG,CAAC,kCAAT,CAA4C,KAAK,CAAC,KAAlD,EADF,CAzDF,cA6DE,8BA7DF,cA6DO,wCACM,IADN,2FACqF,oBAAC,UAAD,EAAY,IAAI,CAAC,GAAjB,EADrF,wBACgI,IADhI,oDACwK,4CADxK,qFACgQ,oBAAC,UAAD,EAAY,IAAI,CAAC,WAAjB,EADhQ,4BACwS,4CADxS,wHACma,oBAAC,UAAD,EAAY,IAAI,CAAC,GAAjB,EADna,6DACqe,oBAAC,UAAD,EAAY,IAAI,CAAC,0BAAjB,EADre,QACqhB,IADrhB,sBAC+hB,oCAD/hB,cA7DP,cAiEE,8BAAQ,KAAK,CAAE,CAAE,mBAAoB,OAAtB,CAA+B,UAAW,MAA1C,CAAkD,gBAAiB,MAAnE,CAAf,eACE,2BAAK,GAAG,CAAC,kCAAT,CAA4C,KAAK,CAAC,KAAlD,EADF,CAjEF,cAqEE,8BArEF,cAqEO,8EAC8B,oBAAC,UAAD,EAAY,IAAI,CAAC,WAAjB,EAD9B,8BACwE,oBAAC,UAAD,EAAY,IAAI,CAAC,WAAjB,EADxE,8BACkH,oBAAC,UAAD,EAAY,IAAI,CAAC,WAAjB,EADlH,8BAC4J,oBAAC,UAAD,EAAY,IAAI,CAAC,0BAAjB,EAD5J,8CACoO,oBAAC,UAAD,EAAY,IAAI,CAAC,WAAjB,EADpO,yCACyR,oBAAC,UAAD,EAAY,IAAI,CAAC,0BAAjB,EADzR,2KAC8d,oBAAC,UAAD,EAAY,IAAI,CAAC,0BAAjB,EAD9d,qBAC2hB,IAD3hB,qDACokB,oBAAC,UAAD,EAAY,IAAI,CAAC,WAAjB,EADpkB,oCACkoB,IADloB,+CACqqB,oBAAC,UAAD,EAAY,IAAI,CAAC,WAAjB,EADrqB,sCACutB,oBAAC,UAAD,EAAY,IAAI,CAAC,0BAAjB,EADvtB,kCACmxB,oBAAC,UAAD,EAAY,IAAI,CAAC,GAAjB,EADnxB,gBArEP,cAyEE,8BAAQ,KAAK,CAAE,CAAE,mBAAoB,OAAtB,CAA+B,UAAW,MAA1C,CAAkD,gBAAiB,MAAnE,CAAf,eACE,2BAAK,GAAG,CAAC,kCAAT,CAA4C,KAAK,CAAC,KAAlD,EADF,CAzEF,cA6EE,8BA7EF,cA6EO,2JAC2G,oBAAC,UAAD,EAAY,IAAI,CAAC,GAAjB,EAD3G,sKA7EP,cAiFE,8BAAQ,KAAK,CAAE,CAAE,mBAAoB,OAAtB,CAA+B,UAAW,MAA1C,CAAkD,gBAAiB,MAAnE,CAAf,eACE,2BAAK,GAAG,CAAC,kCAAT,CAA4C,KAAK,CAAC,KAAlD,EADF,CAjFF,cAqFE,8BArFF,cAqFO,qGACqD,oBAAC,UAAD,EAAY,IAAI,CAAC,0BAAjB,EADrD,wDACuI,oBAAC,UAAD,EAAY,IAAI,CAAC,WAAjB,EADvI,sBACyK,oBAAC,UAAD,EAAY,IAAI,CAAC,SAAjB,EADzK,6CArFP,cAyFE,8BAAQ,KAAK,CAAE,CAAE,mBAAoB,OAAtB,CAA+B,UAAW,MAA1C,CAAkD,gBAAiB,MAAnE,CAAf,eACE,2BAAK,GAAG,CAAC,kCAAT,CAA4C,KAAK,CAAC,KAAlD,EADF,CAzFF,cA6FE,8BA7FF,cA6FO,qNACqK,oBAAC,UAAD,EAAY,IAAI,CAAC,WAAjB,EADrK,sBACuM,oBAAC,UAAD,EAAY,IAAI,CAAC,SAAjB,EADvM,qIA7FP,cAiGE,0CAjGF,cAoGE,2PACgN,yBAAG,IAAI,CAAC,qDAAR,SADhN,iCApGF,CADF,CA0GH,CAED,cAAeA,CAAAA,IAAf","sourcesContent":["import React from 'react';\nimport 'katex/dist/katex.min.css';\nimport { InlineMath, BlockMath } from 'react-katex';\n\n\nfunction Post() {\n    return (\n      <>\n        <h2>\n          Machine Unlearning\n        </h2>\n        <p>\n          Here{\"\\'\"}s an interesting problem: let{\"\\'\"}s imagine your data was used to train some machine learning model. Now, you want to request that your data be <em>unlearned</em> from the model. That is, the model is updated so that its parameters and outputs have no knowledge of you anymore.\n        </p>\n        <p>\n          For <InlineMath math=\"k\"/>-nearest neighbors, the problem is trivial - simply remove the point from the data. For something more complex like a neural network, it{\"\\'\"}s not immediately clear what you would do. Is their an interesting middle-ground?\n        </p>\n        <p>\n          First, let{\"\\'\"}s define the problem. Consider the two following scenarios:\n        </p>\n        <ul>\n          <li>\n          <em>\n            Case A (Real): We train the model on the full dataset containing <InlineMath math=\"x\"/>, we remove <InlineMath math=\"x\"/> from the model parameters, and then we publish the model to the public.\n          </em>\n          </li>\n          <li>\n          <em>\n            Case B (Imaginary): The point <InlineMath math=\"x\"/> was never in the dataset, we train the model on the dataset (which doesn{\"\\'\"}t contain <InlineMath math=\"x\"/>), and then publish the model to the public.\n          </em>\n          </li>\n        </ul>\n        <p>\n          If we can find a procedure for Case A which yields an indistinguishable outcome to Case B, then we will have achieved our goal. In this respect, I will detail the <a href=\"https://arxiv.org/abs/2007.02923\">descent-to-delete</a> approach by Neel et al.\n        </p>\n        <p>\n          For the purposes of this post, we are going to tackle the case of a convex parametric model. For example, this would include logistic regression with binary cross entropy loss and <InlineMath math=\"\\ell_2\" /> regularization.\n        </p>\n        <p>\n          To be concrete, let{\"\\'\"}s say our loss function is:\n        </p>\n        <BlockMath math=\"\\ell(\\theta) = - \\frac{1}{n} \\left( \\sum_{i \\in [n]} y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right) + \\lambda ||\\theta||_2\"/>\n        <p>\n          Then we would have that our loss function is <InlineMath math=\"m\"/>-strongly convex and <InlineMath math=\"M\"/>-smooth where <InlineMath math=\"m = \\lambda\"/> and <InlineMath math=\"M = 4 - \\lambda\"/>.\n        </p>\n        <p>\n          Now, the great part about working in this setting is that we have convergence guarantees. That is, as long as we know how far we are at most from the global optimum, if we run gradient descent, we know we will be at most some distance after some number of steps we take.\n        </p>\n        <em>\n          Let <InlineMath math=\"\\ell(\\theta)\" /> be <InlineMath math=\"m\" />-strongly convex and M-smooth, and let <InlineMath math=\"\\theta^* = argmin_{\\theta \\in \\Theta} \\ell(\\theta)\" />. We have that after <InlineMath math=\"T\" /> steps of gradient descent with step size <InlineMath math=\"\\eta = \\frac{2}{m + M}\" />:\n        </em>\n        <BlockMath math=\"||\\theta_T - \\theta^*||_2 \\leq \\left( \\frac{M - m}{M + m} \\right)^T || \\theta_0 - \\theta^*||_2\" />\n        <p>\n          Now, for sake of intuition, I will give a geometric argument for showing how we can get some notion of machine unlearning to work.\n        </p>\n\n        <center style={{ 'background-color': 'white', 'padding': '20px', 'border-radius': '20px' }}>\n          <img src=\"../machine-unlearning/image2.svg\" width=\"50%\"/>\n        </center>\n\n        <br/><p>\n          We begin with Case A, where we perform gradient descent on the full dataset for <InlineMath math=\"t\"/> steps until <InlineMath math=\"\\theta_0\"/> becomes <InlineMath math=\"\\theta_t\"/>. The dotted line represents how far away <InlineMath math=\"\\theta_t\"/> can possibly be from <InlineMath math=\"\\theta^D\"/> due to our convergence guarantee.\n        </p>\n\n        <center style={{ 'background-color': 'white', 'padding': '20px', 'border-radius': '20px' }}>\n          <img src=\"../machine-unlearning/image1.svg\" width=\"50%\"/>\n        </center>\n\n        <br/><p>\n          Now, let{\"\\'\"}s say that this point in training we receive a deletion request for point <InlineMath math=\"x\"/>. To handle this, we{\"\\'\"}ll need to establish the notion of <em>sensitivity</em>. That is, if we have some global optimum across the entire dataset <InlineMath math=\"\\theta^D\"/>, then the <em>sensitivity</em> is the furthest away the global minimum can move due to the removal of a single point. When we remove <InlineMath math=\"x\"/>, we know that the resulting global minimum <InlineMath math=\"\\theta^{D \\setminus x}\"/> can{\"\\'\"}t be <em>too</em> far away.\n        </p>\n\n        <center style={{ 'background-color': 'white', 'padding': '20px', 'border-radius': '20px' }}>\n          <img src=\"../machine-unlearning/image3.svg\" width=\"50%\"/>\n        </center>\n\n        <br/><p>\n          Given this, we know how far away <InlineMath math=\"\\theta_t\"/> can be from <InlineMath math=\"\\theta^D\"/> and how far <InlineMath math=\"\\theta^D\"/> can be from <InlineMath math=\"\\theta^{D \\setminus x}\"/>. Therefore, we know how far <InlineMath math=\"\\theta_t\"/> could possibly be from <InlineMath math=\"\\theta^{D \\setminus x}\"/>. Given this information, we can again apply the convergence guarantee and perform gradient descent for some required number of steps in the direction of <InlineMath math=\"\\theta^{D \\setminus x}\"/> until we know we{\"\\'\"}re some distance away, resulting in <InlineMath math=\"\\theta_T\"/>. This finishes Case A, where we{\"\\'\"}ve gone from an initial point <InlineMath math=\"\\theta_0\"/> and gotten close to <InlineMath math=\"\\theta^{D \\setminus x}\"/> without knowing <InlineMath math=\"x\"/> beforehand.\n        </p>\n\n        <center style={{ 'background-color': 'white', 'padding': '20px', 'border-radius': '20px' }}>\n          <img src=\"../machine-unlearning/image4.svg\" width=\"50%\"/>\n        </center>\n\n        <br/><p>\n          Now we consider alternative reality, Case B. That is, we start from our initial point and train regularly but <InlineMath math=\"x\"/> was never in the dataset. Given our convergence guarantee, again we can know after some number of steps that we are at least within some distance of the optimum.\n        </p>\n\n        <center style={{ 'background-color': 'white', 'padding': '20px', 'border-radius': '20px' }}>\n          <img src=\"../machine-unlearning/image5.svg\" width=\"50%\"/>\n        </center>\n\n        <br/><p>\n          Now given that we can guarantee a certain distance from <InlineMath math=\"\\theta^{D \\setminus x}\"/> in either case, we can guarantee that <InlineMath math=\"\\theta_T\"/> and <InlineMath math=\"\\theta\"/> are within some distance of one another.\n        </p>\n\n        <center style={{ 'background-color': 'white', 'padding': '20px', 'border-radius': '20px' }}>\n          <img src=\"../machine-unlearning/image6.svg\" width=\"50%\"/>\n        </center>\n\n        <br/><p>\n          Finally, in either case, we publish the models according to the same publishing scheme, namely an injection of Gaussian noise to the model parameters scaled to how far <InlineMath math=\"\\theta_T\"/> and <InlineMath math=\"\\theta\"/> could possibly be from one another. All of this entails that both outcomes are statistically indistinguishable from one another.\n        </p>\n\n        <h3>\n          Conlusion\n        </h3>\n        <p>\n          Hopefully this was a useful introduction into the problem of machine unlearning and how it can be addressed. For further reading, make sure to check out the amazing blog post about an alternative approach, <a href=\"http://www.cleverhans.io/2020/07/20/unlearning.html\">SISA</a>, proposed by Papernot et al.\n        </p>\n      </>\n    );\n}\n\nexport default Post;\n"]},"metadata":{},"sourceType":"module"}