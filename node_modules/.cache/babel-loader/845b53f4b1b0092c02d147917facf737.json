{"ast":null,"code":"var _jsxFileName = \"/Users/chriswaites/Documents/projects/ChrisWaites.github.io/src/posts/machine-unlearning.js\";\nimport React from 'react';\nimport 'katex/dist/katex.min.css';\nimport { InlineMath, BlockMath } from 'react-katex';\n\nfunction Post() {\n  return /*#__PURE__*/React.createElement(React.Fragment, null, /*#__PURE__*/React.createElement(\"h2\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 9,\n      columnNumber: 9\n    }\n  }, \"Deleting Data from Machine Learning Models\"), /*#__PURE__*/React.createElement(\"p\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 12,\n      columnNumber: 9\n    }\n  }, \"Let \", /*#__PURE__*/React.createElement(InlineMath, {\n    math: \"\\\\ell(\\\\theta)\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 13,\n      columnNumber: 15\n    }\n  }), \" be \", /*#__PURE__*/React.createElement(InlineMath, {\n    math: \"m\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 13,\n      columnNumber: 53\n    }\n  }), \"-strongly convex and M-smooth, and let \", /*#__PURE__*/React.createElement(InlineMath, {\n    math: \"\\\\theta^* = argmin_{\\\\theta \\\\in \\\\Theta} \\\\ell(\\\\theta)\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 13,\n      columnNumber: 115\n    }\n  }), \". We have that after \", /*#__PURE__*/React.createElement(InlineMath, {\n    math: \"T\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 13,\n      columnNumber: 208\n    }\n  }), \" steps of gradient descent with step size \", /*#__PURE__*/React.createElement(InlineMath, {\n    math: \"\\\\eta = \\\\frac{2}{m + M}\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 13,\n      columnNumber: 273\n    }\n  }), \",\"), /*#__PURE__*/React.createElement(BlockMath, {\n    math: \"||\\\\theta_T - \\\\theta^*||_2 \\\\leq \\\\left( \\\\frac{M - m}{M + m}^T \\\\right) || \\\\theta_0 - \\\\theta^*||_2\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 15,\n      columnNumber: 9\n    }\n  }), /*#__PURE__*/React.createElement(\"h3\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 17,\n      columnNumber: 9\n    }\n  }, \"Conlusion\"), /*#__PURE__*/React.createElement(\"p\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 20,\n      columnNumber: 9\n    }\n  }, \"Hopefully this was a useful tutorial outlining differential privacy and its applications to deep learning in plain English. If you have any questions or have caught any errors, feel free to reach out!\"));\n}\n\nexport default Post;","map":{"version":3,"sources":["/Users/chriswaites/Documents/projects/ChrisWaites.github.io/src/posts/machine-unlearning.js"],"names":["React","InlineMath","BlockMath","Post"],"mappings":";AAAA,OAAOA,KAAP,MAAkB,OAAlB;AACA,OAAO,0BAAP;AACA,SAASC,UAAT,EAAqBC,SAArB,QAAsC,aAAtC;;AAGA,SAASC,IAAT,GAAgB;AACZ,sBACE,uDACE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,kDADF,eAIE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,0BACM,oBAAC,UAAD;AAAY,IAAA,IAAI,EAAC,gBAAjB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IADN,uBAC4C,oBAAC,UAAD;AAAY,IAAA,IAAI,EAAC,GAAjB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IAD5C,0DAC0G,oBAAC,UAAD;AAAY,IAAA,IAAI,EAAC,0DAAjB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IAD1G,wCACuM,oBAAC,UAAD;AAAY,IAAA,IAAI,EAAC,GAAjB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IADvM,6DACwQ,oBAAC,UAAD;AAAY,IAAA,IAAI,EAAC,0BAAjB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IADxQ,MAJF,eAOE,oBAAC,SAAD;AAAW,IAAA,IAAI,EAAC,wGAAhB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IAPF,eASE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,iBATF,eAYE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,gNAZF,CADF;AAkBH;;AAED,eAAeA,IAAf","sourcesContent":["import React from 'react';\nimport 'katex/dist/katex.min.css';\nimport { InlineMath, BlockMath } from 'react-katex';\n\n\nfunction Post() {\n    return (\n      <>\n        <h2>\n          Deleting Data from Machine Learning Models\n        </h2>\n        <p>\n          Let <InlineMath math=\"\\ell(\\theta)\" /> be <InlineMath math=\"m\" />-strongly convex and M-smooth, and let <InlineMath math=\"\\theta^* = argmin_{\\theta \\in \\Theta} \\ell(\\theta)\" />. We have that after <InlineMath math=\"T\" /> steps of gradient descent with step size <InlineMath math=\"\\eta = \\frac{2}{m + M}\" />,\n        </p>\n        <BlockMath math=\"||\\theta_T - \\theta^*||_2 \\leq \\left( \\frac{M - m}{M + m}^T \\right) || \\theta_0 - \\theta^*||_2\" />\n\n        <h3>\n          Conlusion\n        </h3>\n        <p>\n          Hopefully this was a useful tutorial outlining differential privacy and its applications to deep learning in plain English. If you have any questions or have caught any errors, feel free to reach out!\n        </p>\n      </>\n    );\n}\n\nexport default Post;\n"]},"metadata":{},"sourceType":"module"}