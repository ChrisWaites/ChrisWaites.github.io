{"ast":null,"code":"var _jsxFileName = \"/Users/chriswaites/Documents/projects/ChrisWaites.github.io/src/posts/differentially-private-deep-learning.js\";\nimport React from 'react';\nimport 'katex/dist/katex.min.css';\nimport { InlineMath, BlockMath } from 'react-katex';\n\nfunction Post() {\n  return /*#__PURE__*/React.createElement(React.Fragment, null, /*#__PURE__*/React.createElement(\"h2\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 9,\n      columnNumber: 9\n    }\n  }, \"A Guide to Differentially Private Deep Learning\"), /*#__PURE__*/React.createElement(\"p\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 12,\n      columnNumber: 9\n    }\n  }, \"In this post we tackle the topic of privacy-preserving deep learning. This commentary will be less so concerned with precision and proof, and moreso geared towards convincing a deep learning practitioner what privacy should mean, why its important, and how it can be achieved.\"), /*#__PURE__*/React.createElement(\"h3\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 16,\n      columnNumber: 9\n    }\n  }, \"Differential Privacy\"), /*#__PURE__*/React.createElement(\"p\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 19,\n      columnNumber: 9\n    }\n  }, \"Before talking about anything else, we need to define what privacy is. There\", \"\\'\", \"s a near infinite continuum of what privacy \", /*#__PURE__*/React.createElement(\"em\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 20,\n      columnNumber: 137\n    }\n  }, \"could\"), \" mean, and naturally some definitions are less vacuous than others.\"), /*#__PURE__*/React.createElement(\"p\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 22,\n      columnNumber: 9\n    }\n  }, \"One such definition is \", /*#__PURE__*/React.createElement(\"em\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 23,\n      columnNumber: 34\n    }\n  }, \"differential privacy\"), \". Differential privacy is concerned with \", /*#__PURE__*/React.createElement(\"em\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 23,\n      columnNumber: 104\n    }\n  }, \"algorithms\"), \", namely functions responsible for mapping a given dataset to some output space, e.g., linear regression mapping a dataset to its coefficients \", /*#__PURE__*/React.createElement(InlineMath, {\n    math: \"w\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 23,\n      columnNumber: 266\n    }\n  }), \" and \", /*#__PURE__*/React.createElement(InlineMath, {\n    math: \"b\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 23,\n      columnNumber: 294\n    }\n  }), \".\"), /*#__PURE__*/React.createElement(\"p\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 25,\n      columnNumber: 9\n    }\n  }, \"On a high level, such an algorithm would \\\"preserve privacy\\\" under the notion of differential privacy if it were to behave (approximately) the same regardless of whether you removed any individual point from the dataset. If you could achieve this property, then you \", /*#__PURE__*/React.createElement(\"em\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 26,\n      columnNumber: 276\n    }\n  }, \"should\"), \" be convinced that this algorithm is privacy-preserving.\"), /*#__PURE__*/React.createElement(\"p\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 28,\n      columnNumber: 9\n    }\n  }, \"Why? Well, let\", \"\\'\", \"s consider how you would feel if one of these points actually corresponded to you. By the definition put forth, you wouldn\", \"\\'\", \"t have grounds to care whether your data is given to the algorithm - the outcome will be the same regardless. In other words, \", /*#__PURE__*/React.createElement(\"em\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 29,\n      columnNumber: 285\n    }\n  }, \"you should feel like this algorithm preserves your privacy because its outputs look the same whether or not your data is given to it\"), \". So at least intuitively, you should feel like some adversary shouldnt be able to reverse engineer the \", /*#__PURE__*/React.createElement(InlineMath, {\n    math: \"w\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 29,\n      columnNumber: 530\n    }\n  }), \" and \", /*#__PURE__*/React.createElement(InlineMath, {\n    math: \"b\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 29,\n      columnNumber: 558\n    }\n  }), \" it observese and somehow be able to reconstruct your individual data.\"), /*#__PURE__*/React.createElement(\"p\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 31,\n      columnNumber: 9\n    }\n  }, \"Formally, we can express this notion via the following\", /*#__PURE__*/React.createElement(\"sup\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 32,\n      columnNumber: 65\n    }\n  }, /*#__PURE__*/React.createElement(\"a\", {\n    href: \"https://stephentu.github.io/writeups/6885-lec20-b.pdf\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 32,\n      columnNumber: 70\n    }\n  }, \"1\")), \":\"), /*#__PURE__*/React.createElement(\"p\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 34,\n      columnNumber: 9\n    }\n  }, /*#__PURE__*/React.createElement(\"em\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 34,\n      columnNumber: 12\n    }\n  }, \"Let \", /*#__PURE__*/React.createElement(InlineMath, {\n    math: \"A : \\\\mathcal{D} \\\\rightarrow \\\\mathcal{Y}\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 35,\n      columnNumber: 15\n    }\n  }), \" be a randomized algorithm. Let \", /*#__PURE__*/React.createElement(InlineMath, {\n    math: \"A\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 35,\n      columnNumber: 108\n    }\n  }), \" be \", /*#__PURE__*/React.createElement(InlineMath, {\n    math: \"\\\\varepsilon\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 35,\n      columnNumber: 135\n    }\n  }), \"-differentially private if for all \", /*#__PURE__*/React.createElement(InlineMath, {\n    math: \"D_1, D_2 \\\\in \\\\mathcal{D}\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 35,\n      columnNumber: 203\n    }\n  }), \" which differ in a single entry, and for all outputs \", /*#__PURE__*/React.createElement(InlineMath, {\n    math: \"y \\\\in \\\\mathcal{Y}\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 35,\n      columnNumber: 302\n    }\n  }), \", we have that:\", /*#__PURE__*/React.createElement(BlockMath, {\n    math: \"e^{-\\\\varepsilon} \\\\leq \\\\frac{\\\\Pr[\\\\mathcal{A(D_1) = y}]}{\\\\Pr[\\\\mathcal{A(D_2) = y}]} \\\\leq e^\\\\varepsilon\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 36,\n      columnNumber: 9\n    }\n  }))), /*#__PURE__*/React.createElement(\"p\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 38,\n      columnNumber: 9\n    }\n  }, \"Upon inspection, you\", \"\\'\", \"ll notice that it\", \"\\'\", \"s saying exactly what we established earlier. Namely, the probability of a particular outcome it about the same whether or not you include any particular individual in the dataset.\"), /*#__PURE__*/React.createElement(\"h3\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 42,\n      columnNumber: 9\n    }\n  }, \"Deep Learning\"), /*#__PURE__*/React.createElement(\"p\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 45,\n      columnNumber: 9\n    }\n  }, \"Deep learning is currently one of the most predominant forms of statistical analysis used today and has been shown to be remarkably effective for a variety of tasks. Deep neural networks, in their standard form, define a function composed of a sequence of layers where each layer represents an operation to be performed on the output of the previous layer. Typically the goal associated with such models is to find the set of parameters which map a set of inputs to a set of outputs in a way which minimizes some function, referred to as the loss function.\"), /*#__PURE__*/React.createElement(BlockMath, {\n    math: \"\\\\{ x^{(1)}, x^{(1)}, \\\\ldots x^{(b)} \\\\} \\\\sim sample(X, b)\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 48,\n      columnNumber: 9\n    }\n  }), /*#__PURE__*/React.createElement(BlockMath, {\n    math: \"\\\\ell(\\\\theta) = \\\\frac{1}{b} \\\\sum_i \\\\ell(x^{(i)} ; \\\\theta)\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 49,\n      columnNumber: 9\n    }\n  }), /*#__PURE__*/React.createElement(BlockMath, {\n    math: \"\\\\theta \\\\leftarrow \\\\theta - \\\\eta \\\\nabla_\\\\theta \\\\ell(\\\\theta)\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 50,\n      columnNumber: 9\n    }\n  }), /*#__PURE__*/React.createElement(\"p\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 51,\n      columnNumber: 9\n    }\n  }, \"A popular method for finding such parameters is via a process of stochastic gradient descent. When conducting stochastic gradient descent, one iteratively updates the parameters of the model by sampling an individual input-output pair from the dataset and partially applying their values to the error function so that the gradient of the error with respect to the parameters of the model can be computed. Then, one would update the parameters of the model in the direction opposite of the gradient, in turn minimizing the error function with respect to that example. Formally, if we let \\u03B80 be the randomly initialized parameters of the model, \\u03B8t be the parameters of the model at iteration t, (xt, yt) be our sampled input-output pair, L be our error function, and \\u03B7t be the learning rate, we iteratively apply the following update rule:\"), /*#__PURE__*/React.createElement(\"p\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 54,\n      columnNumber: 9\n    }\n  }, \"Although, it\\u2019s more common in practice to opt for minibatch gradient descent. Rather than calculating gradients with respect to individual examples, one uniformly samples a subset of B examples without replacement, calculates the gradient with respect to each example, and applies the average of the gradients to the model. This corresponds to the following update rule:\"), /*#__PURE__*/React.createElement(\"h3\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 58,\n      columnNumber: 9\n    }\n  }, \"Differentially Private Stochastic Gradient Descent\"), /*#__PURE__*/React.createElement(\"p\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 61,\n      columnNumber: 9\n    }\n  }, /*#__PURE__*/React.createElement(\"a\", {\n    href: \"https://arxiv.org/abs/1607.00133\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 62,\n      columnNumber: 11\n    }\n  }, \"Abadi et al.\"), \" detail the differentially private stochastic gradient descent (DP-SGD) algorithm to make traditional SGD yield a differential privacy guarantee. To describe it, we need to introduce a number of augmentations.\"), /*#__PURE__*/React.createElement(\"p\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 64,\n      columnNumber: 9\n    }\n  }, \"First, we have to augment our typical method for sampling examples from the dataset. In the context of non-private deep learning, sampling is often achieved by shuffling the dataset and running through partitions of size \", /*#__PURE__*/React.createElement(InlineMath, {\n    math: \"b\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 65,\n      columnNumber: 232\n    }\n  }), \" such that each example is viewed by the model exactly once per epoch. In the context of DP-SGD, we have to opt for either Poisson\", /*#__PURE__*/React.createElement(\"sup\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 65,\n      columnNumber: 385\n    }\n  }, \"2\"), \" or uniform\", /*#__PURE__*/React.createElement(\"sup\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 65,\n      columnNumber: 408\n    }\n  }, \"3\"), \" subsampling if we want a differential privacy guarantee.\"), /*#__PURE__*/React.createElement(\"p\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 67,\n      columnNumber: 9\n    }\n  }, \"Second, we need to introduce a number of additional training parameters. In particular, we need to introduce a clipping parameter \", /*#__PURE__*/React.createElement(InlineMath, {\n    math: \"C\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 68,\n      columnNumber: 141\n    }\n  }), \", an upper bound on the \", /*#__PURE__*/React.createElement(InlineMath, {\n    math: \"\\\\ell_2\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 68,\n      columnNumber: 188\n    }\n  }), \"-norm of each per-example gradient, as well as the noise multiplier \", /*#__PURE__*/React.createElement(InlineMath, {\n    math: \"\\\\sigma\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 68,\n      columnNumber: 284\n    }\n  }), \", which acts as the ratio between the clipping parameter and variance of the Gaussian noise applied to each gradient update after clipping.\"), /*#__PURE__*/React.createElement(BlockMath, {\n    math: \"\\\\{ x^{(1)}, x^{(1)}, \\\\ldots x^{(b)} \\\\} \\\\sim sample(X, b)\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 70,\n      columnNumber: 9\n    }\n  }), /*#__PURE__*/React.createElement(BlockMath, {\n    math: \"g^{(i)} \\\\leftarrow \\\\nabla_{\\\\theta} \\\\ell(x^{(i)} ; \\\\theta)\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 71,\n      columnNumber: 9\n    }\n  }), /*#__PURE__*/React.createElement(BlockMath, {\n    math: \"\\\\bar{g}^{(i)} \\\\leftarrow g^{(i)} / \\\\max\\\\{ 1, ||g^{(i)}||_2 / C\\\\}\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 72,\n      columnNumber: 9\n    }\n  }), /*#__PURE__*/React.createElement(BlockMath, {\n    math: \"\\\\tilde{g} \\\\leftarrow \\\\frac{1}{b} (\\\\sum_i \\\\bar{g}^{(i)} + \\\\sigma C \\\\mathcal{N}(0, I))\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 73,\n      columnNumber: 9\n    }\n  }), /*#__PURE__*/React.createElement(BlockMath, {\n    math: \"\\\\theta \\\\leftarrow \\\\theta - \\\\eta \\\\tilde{g}\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 74,\n      columnNumber: 9\n    }\n  }), /*#__PURE__*/React.createElement(\"p\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 75,\n      columnNumber: 9\n    }\n  }, \"In order to calculate the privacy loss corresponding to k executions of the above update rule, Abadi et al. detail the moments accountant as a method to report privacy loss over time. A full deep dive into the foundations backing the moments accountant are likely outside the scope of this post, but on a high level it can be thought of as a black box which takes in values which characterize your training loop (sampling probabilities, number of minibatches, delta, etc.) and outputs epsilon. But importantly, their method yields much tighter bounds on the privacy loss achieved than what is reported via the strong composition theorem. If interested in learning more, the algorithm was originally introduced in Abadi et al. and has a corresponding implementation within Tensorflow Privacy.\"), /*#__PURE__*/React.createElement(\"h3\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 79,\n      columnNumber: 9\n    }\n  }, \"Private Aggregation of Teacher Ensembles\"), /*#__PURE__*/React.createElement(\"p\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 82,\n      columnNumber: 9\n    }\n  }, \"Private Aggregation of Teacher Ensembles or PATE is an alternative method to DPSGD for conducting differentially private learning. The key idea is to, rather than train a single strong model which captures a complex criterion in a differentially private manner, train a set of weaker, non-private models on partitions of the data and then perform a noisy aggregation of their predictions. Overall the method has been shown to be quite effective at the expense of some assumptions about the training procedure. There has even been an application of this technique to GANs via PATE-GAN.\"), /*#__PURE__*/React.createElement(\"h3\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 86,\n      columnNumber: 9\n    }\n  }, \"Differentially Private Federated Learning\"), /*#__PURE__*/React.createElement(\"h3\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 90,\n      columnNumber: 9\n    }\n  }, \"Proper Development\"), /*#__PURE__*/React.createElement(\"p\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 93,\n      columnNumber: 9\n    }\n  }, \"There are a number of common gotchas involved with the proper development and deployment of differentially private deep learning models.\"), /*#__PURE__*/React.createElement(\"p\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 96,\n      columnNumber: 9\n    }\n  }, \"The biggest is hyperparameter search and model selection. You might think intuitively that, as long as you implement and execute something like DP-SGD, you can sleep soundly at night knowing your model is privacy preserving. Although, typically many models are trained in the process of during the hyperparameter . Although in theory, the hyperparameters you\", \"\\'\", \"re selecting are technically data-inspired, and hence indirectly leak information. Frankly this detail is swept under the rug in research contexts. But in industry, this absolutely needs to be taken into consideration, or at the very least consciously acknowledged as a risk.\"), /*#__PURE__*/React.createElement(\"p\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 99,\n      columnNumber: 9\n    }\n  }, \"The second is proper evaluation. It does not suffice to simply train a single model and report its accuracy. Recall that\"));\n}\n\nexport default Post;","map":{"version":3,"sources":["/Users/chriswaites/Documents/projects/ChrisWaites.github.io/src/posts/differentially-private-deep-learning.js"],"names":["React","InlineMath","BlockMath","Post"],"mappings":";AAAA,OAAOA,KAAP,MAAkB,OAAlB;AACA,OAAO,0BAAP;AACA,SAASC,UAAT,EAAqBC,SAArB,QAAsC,aAAtC;;AAGA,SAASC,IAAT,GAAgB;AACZ,sBACE,uDACE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,uDADF,eAIE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,4RAJF,eAQE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,4BARF,eAWE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,qFAC+E,IAD/E,+DACgI;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,aADhI,wEAXF,eAcE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,6CACyB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,4BADzB,4DAC+F;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,kBAD/F,kKACiQ,oBAAC,UAAD;AAAY,IAAA,IAAI,EAAC,GAAjB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IADjQ,wBAC6R,oBAAC,UAAD;AAAY,IAAA,IAAI,EAAC,GAAjB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IAD7R,MAdF,eAiBE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,iSAC2Q;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,cAD3Q,6DAjBF,eAoBE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,uBACiB,IADjB,gIACiJ,IADjJ,iJACoR;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,4IADpR,2HACygB,oBAAC,UAAD;AAAY,IAAA,IAAI,EAAC,GAAjB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IADzgB,wBACqiB,oBAAC,UAAD;AAAY,IAAA,IAAI,EAAC,GAAjB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IADriB,2EApBF,eAuBE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,4EACwD;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,kBAAK;AAAG,IAAA,IAAI,EAAC,uDAAR;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,SAAL,CADxD,MAvBF,eA0BE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,kBAAG;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,0BACG,oBAAC,UAAD;AAAY,IAAA,IAAI,EAAC,4CAAjB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IADH,mDACgG,oBAAC,UAAD;AAAY,IAAA,IAAI,EAAC,GAAjB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IADhG,uBAC2H,oBAAC,UAAD;AAAY,IAAA,IAAI,EAAC,cAAjB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IAD3H,sDAC+L,oBAAC,UAAD;AAAY,IAAA,IAAI,EAAC,4BAAjB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IAD/L,wEACkS,oBAAC,UAAD;AAAY,IAAA,IAAI,EAAC,qBAAjB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IADlS,kCAEH,oBAAC,SAAD;AAAW,IAAA,IAAI,EAAC,+GAAhB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IAFG,CAAH,CA1BF,eA8BE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,6BACuB,IADvB,uBAC8C,IAD9C,yLA9BF,eAkCE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,qBAlCF,eAqCE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,ojBArCF,eAwCE,oBAAC,SAAD;AAAW,IAAA,IAAI,EAAC,8DAAhB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IAxCF,eAyCE,oBAAC,SAAD;AAAW,IAAA,IAAI,EAAC,gEAAhB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IAzCF,eA0CE,oBAAC,SAAD;AAAW,IAAA,IAAI,EAAC,oEAAhB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IA1CF,eA2CE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,41BA3CF,eA8CE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,+XA9CF,eAkDE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,0DAlDF,eAqDE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,kBACE;AAAG,IAAA,IAAI,EAAC,kCAAR;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,oBADF,sNArDF,eAwDE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,mPAC+N,oBAAC,UAAD;AAAY,IAAA,IAAI,EAAC,GAAjB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IAD/N,qJACwX;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,SADxX,8BAC+Y;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,SAD/Y,8DAxDF,eA2DE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,wJACoI,oBAAC,UAAD;AAAY,IAAA,IAAI,EAAC,GAAjB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IADpI,2CACmL,oBAAC,UAAD;AAAY,IAAA,IAAI,EAAC,SAAjB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IADnL,uFACmR,oBAAC,UAAD;AAAY,IAAA,IAAI,EAAC,SAAjB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IADnR,gJA3DF,eA8DE,oBAAC,SAAD;AAAW,IAAA,IAAI,EAAC,8DAAhB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IA9DF,eA+DE,oBAAC,SAAD;AAAW,IAAA,IAAI,EAAC,gEAAhB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IA/DF,eAgEE,oBAAC,SAAD;AAAW,IAAA,IAAI,EAAC,uEAAhB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IAhEF,eAiEE,oBAAC,SAAD;AAAW,IAAA,IAAI,EAAC,6FAAhB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IAjEF,eAkEE,oBAAC,SAAD;AAAW,IAAA,IAAI,EAAC,gDAAhB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IAlEF,eAmEE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,+xBAnEF,eAuEE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,gDAvEF,eA0EE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,glBA1EF,eA8EE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,iDA9EF,eAkFE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,0BAlFF,eAqFE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,gJArFF,eAwFE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,+WACyW,IADzW,wRAxFF,eA2FE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,gIA3FF,CADF;AAiGH;;AAED,eAAeA,IAAf","sourcesContent":["import React from 'react';\nimport 'katex/dist/katex.min.css';\nimport { InlineMath, BlockMath } from 'react-katex';\n\n\nfunction Post() {\n    return (\n      <>\n        <h2>\n          A Guide to Differentially Private Deep Learning\n        </h2>\n        <p>\n          In this post we tackle the topic of privacy-preserving deep learning. This commentary will be less so concerned with precision and proof, and moreso geared towards convincing a deep learning practitioner what privacy should mean, why its important, and how it can be achieved.\n        </p>\n\n        <h3>\n          Differential Privacy\n        </h3>\n        <p>\n          Before talking about anything else, we need to define what privacy is. There{\"\\'\"}s a near infinite continuum of what privacy <em>could</em> mean, and naturally some definitions are less vacuous than others.\n        </p>\n        <p>\n          One such definition is <em>differential privacy</em>. Differential privacy is concerned with <em>algorithms</em>, namely functions responsible for mapping a given dataset to some output space, e.g., linear regression mapping a dataset to its coefficients <InlineMath math=\"w\" /> and <InlineMath math=\"b\" />.\n        </p>\n        <p>\n          On a high level, such an algorithm would \"preserve privacy\" under the notion of differential privacy if it were to behave (approximately) the same regardless of whether you removed any individual point from the dataset. If you could achieve this property, then you <em>should</em> be convinced that this algorithm is privacy-preserving.\n        </p>\n        <p>\n          Why? Well, let{\"\\'\"}s consider how you would feel if one of these points actually corresponded to you. By the definition put forth, you wouldn{\"\\'\"}t have grounds to care whether your data is given to the algorithm - the outcome will be the same regardless. In other words, <em>you should feel like this algorithm preserves your privacy because its outputs look the same whether or not your data is given to it</em>. So at least intuitively, you should feel like some adversary shouldnt be able to reverse engineer the <InlineMath math=\"w\" /> and <InlineMath math=\"b\" /> it observese and somehow be able to reconstruct your individual data.\n        </p>\n        <p>\n          Formally, we can express this notion via the following<sup><a href=\"https://stephentu.github.io/writeups/6885-lec20-b.pdf\">1</a></sup>:\n        </p>\n        <p><em>\n          Let <InlineMath math=\"A : \\mathcal{D} \\rightarrow \\mathcal{Y}\" /> be a randomized algorithm. Let <InlineMath math=\"A\" /> be <InlineMath math=\"\\varepsilon\" />-differentially private if for all <InlineMath math=\"D_1, D_2 \\in \\mathcal{D}\" /> which differ in a single entry, and for all outputs <InlineMath math=\"y \\in \\mathcal{Y}\" />, we have that:\n        <BlockMath math=\"e^{-\\varepsilon} \\leq \\frac{\\Pr[\\mathcal{A(D_1) = y}]}{\\Pr[\\mathcal{A(D_2) = y}]} \\leq e^\\varepsilon\" />\n        </em></p>\n        <p>\n          Upon inspection, you{\"\\'\"}ll notice that it{\"\\'\"}s saying exactly what we established earlier. Namely, the probability of a particular outcome it about the same whether or not you include any particular individual in the dataset.\n        </p>\n\n        <h3>\n          Deep Learning\n        </h3>\n        <p>\nDeep learning is currently one of the most predominant forms of statistical analysis used today and has been shown to be remarkably effective for a variety of tasks. Deep neural networks, in their standard form, define a function composed of a sequence of layers where each layer represents an operation to be performed on the output of the previous layer. Typically the goal associated with such models is to find the set of parameters which map a set of inputs to a set of outputs in a way which minimizes some function, referred to as the loss function.\n        </p>\n        <BlockMath math=\"\\{ x^{(1)}, x^{(1)}, \\ldots x^{(b)} \\} \\sim sample(X, b)\" />\n        <BlockMath math=\"\\ell(\\theta) = \\frac{1}{b} \\sum_i \\ell(x^{(i)} ; \\theta)\" />\n        <BlockMath math=\"\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\ell(\\theta)\" />\n        <p>\n          A popular method for finding such parameters is via a process of stochastic gradient descent. When conducting stochastic gradient descent, one iteratively updates the parameters of the model by sampling an individual input-output pair from the dataset and partially applying their values to the error function so that the gradient of the error with respect to the parameters of the model can be computed. Then, one would update the parameters of the model in the direction opposite of the gradient, in turn minimizing the error function with respect to that example. Formally, if we let θ0 be the randomly initialized parameters of the model, θt be the parameters of the model at iteration t, (xt, yt) be our sampled input-output pair, L be our error function, and ηt be the learning rate, we iteratively apply the following update rule:\n        </p>\n        <p>\n          Although, it’s more common in practice to opt for minibatch gradient descent. Rather than calculating gradients with respect to individual examples, one uniformly samples a subset of B examples without replacement, calculates the gradient with respect to each example, and applies the average of the gradients to the model. This corresponds to the following update rule:\n        </p>\n\n        <h3>\n          Differentially Private Stochastic Gradient Descent\n        </h3>\n        <p>\n          <a href=\"https://arxiv.org/abs/1607.00133\">Abadi et al.</a> detail the differentially private stochastic gradient descent (DP-SGD) algorithm to make traditional SGD yield a differential privacy guarantee. To describe it, we need to introduce a number of augmentations.\n        </p>\n        <p>\n          First, we have to augment our typical method for sampling examples from the dataset. In the context of non-private deep learning, sampling is often achieved by shuffling the dataset and running through partitions of size <InlineMath math=\"b\" /> such that each example is viewed by the model exactly once per epoch. In the context of DP-SGD, we have to opt for either Poisson<sup>2</sup> or uniform<sup>3</sup> subsampling if we want a differential privacy guarantee.\n        </p>\n        <p>\n          Second, we need to introduce a number of additional training parameters. In particular, we need to introduce a clipping parameter <InlineMath math=\"C\" />, an upper bound on the <InlineMath math=\"\\ell_2\" />-norm of each per-example gradient, as well as the noise multiplier <InlineMath math=\"\\sigma\" />, which acts as the ratio between the clipping parameter and variance of the Gaussian noise applied to each gradient update after clipping.\n        </p>\n        <BlockMath math=\"\\{ x^{(1)}, x^{(1)}, \\ldots x^{(b)} \\} \\sim sample(X, b)\" />\n        <BlockMath math=\"g^{(i)} \\leftarrow \\nabla_{\\theta} \\ell(x^{(i)} ; \\theta)\" />\n        <BlockMath math=\"\\bar{g}^{(i)} \\leftarrow g^{(i)} / \\max\\{ 1, ||g^{(i)}||_2 / C\\}\" />\n        <BlockMath math=\"\\tilde{g} \\leftarrow \\frac{1}{b} (\\sum_i \\bar{g}^{(i)} + \\sigma C \\mathcal{N}(0, I))\" />\n        <BlockMath math=\"\\theta \\leftarrow \\theta - \\eta \\tilde{g}\" />\n        <p>\n          In order to calculate the privacy loss corresponding to k executions of the above update rule, Abadi et al. detail the moments accountant as a method to report privacy loss over time. A full deep dive into the foundations backing the moments accountant are likely outside the scope of this post, but on a high level it can be thought of as a black box which takes in values which characterize your training loop (sampling probabilities, number of minibatches, delta, etc.) and outputs epsilon. But importantly, their method yields much tighter bounds on the privacy loss achieved than what is reported via the strong composition theorem. If interested in learning more, the algorithm was originally introduced in Abadi et al. and has a corresponding implementation within Tensorflow Privacy.\n        </p>\n\n        <h3>\n          Private Aggregation of Teacher Ensembles\n        </h3>\n        <p>\n          Private Aggregation of Teacher Ensembles or PATE is an alternative method to DPSGD for conducting differentially private learning. The key idea is to, rather than train a single strong model which captures a complex criterion in a differentially private manner, train a set of weaker, non-private models on partitions of the data and then perform a noisy aggregation of their predictions. Overall the method has been shown to be quite effective at the expense of some assumptions about the training procedure. There has even been an application of this technique to GANs via PATE-GAN.\n        </p>\n\n        <h3>\n          Differentially Private Federated Learning\n        </h3>\n\n        <h3>\n          Proper Development\n        </h3>\n        <p>\n          There are a number of common gotchas involved with the proper development and deployment of differentially private deep learning models.\n        </p>\n        <p>\n          The biggest is hyperparameter search and model selection. You might think intuitively that, as long as you implement and execute something like DP-SGD, you can sleep soundly at night knowing your model is privacy preserving. Although, typically many models are trained in the process of during the hyperparameter . Although in theory, the hyperparameters you{\"\\'\"}re selecting are technically data-inspired, and hence indirectly leak information. Frankly this detail is swept under the rug in research contexts. But in industry, this absolutely needs to be taken into consideration, or at the very least consciously acknowledged as a risk.\n        </p>\n        <p>\n          The second is proper evaluation. It does not suffice to simply train a single model and report its accuracy. Recall that\n        </p>\n      </>\n    );\n}\n\nexport default Post;\n"]},"metadata":{},"sourceType":"module"}