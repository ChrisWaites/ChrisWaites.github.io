{"ast":null,"code":"var _jsxFileName = \"/Users/chriswaites/Documents/projects/ChrisWaites.github.io/src/posts/machine-unlearning.js\";\nimport React from 'react';\nimport 'katex/dist/katex.min.css';\nimport { InlineMath, BlockMath } from 'react-katex';\n\nfunction Post() {\n  return /*#__PURE__*/React.createElement(React.Fragment, null, /*#__PURE__*/React.createElement(\"h2\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 9,\n      columnNumber: 9\n    }\n  }, \"Machine Unlearning\"), /*#__PURE__*/React.createElement(\"p\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 12,\n      columnNumber: 9\n    }\n  }, \"Here\", \"\\'\", \"s an interesting problem: Let\", \"\\'\", \"s imagine your data was used to train some machine learning model. Now, you want to request that your data be \", /*#__PURE__*/React.createElement(\"em\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 13,\n      columnNumber: 166\n    }\n  }, \"unlearned\"), \" from the model. That is, the model is updated so that its parameters and outputs have no knowledge of you anymore.\"), /*#__PURE__*/React.createElement(\"p\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 15,\n      columnNumber: 9\n    }\n  }, \"For \", /*#__PURE__*/React.createElement(InlineMath, {\n    math: \"k\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 16,\n      columnNumber: 15\n    }\n  }), \"-nearest neighbors, the problem is trivial - simply remove the point from the data. For something more complex like a neural network, it\", \"\\'\", \"s not immediately clear what you would do. Is their an interesting middle-ground?\"), /*#__PURE__*/React.createElement(\"p\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 18,\n      columnNumber: 9\n    }\n  }, \"First, let\", \"\\'\", \"s define the problem.\"), /*#__PURE__*/React.createElement(\"ul\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 21,\n      columnNumber: 9\n    }\n  }, /*#__PURE__*/React.createElement(\"li\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 22,\n      columnNumber: 11\n    }\n  }, /*#__PURE__*/React.createElement(\"em\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 23,\n      columnNumber: 11\n    }\n  }, \"Case A (Real): We train the model on the full dataset containing \", /*#__PURE__*/React.createElement(InlineMath, {\n    math: \"x\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 24,\n      columnNumber: 78\n    }\n  }), \", we remove \", /*#__PURE__*/React.createElement(InlineMath, {\n    math: \"x\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 24,\n      columnNumber: 112\n    }\n  }), \" from the model parameters, and then we publish the model to the public.\")), /*#__PURE__*/React.createElement(\"li\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 27,\n      columnNumber: 11\n    }\n  }, /*#__PURE__*/React.createElement(\"em\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 28,\n      columnNumber: 11\n    }\n  }, \"Case B (Imaginary): The point \", /*#__PURE__*/React.createElement(InlineMath, {\n    math: \"x\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 29,\n      columnNumber: 43\n    }\n  }), \" was never in the dataset, we train the model on the dataset (which doesn\", \"\\'\", \"t contain \", /*#__PURE__*/React.createElement(InlineMath, {\n    math: \"x\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 29,\n      columnNumber: 154\n    }\n  }), \"), and then publish the model to the public.\"))), /*#__PURE__*/React.createElement(\"p\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 33,\n      columnNumber: 9\n    }\n  }, \"If we can find a procedure for Case A which yields an indistinguishable outcome to Case B, then we will have achieved our goal.\"), /*#__PURE__*/React.createElement(\"p\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 36,\n      columnNumber: 9\n    }\n  }, \"For the purposes of this post, we are going to tackle the case of a convex parametric model. For example, this would include logistic regression with binary cross entropy loss and \", /*#__PURE__*/React.createElement(InlineMath, {\n    math: \"\\\\ell_2\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 37,\n      columnNumber: 191\n    }\n  }), \" regularization.\"), /*#__PURE__*/React.createElement(\"p\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 39,\n      columnNumber: 9\n    }\n  }, \"To be concrete, let\", \"\\'\", \"s say our loss function is:\"), /*#__PURE__*/React.createElement(BlockMath, {\n    math: \"\\\\ell(\\\\theta) = - \\\\frac{1}{n} \\\\left( \\\\sum_{i \\\\in [n]} y_i \\\\log(\\\\hat{y}_i) + (1 - y_i) \\\\log(1 - \\\\hat{y}_i) \\\\right) + \\\\lambda ||\\\\theta||_2\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 42,\n      columnNumber: 9\n    }\n  }), /*#__PURE__*/React.createElement(\"p\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 43,\n      columnNumber: 9\n    }\n  }, \"Then we would have that our loss function is \", /*#__PURE__*/React.createElement(InlineMath, {\n    math: \"m\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 44,\n      columnNumber: 56\n    }\n  }), \"-strongly convex and \", /*#__PURE__*/React.createElement(InlineMath, {\n    math: \"M\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 44,\n      columnNumber: 99\n    }\n  }), \"-smooth where \", /*#__PURE__*/React.createElement(InlineMath, {\n    math: \"m = \\\\lambda\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 44,\n      columnNumber: 135\n    }\n  }), \" and \", /*#__PURE__*/React.createElement(InlineMath, {\n    math: \"M = 4 - \\\\lambda\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 44,\n      columnNumber: 172\n    }\n  }), \".\"), /*#__PURE__*/React.createElement(\"p\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 46,\n      columnNumber: 9\n    }\n  }, \"Now, the great part about working in this setting is that we have convergence guarantees. That is, as long as we know how far we are at most from the global optimum, if we run gradient descent, we know we will be at most some distance after some number of steps we take.\"), /*#__PURE__*/React.createElement(\"em\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 49,\n      columnNumber: 9\n    }\n  }, \"Let \", /*#__PURE__*/React.createElement(InlineMath, {\n    math: \"\\\\ell(\\\\theta)\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 50,\n      columnNumber: 15\n    }\n  }), \" be \", /*#__PURE__*/React.createElement(InlineMath, {\n    math: \"m\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 50,\n      columnNumber: 53\n    }\n  }), \"-strongly convex and M-smooth, and let \", /*#__PURE__*/React.createElement(InlineMath, {\n    math: \"\\\\theta^* = argmin_{\\\\theta \\\\in \\\\Theta} \\\\ell(\\\\theta)\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 50,\n      columnNumber: 115\n    }\n  }), \". We have that after \", /*#__PURE__*/React.createElement(InlineMath, {\n    math: \"T\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 50,\n      columnNumber: 208\n    }\n  }), \" steps of gradient descent with step size \", /*#__PURE__*/React.createElement(InlineMath, {\n    math: \"\\\\eta = \\\\frac{2}{m + M}\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 50,\n      columnNumber: 273\n    }\n  }), \":\"), /*#__PURE__*/React.createElement(BlockMath, {\n    math: \"||\\\\theta_T - \\\\theta^*||_2 \\\\leq \\\\left( \\\\frac{M - m}{M + m} \\\\right)^T || \\\\theta_0 - \\\\theta^*||_2\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 52,\n      columnNumber: 9\n    }\n  }), /*#__PURE__*/React.createElement(\"p\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 53,\n      columnNumber: 9\n    }\n  }, \"Now, for sake of intuition, I will give a geometric argument for showing how we can get some notion of machine unlearning to work.\"), /*#__PURE__*/React.createElement(\"center\", {\n    style: {\n      'background-color': 'white',\n      'padding': '20px',\n      'border-radius': '20px'\n    },\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 57,\n      columnNumber: 9\n    }\n  }, /*#__PURE__*/React.createElement(\"img\", {\n    src: \"../machine-unlearning/image2.svg\",\n    width: \"50%\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 58,\n      columnNumber: 11\n    }\n  })), /*#__PURE__*/React.createElement(\"br\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 61,\n      columnNumber: 9\n    }\n  }), /*#__PURE__*/React.createElement(\"p\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 61,\n      columnNumber: 14\n    }\n  }, \"We begin with Case A, where we train on the full dataset and eventually receive a deletion request. To begin, we perform gradient descent on the full dataset for \", /*#__PURE__*/React.createElement(InlineMath, {\n    math: \"t\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 62,\n      columnNumber: 173\n    }\n  }), \" steps until \", /*#__PURE__*/React.createElement(InlineMath, {\n    math: \"\\\\theta_0\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 62,\n      columnNumber: 208\n    }\n  }), \" becomes \", /*#__PURE__*/React.createElement(InlineMath, {\n    math: \"\\\\theta_t\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 62,\n      columnNumber: 246\n    }\n  }), \". The dotted line represents how far away \", /*#__PURE__*/React.createElement(InlineMath, {\n    math: \"\\\\theta_t\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 62,\n      columnNumber: 317\n    }\n  }), \" can possibly be from \", /*#__PURE__*/React.createElement(InlineMath, {\n    math: \"\\\\theta^D\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 62,\n      columnNumber: 368\n    }\n  }), \" due to our convergence guarantee.\"), /*#__PURE__*/React.createElement(\"center\", {\n    style: {\n      'background-color': 'white',\n      'padding': '20px',\n      'border-radius': '20px'\n    },\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 65,\n      columnNumber: 9\n    }\n  }, /*#__PURE__*/React.createElement(\"img\", {\n    src: \"../machine-unlearning/image1.svg\",\n    width: \"50%\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 66,\n      columnNumber: 11\n    }\n  })), /*#__PURE__*/React.createElement(\"br\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 69,\n      columnNumber: 9\n    }\n  }), /*#__PURE__*/React.createElement(\"p\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 69,\n      columnNumber: 14\n    }\n  }, \"First, let\", \"\\'\", \"s consider Case A. We\", \"\\'\", \"ll need to establish a notion of \", /*#__PURE__*/React.createElement(\"em\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 70,\n      columnNumber: 87\n    }\n  }, \"sensitivity\"), \". That is, we have some global optimum across the entire dataset \", /*#__PURE__*/React.createElement(InlineMath, {\n    math: \"\\\\theta^D\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 70,\n      columnNumber: 172\n    }\n  }), \". Now, the \", /*#__PURE__*/React.createElement(\"em\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 70,\n      columnNumber: 212\n    }\n  }, \"sensitivity\"), \" is the furthest away the global minimum can change due to the removal of a single point \", /*#__PURE__*/React.createElement(InlineMath, {\n    math: \"x\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 70,\n      columnNumber: 321\n    }\n  }), \". That is, when we remove \", /*#__PURE__*/React.createElement(InlineMath, {\n    math: \"x\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 70,\n      columnNumber: 369\n    }\n  }), \", we know that the global minimum can\", \"\\'\", \"t be \", /*#__PURE__*/React.createElement(\"em\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 70,\n      columnNumber: 439\n    }\n  }, \"too\"), \" far away.\"), /*#__PURE__*/React.createElement(\"center\", {\n    style: {\n      'background-color': 'white',\n      'padding': '20px',\n      'border-radius': '20px'\n    },\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 73,\n      columnNumber: 9\n    }\n  }, /*#__PURE__*/React.createElement(\"img\", {\n    src: \"../machine-unlearning/image3.svg\",\n    width: \"50%\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 74,\n      columnNumber: 11\n    }\n  })), /*#__PURE__*/React.createElement(\"br\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 77,\n      columnNumber: 9\n    }\n  }), /*#__PURE__*/React.createElement(\"p\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 77,\n      columnNumber: 14\n    }\n  }, \"Now, at this point, let\", \"\\'\", \"s say we receive a request to remove \", /*#__PURE__*/React.createElement(InlineMath, {\n    math: \"x\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 78,\n      columnNumber: 77\n    }\n  }), \". In which case, we will start to perform gradient descent in the direction of \", /*#__PURE__*/React.createElement(InlineMath, {\n    math: \"\\\\theta^{D \\\\setminus x}\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 78,\n      columnNumber: 178\n    }\n  }), \". Say we perform an additional set of steps until we reach \", /*#__PURE__*/React.createElement(InlineMath, {\n    math: \"\\\\theta_T\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 78,\n      columnNumber: 280\n    }\n  }), \".\"), /*#__PURE__*/React.createElement(\"center\", {\n    style: {\n      'background-color': 'white',\n      'padding': '20px',\n      'border-radius': '20px'\n    },\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 81,\n      columnNumber: 9\n    }\n  }, /*#__PURE__*/React.createElement(\"img\", {\n    src: \"../machine-unlearning/image4.svg\",\n    width: \"50%\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 82,\n      columnNumber: 11\n    }\n  })), /*#__PURE__*/React.createElement(\"br\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 85,\n      columnNumber: 9\n    }\n  }), /*#__PURE__*/React.createElement(\"p\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 85,\n      columnNumber: 14\n    }\n  }, \"Now we consider alternative reality, Case B. That is, we started from our initial point and performed gradient descent but \", /*#__PURE__*/React.createElement(InlineMath, {\n    math: \"x\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 86,\n      columnNumber: 134\n    }\n  }), \" was never in the dataset. Given our convergence guarantee, we can know that we are at least within some distance of the optimum.\"), /*#__PURE__*/React.createElement(\"center\", {\n    style: {\n      'background-color': 'white',\n      'padding': '20px',\n      'border-radius': '20px'\n    },\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 89,\n      columnNumber: 9\n    }\n  }, /*#__PURE__*/React.createElement(\"img\", {\n    src: \"../machine-unlearning/image5.svg\",\n    width: \"50%\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 90,\n      columnNumber: 11\n    }\n  })), /*#__PURE__*/React.createElement(\"br\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 93,\n      columnNumber: 9\n    }\n  }), /*#__PURE__*/React.createElement(\"p\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 93,\n      columnNumber: 14\n    }\n  }, \"Now note that we can guarantee a certain distance from \", /*#__PURE__*/React.createElement(InlineMath, {\n    math: \"\\\\theta^{D \\\\setminus x}\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 94,\n      columnNumber: 66\n    }\n  }), \" regardless. And in turn, we can guarantee that \", /*#__PURE__*/React.createElement(InlineMath, {\n    math: \"\\\\theta_T\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 94,\n      columnNumber: 157\n    }\n  }), \" and \", /*#__PURE__*/React.createElement(InlineMath, {\n    math: \"\\\\theta\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 94,\n      columnNumber: 191\n    }\n  }), \" are within some distance of one another.\"), /*#__PURE__*/React.createElement(\"center\", {\n    style: {\n      'background-color': 'white',\n      'padding': '20px',\n      'border-radius': '20px'\n    },\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 97,\n      columnNumber: 9\n    }\n  }, /*#__PURE__*/React.createElement(\"img\", {\n    src: \"../machine-unlearning/image6.svg\",\n    width: \"50%\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 98,\n      columnNumber: 11\n    }\n  })), /*#__PURE__*/React.createElement(\"br\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 101,\n      columnNumber: 9\n    }\n  }), /*#__PURE__*/React.createElement(\"p\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 101,\n      columnNumber: 14\n    }\n  }, \"Finally, in either case, we publish the models according to the same publishing scheme, namely an injection of Gaussian noise scaled to how far \", /*#__PURE__*/React.createElement(InlineMath, {\n    math: \"\\\\theta_T\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 102,\n      columnNumber: 155\n    }\n  }), \" and \", /*#__PURE__*/React.createElement(InlineMath, {\n    math: \"\\\\theta\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 102,\n      columnNumber: 189\n    }\n  }), \" can be from one another. All of this will entail that we can actually attain statistical indistinguishability while also achieving some utility.\"), /*#__PURE__*/React.createElement(\"h3\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 105,\n      columnNumber: 9\n    }\n  }, \"Conlusion\"), /*#__PURE__*/React.createElement(\"p\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 108,\n      columnNumber: 9\n    }\n  }, \"Hopefully this was a useful tutorial outlining differential privacy and its applications to deep learning in plain English. If you have any questions or have caught any errors, feel free to reach out!\"));\n}\n\nexport default Post;","map":{"version":3,"sources":["/Users/chriswaites/Documents/projects/ChrisWaites.github.io/src/posts/machine-unlearning.js"],"names":["React","InlineMath","BlockMath","Post"],"mappings":";AAAA,OAAOA,KAAP,MAAkB,OAAlB;AACA,OAAO,0BAAP;AACA,SAASC,UAAT,EAAqBC,SAArB,QAAsC,aAAtC;;AAGA,SAASC,IAAT,GAAgB;AACZ,sBACE,uDACE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,0BADF,eAIE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,aACO,IADP,mCAC0C,IAD1C,iIAC6J;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,iBAD7J,wHAJF,eAOE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,0BACM,oBAAC,UAAD;AAAY,IAAA,IAAI,EAAC,GAAjB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IADN,8IACqK,IADrK,sFAPF,eAUE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,mBACa,IADb,0BAVF,eAaE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,kBACE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,kBACA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,uFACmE,oBAAC,UAAD;AAAY,IAAA,IAAI,EAAC,GAAjB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IADnE,+BACqG,oBAAC,UAAD;AAAY,IAAA,IAAI,EAAC,GAAjB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IADrG,6EADA,CADF,eAME;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,kBACA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,oDACgC,oBAAC,UAAD;AAAY,IAAA,IAAI,EAAC,GAAjB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IADhC,+EACgI,IADhI,6BAC+I,oBAAC,UAAD;AAAY,IAAA,IAAI,EAAC,GAAjB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IAD/I,iDADA,CANF,CAbF,eAyBE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,uIAzBF,eA4BE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,0MACsL,oBAAC,UAAD;AAAY,IAAA,IAAI,EAAC,SAAjB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IADtL,qBA5BF,eA+BE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,4BACsB,IADtB,gCA/BF,eAkCE,oBAAC,SAAD;AAAW,IAAA,IAAI,EAAC,sJAAhB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IAlCF,eAmCE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,mEAC+C,oBAAC,UAAD;AAAY,IAAA,IAAI,EAAC,GAAjB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IAD/C,wCAC0F,oBAAC,UAAD;AAAY,IAAA,IAAI,EAAC,GAAjB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IAD1F,iCAC8H,oBAAC,UAAD;AAAY,IAAA,IAAI,EAAC,cAAjB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IAD9H,wBACmK,oBAAC,UAAD;AAAY,IAAA,IAAI,EAAC,kBAAjB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IADnK,MAnCF,eAsCE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,sRAtCF,eAyCE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,0BACM,oBAAC,UAAD;AAAY,IAAA,IAAI,EAAC,gBAAjB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IADN,uBAC4C,oBAAC,UAAD;AAAY,IAAA,IAAI,EAAC,GAAjB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IAD5C,0DAC0G,oBAAC,UAAD;AAAY,IAAA,IAAI,EAAC,0DAAjB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IAD1G,wCACuM,oBAAC,UAAD;AAAY,IAAA,IAAI,EAAC,GAAjB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IADvM,6DACwQ,oBAAC,UAAD;AAAY,IAAA,IAAI,EAAC,0BAAjB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IADxQ,MAzCF,eA4CE,oBAAC,SAAD;AAAW,IAAA,IAAI,EAAC,wGAAhB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IA5CF,eA6CE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,0IA7CF,eAiDE;AAAQ,IAAA,KAAK,EAAE;AAAE,0BAAoB,OAAtB;AAA+B,iBAAW,MAA1C;AAAkD,uBAAiB;AAAnE,KAAf;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,kBACE;AAAK,IAAA,GAAG,EAAC,kCAAT;AAA4C,IAAA,KAAK,EAAC,KAAlD;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IADF,CAjDF,eAqDE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IArDF,eAqDO;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,wLAC+J,oBAAC,UAAD;AAAY,IAAA,IAAI,EAAC,GAAjB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IAD/J,gCACkM,oBAAC,UAAD;AAAY,IAAA,IAAI,EAAC,WAAjB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IADlM,4BACwO,oBAAC,UAAD;AAAY,IAAA,IAAI,EAAC,WAAjB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IADxO,6DAC+S,oBAAC,UAAD;AAAY,IAAA,IAAI,EAAC,WAAjB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IAD/S,yCACkW,oBAAC,UAAD;AAAY,IAAA,IAAI,EAAC,WAAjB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IADlW,uCArDP,eAyDE;AAAQ,IAAA,KAAK,EAAE;AAAE,0BAAoB,OAAtB;AAA+B,iBAAW,MAA1C;AAAkD,uBAAiB;AAAnE,KAAf;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,kBACE;AAAK,IAAA,GAAG,EAAC,kCAAT;AAA4C,IAAA,KAAK,EAAC,KAAlD;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IADF,CAzDF,eA6DE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IA7DF,eA6DO;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,mBACQ,IADR,2BACmC,IADnC,oDACyE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,mBADzE,oFAC8J,oBAAC,UAAD;AAAY,IAAA,IAAI,EAAC,WAAjB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IAD9J,8BACsM;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,mBADtM,4GACmT,oBAAC,UAAD;AAAY,IAAA,IAAI,EAAC,GAAjB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IADnT,6CACmW,oBAAC,UAAD;AAAY,IAAA,IAAI,EAAC,GAAjB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IADnW,2CAC+Z,IAD/Z,wBACya;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,WADza,eA7DP,eAiEE;AAAQ,IAAA,KAAK,EAAE;AAAE,0BAAoB,OAAtB;AAA+B,iBAAW,MAA1C;AAAkD,uBAAiB;AAAnE,KAAf;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,kBACE;AAAK,IAAA,GAAG,EAAC,kCAAT;AAA4C,IAAA,KAAK,EAAC,KAAlD;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IADF,CAjEF,eAqEE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IArEF,eAqEO;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,gCACqB,IADrB,wDAC+D,oBAAC,UAAD;AAAY,IAAA,IAAI,EAAC,GAAjB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IAD/D,kGACoK,oBAAC,UAAD;AAAY,IAAA,IAAI,EAAC,0BAAjB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IADpK,8EAC0Q,oBAAC,UAAD;AAAY,IAAA,IAAI,EAAC,WAAjB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IAD1Q,MArEP,eAyEE;AAAQ,IAAA,KAAK,EAAE;AAAE,0BAAoB,OAAtB;AAA+B,iBAAW,MAA1C;AAAkD,uBAAiB;AAAnE,KAAf;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,kBACE;AAAK,IAAA,GAAG,EAAC,kCAAT;AAA4C,IAAA,KAAK,EAAC,KAAlD;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IADF,CAzEF,eA6EE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IA7EF,eA6EO;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,iJACwH,oBAAC,UAAD;AAAY,IAAA,IAAI,EAAC,GAAjB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IADxH,sIA7EP,eAiFE;AAAQ,IAAA,KAAK,EAAE;AAAE,0BAAoB,OAAtB;AAA+B,iBAAW,MAA1C;AAAkD,uBAAiB;AAAnE,KAAf;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,kBACE;AAAK,IAAA,GAAG,EAAC,kCAAT;AAA4C,IAAA,KAAK,EAAC,KAAlD;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IADF,CAjFF,eAqFE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IArFF,eAqFO;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,6EACoD,oBAAC,UAAD;AAAY,IAAA,IAAI,EAAC,0BAAjB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IADpD,mEAC+I,oBAAC,UAAD;AAAY,IAAA,IAAI,EAAC,WAAjB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IAD/I,wBACiL,oBAAC,UAAD;AAAY,IAAA,IAAI,EAAC,SAAjB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IADjL,8CArFP,eAyFE;AAAQ,IAAA,KAAK,EAAE;AAAE,0BAAoB,OAAtB;AAA+B,iBAAW,MAA1C;AAAkD,uBAAiB;AAAnE,KAAf;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,kBACE;AAAK,IAAA,GAAG,EAAC,kCAAT;AAA4C,IAAA,KAAK,EAAC,KAAlD;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IADF,CAzFF,eA6FE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IA7FF,eA6FO;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,sKAC6I,oBAAC,UAAD;AAAY,IAAA,IAAI,EAAC,WAAjB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IAD7I,wBAC+K,oBAAC,UAAD;AAAY,IAAA,IAAI,EAAC,SAAjB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IAD/K,sJA7FP,eAiGE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,iBAjGF,eAoGE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,gNApGF,CADF;AA0GH;;AAED,eAAeA,IAAf","sourcesContent":["import React from 'react';\nimport 'katex/dist/katex.min.css';\nimport { InlineMath, BlockMath } from 'react-katex';\n\n\nfunction Post() {\n    return (\n      <>\n        <h2>\n          Machine Unlearning\n        </h2>\n        <p>\n          Here{\"\\'\"}s an interesting problem: Let{\"\\'\"}s imagine your data was used to train some machine learning model. Now, you want to request that your data be <em>unlearned</em> from the model. That is, the model is updated so that its parameters and outputs have no knowledge of you anymore.\n        </p>\n        <p>\n          For <InlineMath math=\"k\"/>-nearest neighbors, the problem is trivial - simply remove the point from the data. For something more complex like a neural network, it{\"\\'\"}s not immediately clear what you would do. Is their an interesting middle-ground?\n        </p>\n        <p>\n          First, let{\"\\'\"}s define the problem.\n        </p>\n        <ul>\n          <li>\n          <em>\n            Case A (Real): We train the model on the full dataset containing <InlineMath math=\"x\"/>, we remove <InlineMath math=\"x\"/> from the model parameters, and then we publish the model to the public.\n          </em>\n          </li>\n          <li>\n          <em>\n            Case B (Imaginary): The point <InlineMath math=\"x\"/> was never in the dataset, we train the model on the dataset (which doesn{\"\\'\"}t contain <InlineMath math=\"x\"/>), and then publish the model to the public.\n          </em>\n          </li>\n        </ul>\n        <p>\n          If we can find a procedure for Case A which yields an indistinguishable outcome to Case B, then we will have achieved our goal.\n        </p>\n        <p>\n          For the purposes of this post, we are going to tackle the case of a convex parametric model. For example, this would include logistic regression with binary cross entropy loss and <InlineMath math=\"\\ell_2\" /> regularization.\n        </p>\n        <p>\n          To be concrete, let{\"\\'\"}s say our loss function is:\n        </p>\n        <BlockMath math=\"\\ell(\\theta) = - \\frac{1}{n} \\left( \\sum_{i \\in [n]} y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right) + \\lambda ||\\theta||_2\"/>\n        <p>\n          Then we would have that our loss function is <InlineMath math=\"m\"/>-strongly convex and <InlineMath math=\"M\"/>-smooth where <InlineMath math=\"m = \\lambda\"/> and <InlineMath math=\"M = 4 - \\lambda\"/>.\n        </p>\n        <p>\n          Now, the great part about working in this setting is that we have convergence guarantees. That is, as long as we know how far we are at most from the global optimum, if we run gradient descent, we know we will be at most some distance after some number of steps we take.\n        </p>\n        <em>\n          Let <InlineMath math=\"\\ell(\\theta)\" /> be <InlineMath math=\"m\" />-strongly convex and M-smooth, and let <InlineMath math=\"\\theta^* = argmin_{\\theta \\in \\Theta} \\ell(\\theta)\" />. We have that after <InlineMath math=\"T\" /> steps of gradient descent with step size <InlineMath math=\"\\eta = \\frac{2}{m + M}\" />:\n        </em>\n        <BlockMath math=\"||\\theta_T - \\theta^*||_2 \\leq \\left( \\frac{M - m}{M + m} \\right)^T || \\theta_0 - \\theta^*||_2\" />\n        <p>\n          Now, for sake of intuition, I will give a geometric argument for showing how we can get some notion of machine unlearning to work.\n        </p>\n\n        <center style={{ 'background-color': 'white', 'padding': '20px', 'border-radius': '20px' }}>\n          <img src=\"../machine-unlearning/image2.svg\" width=\"50%\"/>\n        </center>\n\n        <br/><p>\n          We begin with Case A, where we train on the full dataset and eventually receive a deletion request. To begin, we perform gradient descent on the full dataset for <InlineMath math=\"t\"/> steps until <InlineMath math=\"\\theta_0\"/> becomes <InlineMath math=\"\\theta_t\"/>. The dotted line represents how far away <InlineMath math=\"\\theta_t\"/> can possibly be from <InlineMath math=\"\\theta^D\"/> due to our convergence guarantee.\n        </p>\n\n        <center style={{ 'background-color': 'white', 'padding': '20px', 'border-radius': '20px' }}>\n          <img src=\"../machine-unlearning/image1.svg\" width=\"50%\"/>\n        </center>\n\n        <br/><p>\n          First, let{\"\\'\"}s consider Case A. We{\"\\'\"}ll need to establish a notion of <em>sensitivity</em>. That is, we have some global optimum across the entire dataset <InlineMath math=\"\\theta^D\"/>. Now, the <em>sensitivity</em> is the furthest away the global minimum can change due to the removal of a single point <InlineMath math=\"x\"/>. That is, when we remove <InlineMath math=\"x\"/>, we know that the global minimum can{\"\\'\"}t be <em>too</em> far away.\n        </p>\n\n        <center style={{ 'background-color': 'white', 'padding': '20px', 'border-radius': '20px' }}>\n          <img src=\"../machine-unlearning/image3.svg\" width=\"50%\"/>\n        </center>\n\n        <br/><p>\n          Now, at this point, let{\"\\'\"}s say we receive a request to remove <InlineMath math=\"x\"/>. In which case, we will start to perform gradient descent in the direction of <InlineMath math=\"\\theta^{D \\setminus x}\"/>. Say we perform an additional set of steps until we reach <InlineMath math=\"\\theta_T\"/>.\n        </p>\n\n        <center style={{ 'background-color': 'white', 'padding': '20px', 'border-radius': '20px' }}>\n          <img src=\"../machine-unlearning/image4.svg\" width=\"50%\"/>\n        </center>\n\n        <br/><p>\n          Now we consider alternative reality, Case B. That is, we started from our initial point and performed gradient descent but <InlineMath math=\"x\"/> was never in the dataset. Given our convergence guarantee, we can know that we are at least within some distance of the optimum.\n        </p>\n\n        <center style={{ 'background-color': 'white', 'padding': '20px', 'border-radius': '20px' }}>\n          <img src=\"../machine-unlearning/image5.svg\" width=\"50%\"/>\n        </center>\n\n        <br/><p>\n          Now note that we can guarantee a certain distance from <InlineMath math=\"\\theta^{D \\setminus x}\"/> regardless. And in turn, we can guarantee that <InlineMath math=\"\\theta_T\"/> and <InlineMath math=\"\\theta\"/> are within some distance of one another.\n        </p>\n\n        <center style={{ 'background-color': 'white', 'padding': '20px', 'border-radius': '20px' }}>\n          <img src=\"../machine-unlearning/image6.svg\" width=\"50%\"/>\n        </center>\n\n        <br/><p>\n          Finally, in either case, we publish the models according to the same publishing scheme, namely an injection of Gaussian noise scaled to how far <InlineMath math=\"\\theta_T\"/> and <InlineMath math=\"\\theta\"/> can be from one another. All of this will entail that we can actually attain statistical indistinguishability while also achieving some utility.\n        </p>\n\n        <h3>\n          Conlusion\n        </h3>\n        <p>\n          Hopefully this was a useful tutorial outlining differential privacy and its applications to deep learning in plain English. If you have any questions or have caught any errors, feel free to reach out!\n        </p>\n      </>\n    );\n}\n\nexport default Post;\n"]},"metadata":{},"sourceType":"module"}