{"version":3,"sources":["serviceWorker.js","components/page.js","pages/research.js","pages/about.js","pages/writing.js","pages/reading.js","pages/design.js","posts/differentially-private-deep-learning.js","posts/how-differential-privacy-fits-into-industry.js","posts/risk-aware-reinforcement-learning.js","posts/deaths-of-despair.js","App.js","index.js"],"names":["Boolean","window","location","hostname","match","Page","props","style","display","alignItems","justifyContent","padding","width","textAlign","children","paddingTop","className","to","href","Entry","Button","variant","src","borderRadius","Post","math","height","frameborder","allow","allowfullscreen","url","maxWidth","hideCaption","App","exact","path","component","About","Research","Writing","Reading","Design","DeathsOfDespair","HowDifferentialPrivacyFitsIntoIndustry","DifferentiallyPrivateDeepLearning","RiskAwareReinforcementLearning","ReactDOM","render","document","getElementById","navigator","serviceWorker","ready","then","registration","unregister","catch","error","console","message"],"mappings":"gNAYoBA,QACW,cAA7BC,OAAOC,SAASC,UAEe,UAA7BF,OAAOC,SAASC,UAEhBF,OAAOC,SAASC,SAASC,MACvB,2D,0CCkBSC,MAhCf,SAAcC,GACZ,OACE,yBAAKC,MAAO,CAAEC,QAAS,OAAQC,WAAY,aAAcC,eAAgB,WACvE,yBAAKH,MAAO,CAAEI,QAAS,OAAQC,MAAO,MAAOC,UAAW,SACrDP,EAAMQ,UAET,kBAAC,IAAD,CAAWP,MAAO,CAAEQ,WAAY,OAAQF,UAAW,UACjD,+BACE,4BAAI,4BACF,gCAAQ,kBAAC,IAAD,CAAMG,UAAU,OAAOC,GAAG,KAA1B,mBAEV,4BAAI,4BACF,kBAAC,IAAD,CAAMD,UAAU,OAAOC,GAAG,aAA1B,cAEF,4BAAI,4BACF,kBAAC,IAAD,CAAMD,UAAU,OAAOC,GAAG,YAA1B,aAEF,4BAAI,4BACF,kBAAC,IAAD,CAAMD,UAAU,OAAOC,GAAG,YAA1B,aAEF,4BAAI,4BACF,kBAAC,IAAD,CAAMD,UAAU,OAAOC,GAAG,WAA1B,YAEF,4BAAI,4BACF,uBAAGC,KAAK,kCAAR,gB,eCvBZ,SAASC,EAAMb,GACb,OACE,2BACE,kBAACc,EAAA,EAAD,CAAQC,QAAQ,WAAWH,KAAMZ,EAAMY,MACpCZ,EAAMQ,WA4BAT,MApBf,WACI,OACE,oCACE,kBAACc,EAAD,CAAOD,KAAK,oEACV,kHAA2F,6BAC3F,6BAFF,oCAGmC,6BACjC,gIAGF,kBAACC,EAAD,CAAOD,KAAK,oCACV,2GAAoF,6BACpF,6BAFF,mGAGkG,6BAChG,oDCRKb,MApBf,WACI,OACE,oCACE,2BACE,yBAAKiB,IAAI,yBAAyBf,MAAO,CAAEgB,aAAc,OAAQX,MAAO,WAE1E,4CAGA,0CACe,IADf,0CAC2D,uBAAGM,KAAK,6BAAR,uBAD3D,2FAGA,wCAGA,kBAAC,EAAD,QCbR,SAASC,EAAMb,GACb,OACE,2BACE,kBAACc,EAAA,EAAD,CAAQC,QAAQ,WAAWH,KAAMZ,EAAMY,MACrC,4BAAKZ,EAAMQ,YAsBJT,MAhBf,WACI,OACE,oCACE,kBAAC,EAAD,CAAOa,KAAK,8CAAZ,qCAGA,kBAAC,EAAD,CAAQA,KAAK,wDAAb,sDAGA,kBAAC,EAAD,CAAOA,KAAK,iDAAZ,qDCnBR,SAASC,EAAMb,GACb,OACE,2BACE,kBAACc,EAAA,EAAD,CAAQC,QAAQ,WAAWH,KAAMZ,EAAMY,MACrC,4BAAKZ,EAAMQ,YA+BJT,MAzBf,WACI,OACE,oCACE,kBAAC,EAAD,CAAOa,KAAK,6EAAZ,kCAGA,kBAAC,EAAD,CAAOA,KAAK,iGAAZ,8CAGA,kBAAC,EAAD,CAAOA,KAAK,mEAAZ,sDAGA,kBAAC,EAAD,CAAOA,KAAK,oCAAZ,sCAGA,kBAAC,EAAD,CAAOA,KAAK,oCAAZ,wDAGA,kBAAC,EAAD,CAAOA,KAAK,oCAAZ,oECfOb,MAbf,WACI,OACE,oCACE,2BACE,yBAAKiB,IAAI,0BAA0Bf,MAAO,CAAEgB,aAAc,OAAQX,MAAO,WAE3E,2BACE,yBAAKU,IAAI,eAAef,MAAO,CAAEgB,aAAc,OAAQX,MAAO,a,eC6EzDY,MApFf,WACI,OACE,oCACE,+EAGA,sLAIA,oDAGA,sKACqI,oDADrI,ygBAGA,0EAC8C,6BAAK,uBAAGN,KAAK,yDAAR,MADnD,KAIA,6CAGA,2kBAGA,kBAAC,YAAD,CAAWO,KAAK,iEAChB,kBAAC,YAAD,CAAWA,KAAK,mEAChB,kBAAC,YAAD,CAAWA,KAAK,uEAChB,m3BAGA,sZAIA,kFAGA,2BACE,uBAAGP,KAAK,oCAAR,gBADF,qNAGA,2PAC+N,kBAAC,aAAD,CAAYO,KAAK,MADhP,qIACwX,kCADxX,cAC+Y,kCAD/Y,6DAGA,gKACoI,kBAAC,aAAD,CAAYA,KAAK,MADrJ,2BACmL,kBAAC,aAAD,CAAYA,KAAK,YADpM,uEACmR,kBAAC,aAAD,CAAYA,KAAK,YADpS,+IAGA,kBAAC,YAAD,CAAWA,KAAK,iEAChB,kBAAC,YAAD,CAAWA,KAAK,mEAChB,kBAAC,YAAD,CAAWA,KAAK,0EAChB,kBAAC,YAAD,CAAWA,KAAK,gGAChB,kBAAC,YAAD,CAAWA,KAAK,mDAChB,szBAIA,wEAGA,umBAIA,yEAIA,kDAGA,uKAGA,oYACyW,IADzW,uRAGA,yJClCOD,MA9Cf,WACE,OACE,6BACE,kFAGA,2EACkD,IADlD,oQAGA,8HACoG,gDADpG,+XAGA,+gBAGA,kVAGA,0FAC2D,4LAD3D,KAGA,uiBAGA,0TAGA,0jBAGA,gRACsP,sCADtP,QAC0Q,0CAD1Q,KAGA,yVAGA,0bAGA,wRCyDSA,MA9Ff,WACE,OACE,oCACE,iEAIA,6HAIA,4CAGA,0KAC+I,IAD/I,kCAGA,iEACqC,kBAAC,aAAD,CAAYC,KAAK,yEADtD,2BACiJ,kBAAC,aAAD,CAAYA,KAAK,MADlK,kBACuL,kBAAC,aAAD,CAAYA,KAAK,MADxM,6BACwO,kBAAC,aAAD,CAAYA,KAAK,mBADzP,sDAC8T,kBAAC,aAAD,CAAYA,KAAK,MAD/U,aAC+V,kBAAC,aAAD,CAAYA,KAAK,MADhX,+HACkf,kBAAC,aAAD,CAAYA,KAAK,mEADngB,iRAGA,kBAAC,YAAD,CAAWA,KAAK,6FAChB,uEAC2C,kBAAC,aAAD,CAAYA,KAAK,8CAD5D,kCACsI,kBAAC,aAAD,CAAYA,KAAK,MADvJ,0BACoL,kBAAC,aAAD,CAAYA,KAAK,MADrM,+DACuQ,kBAAC,aAAD,CAAYA,KAAK,MADxR,QACmS,kBAAC,aAAD,CAAYA,KAAK,MADpT,gFACuY,kBAAC,aAAD,CAAYA,KAAK,MADxZ,OACka,kBAAC,aAAD,CAAYA,KAAK,MADnb,yLAGA,kBAAC,YAAD,CAAWA,KAAK,kFAChB,oPAIA,6CAGA,+DACoC,IADpC,8EACoH,4CADpH,mLAGA,iFACsD,IADtD,KAC6D,gGAD7D,4CAC+K,oCAD/K,mHAGA,wEAC4C,gEAD5C,0FAC4K,kBAAC,aAAD,CAAYA,KAAK,YAD7L,sBAC2N,kBAAC,aAAD,CAAYA,KAAK,uBAD5O,6DAC2T,kBAAC,aAAD,CAAYA,KAAK,gBAD5U,gJAGA,qEAC2C,4BAAI,uBAAGP,KAAK,oCAAR,+BAD/C,iDAIA,mDAGA,4OAGA,kBAAC,YAAD,CAAWO,KAAK,kEAChB,wEAC8C,kBAAC,aAAD,CAAYA,KAAK,mBAD/D,QACsF,kBAAC,aAAD,CAAYA,KAAK,2BADvG,6NAGA,kBAAC,YAAD,CAAWA,KAAK,mEAChB,4NAGA,kBAAC,YAAD,CAAWA,KAAK,0FAChB,kBAAC,YAAD,CAAWA,KAAK,2IAChB,8CACoB,yCADpB,YAC+C,kBAAC,aAAD,CAAYA,KAAK,mBADhE,QACsF,kBAAC,aAAD,CAAYA,KAAK,2BADvG,oBACgJ,uCADhJ,YACyK,kBAAC,aAAD,CAAYA,KAAK,YAD1L,QAC0M,kBAAC,aAAD,CAAYA,KAAK,oBAD3N,KAGA,4XAGA,kBAAC,YAAD,CAAWA,KAAK,kDAChB,kBAAC,YAAD,CAAWA,KAAK,iGAChB,mGACuE,kBAAC,aAAD,CAAYA,KAAK,6BADxF,QACyH,kBAAC,aAAD,CAAYA,KAAK,6CAD1I,6BAIA,kDAGA,yJAC6H,kBAAC,aAAD,CAAYA,KAAK,MAD9I,8CAC+L,kBAAC,aAAD,CAAYA,KAAK,MADhN,qEACwR,kBAAC,aAAD,CAAYA,KAAK,mBADzS,mBAC2U,kBAAC,aAAD,CAAYA,KAAK,2BAD5V,iEACmb,sCADnb,kCACme,kBAAC,aAAD,CAAYA,KAAK,mBADpf,yDAC2jB,kBAAC,aAAD,CAAYA,KAAK,YAD5kB,QAC4lB,kBAAC,aAAD,CAAYA,KAAK,mBAD7mB,gBAGA,kBAAC,YAAD,CAAWA,KAAK,mHAChB,qHACyF,0CADzF,8BAIA,0CAGA,2L,QCMSD,MA5Ff,WACE,OACE,oCACE,iDAGA,2BACA,0JAIA,iPAGA,+VAGA,uSAGA,sRAIA,iDAGA,4FACmE,IADnE,yDAC+H,IAD/H,wUAGA,+eAGA,kEACyC,IADzC,+vBAC2yB,IAD3yB,2KAGA,8UACqT,IADrT,6YAGA,2yBAGA,kOACyM,IADzM,mEAGA,4BAAQZ,MAAM,MAAMc,OAAO,MAAMJ,IAAI,uDAAuDK,YAAY,IAAIC,MAAM,0EAA0EC,iBAAe,IAE3M,4CAIA,sCAGA,mCACQ,IADR,uFACiG,uBAAGX,KAAK,iJAAR,QADjG,aAIA,2BACE,iCACA,iCAAS,6FACT,2JAC8H,IAD9H,wBACyJ,IADzJ,oDACgN,IADhN,yCAC2P,6BAC3P,6BACE,sCAAY,IAAZ,sDAAqE,IAArE,0GAHF,oGAGwR,IAHxR,8GAGyY,IAHzY,4SAGwrB,IAHxrB,yPAGo7B,IAHp7B,mDAG0+B,IAH1+B,wDAGqiC,IAHriC,eAGsjC,6BACtjC,6BACE,sKALF,+jBAK6sB,IAL7sB,oFAKoyB,IALpyB,8DAKq2B,IALr2B,oDAK45B,IAL55B,4RAK2rC,IAL3rC,qKAKm2C,IALn2C,yJAK+/C,IAL//C,qLAKurD,IALvrD,yIAKm0D,IALn0D,qCAK02D,6BAC12D,6BACE,sCAAY,IAAZ,mBAAkC,IAAlC,2KAPF,uCAOyP,IAPzP,gYAO2nB,6BAC3nB,6BACE,kDAAwB,IAAxB,yKATF,gLASsX,IATtX,kSAS2pB,IAT3pB,qFASmvB,IATnvB,kCASwxB,IATxxB,mJAS66B,6BAC76B,6BACE,qIAA2G,IAA3G,iDAXF,SAW0K,IAX1K,2VAWugB,6BACvgB,6BAZA,+CAa+C,IAb/C,sFAgBA,8BAGF,sCAIA,kBAAC,IAAD,CAAgBY,IAAI,oCAAoCC,SAAU,IAAKC,aAAa,IACpF,kBAAC,IAAD,CAAgBF,IAAI,oCAAoCC,SAAU,IAAKC,aAAa,IAEpF,4BAAQpB,MAAM,MAAMc,OAAO,MAAMJ,IAAI,uDAAuDK,YAAY,IAAIC,MAAM,0EAA0EC,iBAAe,MCtDlMI,MArBf,WACE,OACE,kBAAC,EAAD,KACE,kBAAC,IAAD,KACE,kBAAC,IAAD,CAAOC,OAAK,EAACC,KAAK,IAAIC,UAAWC,IAEjC,kBAAC,IAAD,CAAOH,OAAK,EAACC,KAAK,SAASC,UAAWC,IACtC,kBAAC,IAAD,CAAOH,OAAK,EAACC,KAAK,YAAYC,UAAWE,IACzC,kBAAC,IAAD,CAAOJ,OAAK,EAACC,KAAK,WAAWC,UAAWG,IACxC,kBAAC,IAAD,CAAOL,OAAK,EAACC,KAAK,WAAWC,UAAWI,IACxC,kBAAC,IAAD,CAAON,OAAK,EAACC,KAAK,UAAUC,UAAWK,IAEvC,kBAAC,IAAD,CAAOP,OAAK,EAACC,KAAK,6BAA6BC,UAAWM,IAC1D,kBAAC,IAAD,CAAOR,OAAK,EAACC,KAAK,uDAAuDC,UAAWO,IACpF,kBAAC,IAAD,CAAOT,OAAK,EAACC,KAAK,gDAAgDC,UAAWQ,IAC7E,kBAAC,IAAD,CAAOV,OAAK,EAACC,KAAK,6CAA6CC,UAAWS,O,eCzBlFC,EAASC,OACP,kBAAC,IAAD,KACE,kBAAC,EAAD,OAECC,SAASC,eAAe,SZwHvB,kBAAmBC,WACrBA,UAAUC,cAAcC,MACrBC,MAAK,SAAAC,GACJA,EAAaC,gBAEdC,OAAM,SAAAC,GACLC,QAAQD,MAAMA,EAAME,c","file":"static/js/main.fd2773bb.chunk.js","sourcesContent":["// This optional code is used to register a service worker.\n// register() is not called by default.\n\n// This lets the app load faster on subsequent visits in production, and gives\n// it offline capabilities. However, it also means that developers (and users)\n// will only see deployed updates on subsequent visits to a page, after all the\n// existing tabs open on the page have been closed, since previously cached\n// resources are updated in the background.\n\n// To learn more about the benefits of this model and instructions on how to\n// opt-in, read https://bit.ly/CRA-PWA\n\nconst isLocalhost = Boolean(\n  window.location.hostname === 'localhost' ||\n    // [::1] is the IPv6 localhost address.\n    window.location.hostname === '[::1]' ||\n    // 127.0.0.0/8 are considered localhost for IPv4.\n    window.location.hostname.match(\n      /^127(?:\\.(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)){3}$/\n    )\n);\n\nexport function register(config) {\n  if (process.env.NODE_ENV === 'production' && 'serviceWorker' in navigator) {\n    // The URL constructor is available in all browsers that support SW.\n    const publicUrl = new URL(process.env.PUBLIC_URL, window.location.href);\n    if (publicUrl.origin !== window.location.origin) {\n      // Our service worker won't work if PUBLIC_URL is on a different origin\n      // from what our page is served on. This might happen if a CDN is used to\n      // serve assets; see https://github.com/facebook/create-react-app/issues/2374\n      return;\n    }\n\n    window.addEventListener('load', () => {\n      const swUrl = `${process.env.PUBLIC_URL}/service-worker.js`;\n\n      if (isLocalhost) {\n        // This is running on localhost. Let's check if a service worker still exists or not.\n        checkValidServiceWorker(swUrl, config);\n\n        // Add some additional logging to localhost, pointing developers to the\n        // service worker/PWA documentation.\n        navigator.serviceWorker.ready.then(() => {\n          console.log(\n            'This web app is being served cache-first by a service ' +\n              'worker. To learn more, visit https://bit.ly/CRA-PWA'\n          );\n        });\n      } else {\n        // Is not localhost. Just register service worker\n        registerValidSW(swUrl, config);\n      }\n    });\n  }\n}\n\nfunction registerValidSW(swUrl, config) {\n  navigator.serviceWorker\n    .register(swUrl)\n    .then(registration => {\n      registration.onupdatefound = () => {\n        const installingWorker = registration.installing;\n        if (installingWorker == null) {\n          return;\n        }\n        installingWorker.onstatechange = () => {\n          if (installingWorker.state === 'installed') {\n            if (navigator.serviceWorker.controller) {\n              // At this point, the updated precached content has been fetched,\n              // but the previous service worker will still serve the older\n              // content until all client tabs are closed.\n              console.log(\n                'New content is available and will be used when all ' +\n                  'tabs for this page are closed. See https://bit.ly/CRA-PWA.'\n              );\n\n              // Execute callback\n              if (config && config.onUpdate) {\n                config.onUpdate(registration);\n              }\n            } else {\n              // At this point, everything has been precached.\n              // It's the perfect time to display a\n              // \"Content is cached for offline use.\" message.\n              console.log('Content is cached for offline use.');\n\n              // Execute callback\n              if (config && config.onSuccess) {\n                config.onSuccess(registration);\n              }\n            }\n          }\n        };\n      };\n    })\n    .catch(error => {\n      console.error('Error during service worker registration:', error);\n    });\n}\n\nfunction checkValidServiceWorker(swUrl, config) {\n  // Check if the service worker can be found. If it can't reload the page.\n  fetch(swUrl, {\n    headers: { 'Service-Worker': 'script' },\n  })\n    .then(response => {\n      // Ensure service worker exists, and that we really are getting a JS file.\n      const contentType = response.headers.get('content-type');\n      if (\n        response.status === 404 ||\n        (contentType != null && contentType.indexOf('javascript') === -1)\n      ) {\n        // No service worker found. Probably a different app. Reload the page.\n        navigator.serviceWorker.ready.then(registration => {\n          registration.unregister().then(() => {\n            window.location.reload();\n          });\n        });\n      } else {\n        // Service worker found. Proceed as normal.\n        registerValidSW(swUrl, config);\n      }\n    })\n    .catch(() => {\n      console.log(\n        'No internet connection found. App is running in offline mode.'\n      );\n    });\n}\n\nexport function unregister() {\n  if ('serviceWorker' in navigator) {\n    navigator.serviceWorker.ready\n      .then(registration => {\n        registration.unregister();\n      })\n      .catch(error => {\n        console.error(error.message);\n      });\n  }\n}\n","import React from 'react';\nimport StickyBox from 'react-sticky-box';\nimport { Link } from 'react-router-dom';\n\nfunction Page(props) {\n  return (\n    <div style={{ display: 'flex', alignItems: 'flex-start', justifyContent: 'center' }}>\n      <div style={{ padding: '60px', width: '40%', textAlign: 'left' }}>\n        {props.children}\n      </div>\n      <StickyBox style={{ paddingTop: '60px', textAlign: 'right' }}>\n        <table>\n          <tr><td>\n            <strong><Link className='link' to='/'>Chris Waites</Link></strong>\n          </td></tr>\n          <tr><td>\n            <Link className='link' to='/research'>Research</Link>\n          </td></tr>\n          <tr><td>\n            <Link className='link' to='/writing'>Writing</Link>\n          </td></tr>\n          <tr><td>\n            <Link className='link' to='/reading'>Reading</Link>\n          </td></tr>\n          <tr><td>\n            <Link className='link' to='/design'>Design</Link>\n          </td></tr>\n          <tr><td>\n            <a href='https://github.com/ChrisWaites'>Github</a>\n          </td></tr>\n        </table>\n      </StickyBox>\n    </div>\n  );\n}\n\nexport default Page;\n","import React from 'react';\nimport Button from '@material-ui/core/Button';\nimport '../App.css';\n\n\nfunction Entry(props) {\n  return (\n    <p>\n      <Button variant='outlined' href={props.href}>\n        {props.children}\n      </Button>\n    </p>\n\n  );\n}\n\n\nfunction Page() {\n    return (\n      <>\n        <Entry href='https://invertibleworkshop.github.io/accepted_papers/pdfs/41.pdf'>\n          <em>Differentially Private Normalizing Flows for Privacy-Preserving Density Estimation</em><br/>\n          <hr/>\n          Chris Waites and Rachel Cummings.<br/>\n          <em>ICML Workshop on Invertible Neural Networks, Normalizing Flows, and Explicit Likelihood Models.</em>\n        </Entry>\n\n        <Entry href='https://arxiv.org/abs/1912.03250'>\n          <em>Differentially Private Mixed-Type Data Generation For Unsupervised Learning</em><br/>\n          <hr/>\n          Uthaipon Tantipongpipat*, Chris Waites*, Digvijay Boob, Amaresh Ankit Siva, and Rachel Cummings.<br/>\n          <em>arXiv:1912.03250.</em>\n        </Entry>\n      </>\n    );\n}\n\nexport default Page;\n","import React from 'react';\nimport Research from './research';\n\nfunction Page() {\n    return (\n      <>\n        <p>\n          <img src='./academic-banner.jpeg' style={{ borderRadius: '10px', width: '100%' }} />\n        </p>\n        <p>\n          Hi! Im Chris.\n        </p>\n        <p>\n          Currently, I{\"\\'\"}m doing my M.S. in Computer Science at <a href='https://www.stanford.edu/'>Stanford University</a>. Previously, I did my B.S. in Computer Science at the Georgia Institute of Technology.\n        </p>\n        <h3>\n          Research\n        </h3>\n        <Research/>\n      </>\n    );\n}\n\nexport default Page;\n","import React from 'react';\nimport { Link } from 'react-router-dom';\nimport Button from '@material-ui/core/Button';\nimport '../App.css';\n\nfunction Entry(props) {\n  return (\n    <p>\n      <Button variant='outlined' href={props.href}>\n        <em>{props.children}</em>\n      </Button>\n    </p>\n  );\n}\n\nfunction Page() {\n    return (\n      <>\n        <Entry href='/writing/risk-aware-reinforcement-learning'>\n          Risk-Aware Reinforcement Learning\n        </Entry>\n        <Entry  href='/writing/how-differential-privacy-fits-into-industry'>\n          How Differential Privacy (Could) Fit Into Industry\n        </Entry>\n        <Entry href='/writing/differentially-private-deep-learning'>\n          A Guide to Differentially Private Deep Learning\n        </Entry>\n      </>\n    );\n}\n\nexport default Page;\n","import React from 'react';\nimport Button from '@material-ui/core/Button';\nimport '../App.css';\n\n\nfunction Entry(props) {\n  return (\n    <p>\n      <Button variant='outlined' href={props.href}>\n        <em>{props.children}</em>\n      </Button>\n    </p>\n  );\n}\n\nfunction Page() {\n    return (\n      <>\n        <Entry href='https://law.stanford.edu/wp-content/uploads/2019/01/Bellovin_20190129.pdf'>\n            Privacy and Synthetic Datasets\n        </Entry>\n        <Entry href='https://cset.georgetown.edu/wp-content/uploads/Keeping-Top-AI-Talent-in-the-United-States.pdf'>\n            Keeping Top AI Talent in the United States\n        </Entry>\n        <Entry href='http://web.stanford.edu/class/psych209/Readings/LakeEtAlBBS.pdf'>\n            Building Machines That Learn and Think Like People\n        </Entry>\n        <Entry href='https://arxiv.org/abs/1911.09421'>\n            The Linear Algebra Mapping Problem\n        </Entry>\n        <Entry href='https://arxiv.org/abs/1905.02175'>\n            Adversarial Examples Are Not Bugs, They Are Features\n        </Entry>\n        <Entry href='https://arxiv.org/abs/2003.03384'>\n            AutoML-Zero: Evolving Machine Learning Algorithms From Scratch\n        </Entry>\n      </>\n    );\n}\n\nexport default Page;\n","import React from 'react';\nimport Button from '@material-ui/core/Button';\nimport '../App.css';\n\n\nfunction Page() {\n    return (\n      <>\n        <p>\n          <img src='./normalizing-flows.png' style={{ borderRadius: '10px', width: '100%' }} />\n        </p>\n        <p>\n          <img src='./hoover.gif' style={{ borderRadius: '10px', width: '100%' }} />\n        </p>\n      </>\n    );\n}\n\nexport default Page;\n","import React from 'react';\nimport 'katex/dist/katex.min.css';\nimport { InlineMath, BlockMath } from 'react-katex';\n\n\nfunction Post() {\n    return (\n      <>\n        <h2>\n          A Guide to Differentially Private Deep Learning\n        </h2>\n        <p>\n          In this post we review differential privacy, the task of deep learning, what privacy means in this context, and reference current results in this area.\n        </p>\n\n        <h3>\n          Differential Privacy\n        </h3>\n        <p>\n          Finding a definition which fully satisfies one’s intuitive understanding of privacy is surprisingly tricky. One such definition is <em>differential privacy</em>. On a high level, the idea behind differential privacy is that given a randomized algorithm which performs some statistical task on subsets of a dataset, such an algorithm would “preserve privacy” if it behaved approximately the same regardless of the inclusion or exclusion of any individual in the subset it was acting on. That is, the data of each entry would be thought to be hidden since the behavior of the algorithm closely resembles every possible case where the entry would not have been included.\n        </p>\n        <p>\n          Formally, this is expressed as the following<sup><a href=\"https://stephentu.github.io/writeups/6885-lec20-b.pdf\">1</a></sup>:\n        </p>\n\n        <h3>\n          Deep Learning\n        </h3>\n        <p>\nDeep learning is currently one of the most predominant forms of statistical analysis used today and has been shown to be remarkably effective for a variety of tasks. Deep neural networks, in their standard form, define a function composed of a sequence of layers where each layer represents an operation to be performed on the output of the previous layer. Typically the goal associated with such models is to find the set of parameters which map a set of inputs to a set of outputs in a way which minimizes some function, referred to as the loss function.\n        </p>\n        <BlockMath math=\"\\{ x^{(1)}, x^{(1)}, \\ldots x^{(b)} \\} \\sim sample(X, b)\" />\n        <BlockMath math=\"\\ell(\\theta) = \\frac{1}{b} \\sum_i \\ell(x^{(i)} ; \\theta)\" />\n        <BlockMath math=\"\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\ell(\\theta)\" />\n        <p>\n          A popular method for finding such parameters is via a process of stochastic gradient descent. When conducting stochastic gradient descent, one iteratively updates the parameters of the model by sampling an individual input-output pair from the dataset and partially applying their values to the error function so that the gradient of the error with respect to the parameters of the model can be computed. Then, one would update the parameters of the model in the direction opposite of the gradient, in turn minimizing the error function with respect to that example. Formally, if we let θ0 be the randomly initialized parameters of the model, θt be the parameters of the model at iteration t, (xt, yt) be our sampled input-output pair, L be our error function, and ηt be the learning rate, we iteratively apply the following update rule:\n        </p>\n        <p>\n          Although, it’s more common in practice to opt for minibatch gradient descent. Rather than calculating gradients with respect to individual examples, one uniformly samples a subset of B examples without replacement, calculates the gradient with respect to each example, and applies the average of the gradients to the model. This corresponds to the following update rule:\n        </p>\n\n        <h3>\n          Differentially Private Stochastic Gradient Descent\n        </h3>\n        <p>\n          <a href=\"https://arxiv.org/abs/1607.00133\">Abadi et al.</a> detail the differentially private stochastic gradient descent (DP-SGD) algorithm to make traditional SGD yield a differential privacy guarantee. To describe it, we need to introduce a number of augmentations.\n        </p>\n        <p>\n          First, we have to augment our typical method for sampling examples from the dataset. In the context of non-private deep learning, sampling is often achieved by shuffling the dataset and running through partitions of size <InlineMath math=\"b\" /> such that each example is viewed by the model exactly once per epoch. In the context of DP-SGD, we have to opt for either Poisson<sup>2</sup> or uniform<sup>3</sup> subsampling if we want a differential privacy guarantee.\n        </p>\n        <p>\n          Second, we need to introduce a number of additional training parameters. In particular, we need to introduce a clipping parameter <InlineMath math=\"C\" />, an upper bound on the <InlineMath math=\"\\ell_2\" />-norm of each per-example gradient, as well as the noise multiplier <InlineMath math=\"\\sigma\" />, which acts as the ratio between the clipping parameter and variance of the Gaussian noise applied to each gradient update after clipping.\n        </p>\n        <BlockMath math=\"\\{ x^{(1)}, x^{(1)}, \\ldots x^{(b)} \\} \\sim sample(X, b)\" />\n        <BlockMath math=\"g^{(i)} \\leftarrow \\nabla_{\\theta} \\ell(x^{(i)} ; \\theta)\" />\n        <BlockMath math=\"\\bar{g}^{(i)} \\leftarrow g^{(i)} / \\max\\{ 1, ||g^{(i)}||_2 / C\\}\" />\n        <BlockMath math=\"\\tilde{g} \\leftarrow \\frac{1}{b} (\\sum_i \\bar{g}^{(i)} + \\sigma C \\mathcal{N}(0, I))\" />\n        <BlockMath math=\"\\theta \\leftarrow \\theta - \\eta \\tilde{g}\" />\n        <p>\n          In order to calculate the privacy loss corresponding to k executions of the above update rule, Abadi et al. detail the moments accountant as a method to report privacy loss over time. A full deep dive into the foundations backing the moments accountant are likely outside the scope of this post, but on a high level it can be thought of as a black box which takes in values which characterize your training loop (sampling probabilities, number of minibatches, delta, etc.) and outputs epsilon. But importantly, their method yields much tighter bounds on the privacy loss achieved than what is reported via the strong composition theorem. If interested in learning more, the algorithm was originally introduced in Abadi et al. and has a corresponding implementation within Tensorflow Privacy.\n        </p>\n\n        <h3>\n          Private Aggregation of Teacher Ensembles\n        </h3>\n        <p>\n          Private Aggregation of Teacher Ensembles or PATE is an alternative method to DPSGD for conducting differentially private learning. The key idea is to, rather than train a single strong model which captures a complex criterion in a differentially private manner, train a set of weaker, non-private models on partitions of the data and then perform a noisy aggregation of their predictions. Overall the method has been shown to be quite effective at the expense of some assumptions about the training procedure. There has even been an application of this technique to GANs via PATE-GAN.\n        </p>\n\n        <h3>\n          Differentially Private Federated Learning\n        </h3>\n\n        <h3>\n          Proper Development\n        </h3>\n        <p>\n          There are a number of common gotchas involved with the proper development and deployment of differentially private deep learning models.\n        </p>\n        <p>\n          The biggest is hyperparameter search and model selection. You might think intuitively that, as long as you implement and execute something like DP-SGD, you can sleep soundly at night knowing your model is privacy preserving. Although, typically many models are trained in the process of during the hyperparameter . Although in theory, the hyperparameters you{\"\\'\"}re selecting are technically data-inspired, and hence indirectly leak information. Frankly this detail is swept under the rug in research contexts. But in industry, this absolutely needs to be taken into consideration, or at the very least consciously acknowledged as a risk.\n        </p>\n        <p>\n          The second is proper evaluation. It does not suffice to simply train a single model and report its accuracy. Recall that\n        </p>\n      </>\n    );\n}\n\nexport default Post;\n","import React from 'react';\n\nfunction Post() {\n  return (\n    <div>\n      <h2>\n          How Differential Privacy (Could) Fit Into Industry\n      </h2>\n      <p>\n          In the context of differential privacy, there{\"\\'\"}s typically some data curator (e.g. some silicon valley tech giant) and the outside world. Supposedly the curator has an incentive to release some data analysis result, but in such a way where the privacy leakage associated with this analysis is trackable.\n      </p>\n      <p>\n          Although, there are several practical issues with this. One which is common is this issue of an <em>infinite horizon</em>. That is, say this silicon valley tech giant wants to adhere to a strict privacy budget for the rest of eternity. When they want to release the result of an analysis, what epsilon should they choose? 1? 0.1? 0.0001? It’s unclear, especially if their goal is to stick around for eternity, and we assume that the privacy leakage associated with a given analysis is permanant.\n      </p>\n      <p>\n          What will happen? Well, once they’ve inevitably creeped up on their limit, should we expect them to seriously consider halting the release of further results forever? No - more than likely, they’ll just raise the limit. And inevitably they’ll do it again. This is to say that, in the context of entities which have no foreseeable horizon, the apparent practicality of differential privacy is of concern because the privacy expenditure is a monotonically increasing value over time.\n      </p>\n      <p>\n          This is not a problem I have a general solution to. I’m a big fan of differentially private synthetic datasets, and in certain contexts they can help in this regard. Although, not every undertaking can be easily framed as a synthetic data problem, especially if new relevant data comes in consistently.\n      </p>\n      <p>\n          Although, the core point here I’d like to make is that <em>the continual public release of results to analyses may not be the most interesting or useful context to evaluate the utility of differential privacy within</em>.\n      </p>\n      <p>\n          Instead, consider the case where an engineer at the aforementioned silicon valley tech giant accidentally leaves their laptop in the car and it gets stolen, and say they were doing some work concerning sensitive data. Can you begin to quantify the amount of damage done to the individuals included in the data they were working with? Not by default, but naturally if the results the engineer was working with were computed with differential privacy in mind, then you could actually start to get some form of a guarantee.\n      </p>\n      <p>\n          So, the slight distinction I’m making here is that maybe the utility of differential privacy is not as pronounced in contexts where the forefront goal is information release. Maybe a more useful context for differential privacy is actually behind the walls of your organization.\n      </p>\n      <p>\n          That is, the incorporation of differential privacy might be best served as a means for protection against the worst case scenario, where a data leakage happens against your will. Now the conversation shifts from saying “silicon valley tech giant, use differential privacy so that your public analyses don’t reveal too much about your users”, and it becomes “have your employees speak through the lens of differential privacy, so we know how much damage has been done in the worst case where information is leaked.”\n      </p>\n      <p>\n          This reformulation of the problem setting, in a bit of a roundabout matter, highlights the utility of differential privacy by dampening issues concerning infinite horizons, stemming from the inherent nature of data leakages. Namely, they are <em>sparse</em> and <em>unintended</em>.\n      </p>\n      <p>\n          Given that data leakages are canonically sparse, this allows you to talk about a global privacy budget per individual which might actually be useful. That is, you could actually get away with something like an epsilon of 1.0 per user over a very long timespan if data release occurs every ten years, not every day.\n      </p>\n      <p>\n          In addition, it doesn’t make sense to complain about the limitations of differential privacy with respect to its monotonically increasing nature if data release is unintended by definition - if it’s going to happen regardless, you don’t have to worry as much about the number of releases you intend to perform forever onwards because that’s not a variable you can control in the first place.\n      </p>\n      <p>\n          There are just my thoughts in isolation, and this has been said before by others. But hopefully it sparks additional discussion on the topic, on where differential privacy will make the most sense to be deployed in the real world in years to come.\n      </p>\n    </div>\n  );\n}\n\nexport default Post;\n","import React from 'react';\nimport 'katex/dist/katex.min.css';\nimport { InlineMath, BlockMath } from 'react-katex';\n\nfunction Post() {\n  return (\n    <>\n      <h2>\n          Risk-Aware Reinforcement Learning\n      </h2>\n\n      <p>\n          In this post, we will investigate the notion of risk in the context of reinforcement learning.\n      </p>\n\n      <h3>\n        Actor-Critic\n      </h3>\n      <p>\n        Actor critic methods, in my opinion, are often described in strangely confusing ways despite not being a particularly complex topic. Here we{\"\\'\"}ll offer a brief introduction.\n      </p>\n      <p>\n        First we have a notion of a critic <InlineMath math=\"\\hat{Q}: \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}\" /> which takes in a state <InlineMath math=\"s\" /> and an action <InlineMath math=\"a\" /> and produces an estimate <InlineMath math=\"\\hat{Q}(s, a)\" /> of the expected long-term reward of taking action <InlineMath math=\"a\" /> in state <InlineMath math=\"s\" />, and following the policy afterwards. To learn a good estimate of this function, we compare this estimate against a target <InlineMath math=\"Q(s, a) = r + \\gamma \\sum_{s'} p(s' | s, a) \\hat{Q}(s', a')\" /> during training, which partially incorporates ground-truth information in the form of an observed reward combined with a bootstrapped estimate. We then simply take a gradient step in the direction minimizing the difference between our computed estimate and our target.\n      </p>\n      <BlockMath math=\"\\theta_Q \\leftarrow \\theta_Q - \\eta \\nabla_{\\theta_Q} (\\hat{Q}(s, a) - Q(s, a))^2\" />\n      <p>\n        Second we introduce a notion of an actor <InlineMath math=\"A: \\mathcal{S} \\rightarrow \\mathcal{A}\" /> which simply takes in a state <InlineMath math=\"s\" /> and outputs an action <InlineMath math=\"a\" />. To quantify the quality of this predicted action, we pass <InlineMath math=\"s\" /> and <InlineMath math=\"a\" /> into the critic, which then outputs the expected long-term reward of taking <InlineMath math=\"a\" /> in <InlineMath math=\"s\" />. Assuming this estimate is decent, we would then want to update the actor in such a manner so as to maximize this value, so we take a gradient step in the direction achieving this.\n      </p>\n      <BlockMath math=\"\\theta_A \\leftarrow \\theta_A + \\eta \\nabla_{\\theta_A} \\hat{Q}(s, A(s))\" />\n      <p>\n        After alternating between training the actor and training the critic while exploring the environment, we should eventually converge on a good value function and a good actor characterizing the policy of the agent.\n      </p>\n\n      <h3>\n        What is Risk?\n      </h3>\n      <p>\n        Now, imagine that our critic didn{\"\\'\"}t just estimate the expected long-term reward, but it estimated the entire <em>distribution</em> of long term rewards. Maybe we could assume that this distribution is a Gaussian, and predict its mean and variance. Assuming we could do that, then how might we define risk?\n      </p>\n      <p>\n        Well, one way you could think about risk is that it{\"\\'\"}s <em> the degree to which you are concerned about worst-case outcomes</em>. If you are risk-averse, that means you <em>only</em> care about worst case outcomes. If you are risk-willing, then you might only care about average-case outcomes.\n      </p>\n      <p>\n        A way to formalize this notion is through <em>conditional value at risk (CVaR)</em>. Given a distribution, it is defined as the expectation of the distribution up to the <InlineMath math=\"\\alpha\" />-th percentile. If <InlineMath math=\"\\alpha \\approx 0\" />, then you essentially only care about the worst case. If <InlineMath math=\"\\alpha = 1\" />, then you only care about expected case (which would be regular actor-critic!) Anything in between is simply some domain-specific tradeoff.\n      </p>\n      <p>\n          This is the idea followed in the paper <em><a href=\"https://arxiv.org/abs/1911.03618\">Worst Case Policy Gradient</a></em> by Tang et al., and the basis for this post.\n      </p>\n\n      <h3>\n        Training the Critic\n      </h3>\n      <p>\n          Getting the estimated means and variances from your model is fairly straightforward - just feedforward your critic model! Give it a state and an action and it will spit out the estimated mean and variance.\n      </p>\n      <BlockMath math=\"\\{ \\hat{Q}(s, a), \\hat{\\Upsilon}(s, a) \\} = critic(s, a)\" />\n      <p>\n          Now how will we attain good estimates for <InlineMath math=\"\\hat{Q}(s, a)\" /> and <InlineMath math=\"\\hat{\\Upsilon}(s, a)\" />? By the same idea of computing targets given observed reward information. Fist, the mean is actually quite easy because it has precisely the same semantic interpretation of the regular Q-value, hence going unchanged.\n      </p>\n      <BlockMath math=\"Q(s, a) = r + \\gamma \\sum_{s'} p(s' | s, a) \\hat{Q}(s', a')\" />\n      <p>\n          Now we just need to sort out what the proper variance target should look like. By simply expanding the definition of variance, you should end up with an expression resembling the following.\n      </p>\n      <BlockMath math=\"\\Upsilon(s, a) \\leftarrow r^2 + 2 \\gamma r \\sum_{s} p(s' | s, a) \\hat{Q}(s', a')\" />\n      <BlockMath math=\"+ \\gamma^2 \\sum_{s'} p(s' | s, a) \\hat{\\Upsilon}(s', a') + \\gamma^2 \\sum_{s'} p(s' | s, a) \\hat{Q}(s', a')^2 - \\hat{Q}(s, a)^2\" />\n      <p>\n          Now we have our <em>estimates</em>, namely <InlineMath math=\"\\hat{Q}(s, a)\"/> and <InlineMath math=\"\\hat{\\Upsilon}(s, a)\"/>, as well as our <em>targets</em>, namely <InlineMath math=\"Q(s, a)\"/> and <InlineMath math=\"\\Upsilon(s, a)\"/>.\n      </p>\n      <p>\n        Now we have to construct a loss function which will quantify how good our estimates are. Why not define our loss function as some statistical distance metric between the Gaussian distributions characterized by our estimate and target? It turns out, the Wasserstein distance between the two Gaussians can be characterized by the following expression.\n      </p>\n      <BlockMath math=\"\\ell(\\theta) = (\\hat{Q}(s, a) - Q(s, a))^2\" />\n      <BlockMath math=\"+ \\hat{\\Upsilon}(s, a) + \\Upsilon(s, a) - 2\\sqrt{\\hat{\\Upsilon}(s, a) \\Upsilon(s, a)}\" />\n      <p>\n        This expression should satisfy our intuition - the loss is zero when <InlineMath math=\"\\hat{Q}(s, a) = Q(s, a)\" /> and <InlineMath math=\"\\hat{\\Upsilon}(s, a) = \\Upsilon(s, a)\" />, and positive otherwise.\n      </p>\n\n      <h3>\n        Training the Actor\n      </h3>\n      <p>\n        Now how should we train our actor? This part is easy assuming we have a trained critic. Just as before, we pass in a state <InlineMath math=\"s\" /> to our actor which will give us an action <InlineMath math=\"a\" />. This action is then passed into the critic, which yields a mean <InlineMath math=\"\\hat{Q}(s, a)\" /> and a variance <InlineMath math=\"\\hat{\\Upsilon}(s, a)\" />. Given this mean and variance, we can directly calculate the <emph>CVaR</emph> metric as the following where <InlineMath math=\"\\phi(\\alpha)\" /> is the PDF of the Gaussian distribution evaluated at <InlineMath math=\"\\alpha\" /> and <InlineMath math=\"\\Phi(\\alpha)\" /> is the CDF.\n      </p>\n      <BlockMath math=\"\\Gamma(s, a, \\alpha) = \\hat{Q}(s, a) - \\frac{\\phi(\\alpha)}{\\Phi(\\alpha)}\\sqrt{\\hat{\\Upsilon}(s, a)}\" />\n      <p>\n        This is the scalar we want! Now, the actor just takes a gradient step in the direction <em>maximizing</em> this value. Super simple!\n      </p>\n\n      <h3>\n        Conclusion\n      </h3>\n      <p>\n        Hopefully this acted as a simple introduction to risk in the context of reinforcement learning. If you have any questions, feel free to shoot them my way!\n      </p>\n    </>\n  );\n}\n\nexport default Post;\n","import React from 'react';\nimport 'katex/dist/katex.min.css';\nimport { InlineMath, BlockMath } from 'react-katex';\nimport InstagramEmbed from 'react-instagram-embed';\n\nfunction Post() {\n  return (\n    <>\n      <h2>\n          Deaths of Despair\n      </h2>\n      <p>\n      <em>\n          Ecclesiastes 1:9. What has been will be again, what has been done will be done again; there is nothing new under the sun.\n      </em>\n      </p>\n      <p>\n          I believe this is true in one sense, but false in another. Have these things been echoed in other forms in the past? Absolutely. But are these manifestations becoming increasingly more potent? I would argue so.\n      </p>\n      <p>\n          In the past, you at least had some degree of separation between you and the elite. You can realize this in a few ways, but the way I think the most clearly about is the following. That is, on average, the amount of time these individuals occupy your direct attention, with some implicit weight on subconscious attention.\n      </p>\n      <p>\n          Let’s take the median American who makes around $33,706 a year (2018). Now, convince yourself there exists some probability, no matter how small, that such a person exhibits a chance of breaking down upon seeing such a photo after a long and hard day at work.\n      </p>\n      <p>\n          How many times a day do you think that coin is flipped? There are 350 million americans. Wouldn’t you like to be him? He looks well rested, and if he were to go up to any of his million fans, he would be undoubtedly praised like a living god.\n      </p>\n\n      <h3>\n          Wealth Inequality\n      </h3>\n      <p>\n          I see Amazon as a useful analog. If you had to distill America{\"\\'\"}s problem, it could be physically manifested as Amazon{\"\\'\"}s structure. Essentially, we have a clear balkanization and class structure. Which is, we have the elite which make decisions and live way more well off than necessary, the engineers, the lower class who are treated unreasonably poorly, and the hyper-elite Jeff Bezos money who is more valuable than is really coprehensible.\n      </p>\n      <p>\n          At the time of writing this, Jeff Bezos is worth approximately 180 billion dollars. To conceptualize this, consider a billionaire. To become a billionaire, you would have had to have had a 4 million dollar salary every year throughout all of American history, which is an absolutely ridiculous salary to have persisted through the civil war, the invention of the cotton gin, throught the civil rights movement, until now. That is poverty in relation to Jeff Bezos.\n      </p>\n      <p>\n          Now, the problem per say clearly isn{\"\\'\"}t that automation or wealth inequality is new in some sense, this is clearly false. But I would argue that a problem which rapidly exacerbates is indistinguishable from a new problem, and that this principle applies in this context. Of course, every time a new technology comes around it ripples throughout the economy. But the problem now is that the proportions of the population our innovations are having are growing, and the skills they undermine require more and more prior investment. Of course, when the cotton gin came around people surely lost their jobs, but finding other blue collar work was an alternative and learning the upkeep a cotton gin, which required more skill than picking the cotton itself, took an investment on the order of days. Now, we{\"\\'\"}re talking about technology which could potentially undermine manual labor all together, and using methods which require so much more skill, at an unprecedented speed.\n      </p>\n      <p>\n          The key difference is the following key metric. What proportion of outsourced workers were able to actually contribute or play a part in the technology which outsourced them? With the cotton gin, I would argue that that number was modestly high. That is, the skills you had to be a picker of cotton weren{\"\\'\"}t too far off from maintaining a cotton gin or thinking of new ideas to improve it, it was a mechanical contraption after all. Of course not everyone would exhibit that effort and might simply switch to another industry, but the possibility was there. What proportion of truck drivers outsourced will be able to contribute or play a part in self-driving car technology? Literally zero percent.\n      </p>\n      <p>\n          So lets say you think well get past the self driving truck issue, which id agree we probably will at some cost. Well, again, this is getting worse. Whats the next large scale technological innovation to be had? Almost surey the end of menial white collar work requiring basic linguistic ability and reasoning skills, aka lawyers, accountants and the like. What are we going to tell 40 year old men with kids and a mortgage? Especially when the maintenance of this software requires a team of three engineers? Im sorry but theres no jobs available in this context - midway through your career you might just need to learn how to be a pharmacologist. Its almost in some sense like our efficiency and problem solving ability has become too low of a barrier to entry for our own good.\n      </p>\n      <p>\n          Again, my argument is not that self driving cars will be the end of civilization. Its that the implications of technological innovation are getting worse. If you think this round of innovation isn{\"\\'\"}t sufficient, wait 100 years. What will we be outsourcing then?\n      </p>\n      <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/G1UpFHsbOf0?start=1745\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n\n      <h3>\n          Social Media\n      </h3>\n\n      <h4>\n          Twitch\n      </h4>\n      <p>\n        I can{\"\\'\"}t say my response to such incidents better than it was said by /u/Thopterthallid in <a href=\"https://www.reddit.com/r/AskReddit/comments/ca0gm6/people_who_ordered_belle_delphines_bathwater_why/et5pesf?utm_source=share&utm_medium=web2x\">this</a> comment.\n      </p>\n\n      <p>\n        <details>\n        <summary><i>I want to try and break \"Gamer girl water\" down for people...</i></summary>\n        <i>\n        ...coming from someone who had a dark time in my life where I might have been tempted to buy some \"gamer girl bath water\". It{\"\\'\"}s easy to say \"Oh she{\"\\'\"}s just tricking idiots out of their money\" but it{\"\\'\"}s a little more complicated than that.<br/>\n        <br/>\n          <b>When you{\"\\'\"}re approaching your mid to late 20s and still haven{\"\\'\"}t enjoyed a healthy and intimate relationship, it really does start to fuck with you on a scary level.</b> Mental illness, depression, anxiety, and such seem to be a much more prevalent nowadays since it{\"\\'\"}s so easy to get \"quick fix\" social interaction through the internet. The internet is amazing, but it doesn{\"\\'\"}t make you better at actually being a social human being in real life. You get guys who had quick and easy access to a hundred friends on MSN/AIM/Skype/Discord/Telegram grow increasingly dependant on online interactions for their social needs. I can confirm it gets to the point where you just don{\"\\'\"}t even really want real interactions anymore because pornhub handles your libido, twitch gives you someone to listen to, instant messaging gives you people to talk to, etc. The only thing missing is the sexual physical touch of a woman, and there{\"\\'\"}s a part of you that might think that water that{\"\\'\"}s touched your Twitch crush will satisfy that. It won{\"\\'\"}t of course.<br/>\n        <br/>\n          <b>The way that the most popular Youtubers, and Twitch Streamers interact with their audience is that of being a friend, family, or lover.</b> How many streamers call their viewers some kind of pet name? Like, the [Streamer name] Family, or the [Streamer name] Army. When your livelihood comes from donations and views, you need to be especially charismatic. The biggest names in online video makers always talk directly to their audience. They look into the camera, they call you their friends and family, they want you to hang out with them at conventions, and they want to hear your comments, they want to read your messages in chat, they offer life advice, they tell you they care about you, they tell you they{\"\\'\"}re thankful for you, they want you to take care of yourself, and often times they{\"\\'\"}ll open up their soul and talk about their lives to you. It{\"\\'\"}s all about charisma and being a good host and it{\"\\'\"}s a great talent to have. For people in the previous point, this can actually be super harmful. You spend enough time in their streams and watching their videos, and you really do start to subconsciously believe that Markiplier, Pewdiepie, and Summit are your friends. Obviously it{\"\\'\"}s not the case, but there was a time in my life where some of my favourite Youtubers were appearing in my dreams and were asking me to hang out, and my brain didn{\"\\'\"}t even question that it was something we just did all the time. I recall a few years ago some kids and their mom showed up to a famous Twitch streamer{\"\\'\"}s address and were confused as to why he felt that was a breach of privacy. They expected to hang out with him, get autographs, play games with him, and all around expect that he{\"\\'\"}d be thrilled to see them. Obviously, this is a huge breach of boundaries, and perfectly sane people just forget that these people don{\"\\'\"}t have a 1:1 friendship with them.<br/>\n        <br/>\n          <b>When you{\"\\'\"}re 25+ and haven{\"\\'\"}t had meaningful relationships with women, either romantic or otherwise, you start to get weird ideas about what women are, especially if you talk to the wrong groups.</b> When you get to the point where you{\"\\'\"}ve craved intimate love and never had it for over ten years, you start getting weird ideas. Some men lash out at women and get incel-ish. Some men put women on these pedestals as divine trophies to be won. Some men just assume that women will never like them. But the longer you go without ever knowing love, the easier it is to think of women as being a totally different species.<br/>\n        <br/>\n          <b>When every woman you{\"\\'\"}ve ever known either hates video games or are casual/novice gamers at best, meeting a woman who seems to idolize gaming as much as you do triggers volatile emotions.</b> Sometimes it comes in the form of disdain and skepticism, and end up making a toxic environment for women in gaming. If not that, it comes out as excessive idolization. You{\"\\'\"}re at a point in your life where any woman who would even pay attention to you is a dream, and the thought of a girl who would share your favourite hobby with you may as well be the second coming of Christ. You crush on her hard, you buy all her merch, you send her donations so that she{\"\\'\"}ll start to recognize your name and tell you \"thank you\" and read your messages. I{\"\\'\"}m not saying that women shouldn{\"\\'\"}t be allowed to be successful gaming streamers, but I do think that taking advantage of clinically depressed and lonely fans like this is awful.<br/>\n        <br/>\n          <b>This whole, selling bathwater thing to thirsty fans may seem like memes and trolling, but in reality it{\"\\'\"}s just highlighting a very, very sad reality.</b> There{\"\\'\"}s guys out there who would buy this in a futile attempt just to feel closer to the one girl that ever paid them any attention. I was dangerously close to being one of those guys at one point and I know just how scary the spiral goes. People on my path would either end up doing things like buying this bathwater or becoming misogynistic incels.<br/>\n        <br/>\n          It all sounds so stupid and pathetic, but it{\"\\'\"}s a very true reality for some unfortunate guys suffering from severe depression.\n        </i>\n        </details>\n        <br/>\n      </p>\n\n      <h4>\n          Tinder\n      </h4>\n\n      <InstagramEmbed url='https://instagr.am/p/Bfw2OPinqme/' maxWidth={320} hideCaption={true} />\n      <InstagramEmbed url='https://instagr.am/p/BoPAPVKAnS0/' maxWidth={320} hideCaption={true} />\n\n      <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/PMotykw0SIk?start=1282\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n    </>\n  );\n}\n\nexport default Post;\n","import './App.css';\n\nimport Page from './components/page';\nimport React from 'react';\nimport { Switch, Route } from 'react-router-dom';\n\nimport About from './pages/about';\nimport Research from './pages/research';\nimport Writing from './pages/writing';\nimport Reading from './pages/reading';\nimport Design from './pages/design';\n\nimport DifferentiallyPrivateDeepLearning from './posts/differentially-private-deep-learning';\nimport HowDifferentialPrivacyFitsIntoIndustry from './posts/how-differential-privacy-fits-into-industry';\nimport RiskAwareReinforcementLearning from './posts/risk-aware-reinforcement-learning';\nimport DeathsOfDespair from './posts/deaths-of-despair';\n\nfunction App() {\n  return (\n    <Page>\n      <Switch>\n        <Route exact path='/' component={About}/>\n\n        <Route exact path='/about' component={About}/>\n        <Route exact path='/research' component={Research}/>\n        <Route exact path='/writing' component={Writing}/>\n        <Route exact path='/reading' component={Reading}/>\n        <Route exact path='/design' component={Design}/>\n\n        <Route exact path='/writing/deaths-of-despair' component={DeathsOfDespair} />\n        <Route exact path='/writing/how-differential-privacy-fits-into-industry' component={HowDifferentialPrivacyFitsIntoIndustry} />\n        <Route exact path='/writing/differentially-private-deep-learning' component={DifferentiallyPrivateDeepLearning} />\n        <Route exact path='/writing/risk-aware-reinforcement-learning' component={RiskAwareReinforcementLearning} />\n      </Switch>\n    </Page>\n  );\n}\n\nexport default App;\n","import './index.css';\nimport * as serviceWorker from './serviceWorker';\nimport App from './App';\nimport React from 'react';\nimport ReactDOM from 'react-dom';\nimport { BrowserRouter } from 'react-router-dom';\n\nReactDOM.render((\n  <BrowserRouter>\n    <App/>\n  </BrowserRouter>\n  ), document.getElementById('root')\n);\n\n// If you want your app to work offline and load faster, you can change\n// unregister() to register() below. Note this comes with some pitfalls.\n// Learn more about service workers: https://bit.ly/CRA-PWA\nserviceWorker.unregister();\n"],"sourceRoot":""}